{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ff6541",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e7678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Document Structure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7568115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'file.txt', 'author': 'Laksh', 'date_created': '2025-09-08'}, page_content='This is the content for the RAG system.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"This is the content for the RAG system.\",\n",
    "    metadata={\n",
    "        \"source\": \"file.txt\",\n",
    "        \"author\": \"Laksh\",\n",
    "        \"date_created\": \"2025-09-08\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee85bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple txt file\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01137cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created.\n"
     ]
    }
   ],
   "source": [
    "sample_text ={\n",
    "    \"../data/text_files/Why_RAG.txt\": \"\"\"Why is Retrieval-Augmented Generation important?\n",
    "LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.\n",
    "\n",
    "Known challenges of LLMs include:\n",
    "\n",
    "Presenting false information when it does not have the answer.\n",
    "Presenting out-of-date or generic information when the user expects a specific, current response.\n",
    "Creating a response from non-authoritative sources.\n",
    "Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.\n",
    "You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\n",
    "\n",
    "RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\n",
    "\"\"\",\n",
    "    \"../data/text_files/RAG_benefits.txt\": \"\"\"What are the benefits of Retrieval-Augmented Generation?\n",
    "RAG technology brings several benefits to an organization's generative AI efforts.\n",
    "\n",
    "Cost-effective implementation\n",
    "Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable.\n",
    "\n",
    "Current information\n",
    "Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users.\n",
    "\n",
    "Enhanced user trust\n",
    "RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution.\n",
    "\n",
    "More developer control\n",
    "With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM's information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications.\n",
    "\"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "for filepath, content in sample_text.items():\n",
    "    with open(filepath,'w',encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text files created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47de2a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/Why_RAG.txt'}, page_content='Why is Retrieval-Augmented Generation important?\\nLLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.\\n\\nKnown challenges of LLMs include:\\n\\nPresenting false information when it does not have the answer.\\nPresenting out-of-date or generic information when the user expects a specific, current response.\\nCreating a response from non-authoritative sources.\\nCreating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.\\nYou can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\\n\\nRAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\\n')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader= TextLoader(\"../data/text_files/Why_RAG.txt\",encoding= \"utf-8\")\n",
    "\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5af8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\RAG_benefits.txt'}, page_content=\"What are the benefits of Retrieval-Augmented Generation?\\nRAG technology brings several benefits to an organization's generative AI efforts.\\n\\nCost-effective implementation\\nChatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable.\\n\\nCurrent information\\nEven if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users.\\n\\nEnhanced user trust\\nRAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution.\\n\\nMore developer control\\nWith RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM's information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications.\\n\"),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\Why_RAG.txt'}, page_content='Why is Retrieval-Augmented Generation important?\\nLLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.\\n\\nKnown challenges of LLMs include:\\n\\nPresenting false information when it does not have the answer.\\nPresenting out-of-date or generic information when the user expects a specific, current response.\\nCreating a response from non-authoritative sources.\\nCreating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.\\nYou can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\\n\\nRAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files\n",
    "    loader_cls=TextLoader, ## Loader class to use\n",
    "    loader_kwargs={\"encoding\": 'utf-8'},\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f51af7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 1}, page_content='1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 3}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 6}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 10}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 12}, page_content='Attention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 13}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention-is-all-you-need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 14}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 0}, page_content='Ray: A Distributed Framework for Emerging AI Applications\\nPhilipp Moritz∗, Robert Nishihara∗, Stephanie Wang, Alexey Tumanov, Richard Liaw,\\nEric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, Ion Stoica\\nUniversity of California, Berkeley\\nAbstract\\nThe next generation of AI applications will continuously\\ninteract with the environment and learn from these inter-\\nactions. These applications impose new and demanding\\nsystems requirements, both in terms of performance and\\nﬂexibility. In this paper, we consider these requirements\\nand present Ray—a distributed system to address them.\\nRay implements a uniﬁed interface that can express both\\ntask-parallel and actor-based computations, supported by\\na single dynamic execution engine. To meet the perfor-\\nmance requirements, Ray employs a distributed scheduler\\nand a distributed and fault-tolerant store to manage the\\nsystem’s control state. In our experiments, we demon-\\nstrate scaling beyond 1.8 million tasks per second and\\nbetter performance than existing specialized systems for\\nseveral challenging reinforcement learning applications.\\n1\\nIntroduction\\nOver the past two decades, many organizations have been\\ncollecting—and aiming to exploit—ever-growing quanti-\\nties of data. This has led to the development of a plethora\\nof frameworks for distributed data analysis, including\\nbatch [20, 64, 28], streaming [15, 39, 31], and graph [34,\\n35, 24] processing systems. The success of these frame-\\nworks has made it possible for organizations to analyze\\nlarge data sets as a core part of their business or scientiﬁc\\nstrategy, and has ushered in the age of “Big Data.”\\nMore recently, the scope of data-focused applications\\nhas expanded to encompass more complex artiﬁcial intel-\\nligence (AI) or machine learning (ML) techniques [30].\\nThe paradigm case is that of supervised learning, where\\ndata points are accompanied by labels, and where the\\nworkhorse technology for mapping data points to labels\\nis provided by deep neural networks. The complexity of\\nthese deep networks has led to another ﬂurry of frame-\\nworks that focus on the training of deep neural networks\\n∗equal contribution\\nand their use in prediction. These frameworks often lever-\\nage specialized hardware (e.g., GPUs and TPUs), with the\\ngoal of reducing training time in a batch setting. Examples\\ninclude TensorFlow [7], MXNet [18], and PyTorch [46].\\nThe promise of AI is, however, far broader than classi-\\ncal supervised learning. Emerging AI applications must\\nincreasingly operate in dynamic environments, react to\\nchanges in the environment, and take sequences of ac-\\ntions to accomplish long-term goals [8, 43]. They must\\naim not only to exploit the data gathered, but also to ex-\\nplore the space of possible actions. These broader require-\\nments are naturally framed within the paradigm of rein-\\nforcement learning (RL). RL deals with learning to oper-\\nate continuously within an uncertain environment based\\non delayed and limited feedback [56]. RL-based systems\\nhave already yielded remarkable results, such as Google’s\\nAlphaGo beating a human world champion [54], and are\\nbeginning to ﬁnd their way into dialogue systems, UAVs\\n[42], and robotic manipulation [25, 60].\\nThe central goal of an RL application is to learn a\\npolicy—a mapping from the state of the environment to a\\nchoice of action—that yields effective performance over\\ntime, e.g., winning a game or piloting a drone. Finding ef-\\nfective policies in large-scale applications requires three\\nmain capabilities. First, RL methods often rely on simula-\\ntion to evaluate policies. Simulations make it possible to\\nexplore many different choices of action sequences and to\\nlearn about the long-term consequences of those choices.\\nSecond, like their supervised learning counterparts, RL al-\\ngorithms need to perform distributed training to improve\\nthe policy based on data generated through simulations or\\ninteractions with the physical environment. Third, poli-\\ncies are intended to provide solutions to control problems,\\nand thus it is necessary to serve the policy in interactive\\nclosed-loop and open-loop control scenarios.\\nThese characteristics drive new systems requirements:\\na system for RL must support ﬁne-grained computations\\n(e.g., rendering actions in milliseconds when interacting\\nwith the real world, and performing vast numbers of sim-\\narXiv:1712.05889v2  [cs.DC]  30 Sep 2018'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 1}, page_content='ulations), must support heterogeneity both in time (e.g.,\\na simulation may take milliseconds or hours) and in re-\\nsource usage (e.g., GPUs for training and CPUs for simu-\\nlations), and must support dynamic execution, as results\\nof simulations or interactions with the environment can\\nchange future computations. Thus, we need a dynamic\\ncomputation framework that handles millions of hetero-\\ngeneous tasks per second at millisecond-level latencies.\\nExisting frameworks that have been developed for\\nBig Data workloads or for supervised learning work-\\nloads fall short of satisfying these new requirements for\\nRL. Bulk-synchronous parallel systems such as Map-\\nReduce [20], Apache Spark [64], and Dryad [28] do not\\nsupport ﬁne-grained simulation or policy serving. Task-\\nparallel systems such as CIEL [40] and Dask [48] provide\\nlittle support for distributed training and serving. The\\nsame is true for streaming systems such as Naiad [39]\\nand Storm [31]. Distributed deep-learning frameworks\\nsuch as TensorFlow [7] and MXNet [18] do not naturally\\nsupport simulation and serving. Finally, model-serving\\nsystems such as TensorFlow Serving [6] and Clipper [19]\\nsupport neither training nor simulation.\\nWhile in principle one could develop an end-to-end so-\\nlution by stitching together several existing systems (e.g.,\\nHorovod [53] for distributed training, Clipper [19] for\\nserving, and CIEL [40] for simulation), in practice this ap-\\nproach is untenable due to the tight coupling of these com-\\nponents within applications. As a result, researchers and\\npractitioners today build one-off systems for specialized\\nRL applications [58, 41, 54, 44, 49, 5]. This approach im-\\nposes a massive systems engineering burden on the devel-\\nopment of distributed applications by essentially pushing\\nstandard systems challenges like scheduling, fault toler-\\nance, and data movement onto each application.\\nIn this paper, we propose Ray, a general-purpose\\ncluster-computing framework that enables simulation,\\ntraining, and serving for RL applications. The require-\\nments of these workloads range from lightweight and\\nstateless computations, such as for simulation, to long-\\nrunning and stateful computations, such as for training.\\nTo satisfy these requirements, Ray implements a uniﬁed\\ninterface that can express both task-parallel and actor-\\nbased computations. Tasks enable Ray to efﬁciently and\\ndynamically load balance simulations, process large in-\\nputs and state spaces (e.g., images, video), and recover\\nfrom failures. In contrast, actors enable Ray to efﬁciently\\nsupport stateful computations, such as model training, and\\nexpose shared mutable state to clients, (e.g., a parameter\\nserver). Ray implements the actor and the task abstrac-\\ntions on top of a single dynamic execution engine that is\\nhighly scalable and fault tolerant.\\nTo meet the performance requirements, Ray distributes\\ntwo components that are typically centralized in existing\\nframeworks [64, 28, 40]: (1) the task scheduler and (2) a\\nstate (si+1) \\n(observation)\\nreward (ri+1)\\naction (ai)\\nPolicy \\nimprovement\\n(e.g., SGD)\\ntrajectory: s0, (s1, r1), …, (sn, rn)\\npolicy\\nTraining\\nServing\\nSimulation\\nPolicy\\nevaluation \\nEnvironment\\nAgent\\nFigure 1: Example of an RL system.\\nmetadata store which maintains the computation lineage\\nand a directory for data objects. This allows Ray to sched-\\nule millions of tasks per second with millisecond-level\\nlatencies. Furthermore, Ray provides lineage-based fault\\ntolerance for tasks and actors, and replication-based fault\\ntolerance for the metadata store.\\nWhile Ray supports serving, training, and simulation\\nin the context of RL applications, this does not mean that\\nit should be viewed as a replacement for systems that pro-\\nvide solutions for these workloads in other contexts. In\\nparticular, Ray does not aim to substitute for serving sys-\\ntems like Clipper [19] and TensorFlow Serving [6], as\\nthese systems address a broader set of challenges in de-\\nploying models, including model management, testing,\\nand model composition. Similarly, despite its ﬂexibility,\\nRay is not a substitute for generic data-parallel frame-\\nworks, such as Spark [64], as it currently lacks the rich\\nfunctionality and APIs (e.g., straggler mitigation, query\\noptimization) that these frameworks provide.\\nWe make the following contributions:\\n• We design and build the ﬁrst distributed frame-\\nwork that uniﬁes training, simulation, and serving—\\nnecessary components of emerging RL applications.\\n• To support these workloads, we unify the actor and\\ntask-parallel abstractions on top of a dynamic task\\nexecution engine.\\n• To achieve scalability and fault tolerance, we pro-\\npose a system design principle in which control state\\nis stored in a sharded metadata store and all other\\nsystem components are stateless.\\n• To achieve scalability, we propose a bottom-up dis-\\ntributed scheduling strategy.\\n2\\nMotivation and Requirements\\nWe begin by considering the basic components of an RL\\nsystem and ﬂeshing out the key requirements for Ray. As\\nshown in Figure 1, in an RL setting, an agent interacts\\nrepeatedly with the environment. The goal of the agent\\nis to learn a policy that maximizes a reward. A policy is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 2}, page_content='// evaluate policy by interacting with env. (e.g., simulator)\\nrollout(policy, environment):\\ntrajectory = []\\nstate = environment.initial_state()\\nwhile (not environment.has_terminated()):\\naction = policy.compute(state) // Serving\\nstate, reward = environment.step(action) // Simulation\\ntrajectory.append(state, reward)\\nreturn trajectory\\n// improve policy iteratively until it converges\\ntrain_policy(environment):\\npolicy = initial_policy()\\nwhile (policy has not converged):\\ntrajectories = []\\nfor i from 1 to k:\\n// evaluate policy by generating k rollouts\\ntrajectories.append(rollout(policy, environment))\\n// improve policy\\npolicy = policy.update(trajectories) // Training\\nreturn policy\\nFigure 2: Typical RL pseudocode for learning a policy.\\na mapping from the state of the environment to a choice\\nof action. The precise deﬁnitions of environment, agent,\\nstate, action, and reward are application-speciﬁc.\\nTo learn a policy, an agent typically employs a two-step\\nprocess: (1) policy evaluation and (2) policy improvement.\\nTo evaluate the policy, the agent interacts with the envi-\\nronment (e.g., with a simulation of the environment) to\\ngenerate trajectories, where a trajectory consists of a se-\\nquence of (state, reward) tuples produced by the current\\npolicy. Then, the agent uses these trajectories to improve\\nthe policy; i.e., to update the policy in the direction of the\\ngradient that maximizes the reward. Figure 2 shows an\\nexample of the pseudocode used by an agent to learn a\\npolicy. This pseudocode evaluates the policy by invok-\\ning rollout(environment, policy) to generate trajectories.\\ntrain policy() then uses these trajectories to improve the\\ncurrent policy via policy.update(trajectories). This pro-\\ncess repeats until the policy converges.\\nThus, a framework for RL applications must provide\\nefﬁcient support for training, serving, and simulation\\n(Figure 1). Next, we brieﬂy describe these workloads.\\nTraining typically involves running stochastic gradient\\ndescent (SGD), often in a distributed setting, to update the\\npolicy. Distributed SGD typically relies on an allreduce\\naggregation step or a parameter server [32].\\nServing uses the trained policy to render an action based\\non the current state of the environment. A serving system\\naims to minimize latency, and maximize the number of\\ndecisions per second. To scale, load is typically balanced\\nacross multiple nodes serving the policy.\\nFinally, most existing RL applications use simulations\\nto evaluate the policy—current RL algorithms are not\\nsample-efﬁcient enough to rely solely on data obtained\\nfrom interactions with the physical world. These simula-\\ntions vary widely in complexity. They might take a few ms\\n(e.g., simulate a move in a chess game) to minutes (e.g.,\\nsimulate a realistic environment for a self-driving car).\\nIn contrast with supervised learning, in which train-\\ning and serving can be handled separately by different\\nsystems, in RL all three of these workloads are tightly\\ncoupled in a single application, with stringent latency re-\\nquirements between them. Currently, no framework sup-\\nports this coupling of workloads. In theory, multiple spe-\\ncialized frameworks could be stitched together to provide\\nthe overall capabilities, but in practice, the resulting data\\nmovement and latency between systems is prohibitive in\\nthe context of RL. As a result, researchers and practition-\\ners have been building their own one-off systems.\\nThis state of affairs calls for the development of new\\ndistributed frameworks for RL that can efﬁciently support\\ntraining, serving, and simulation. In particular, such a\\nframework should satisfy the following requirements:\\nFine-grained, heterogeneous computations. The dura-\\ntion of a computation can range from milliseconds (e.g.,\\ntaking an action) to hours (e.g., training a complex pol-\\nicy). Additionally, training often requires heterogeneous\\nhardware (e.g., CPUs, GPUs, or TPUs).\\nFlexible computation model. RL applications require\\nboth stateless and stateful computations. Stateless compu-\\ntations can be executed on any node in the system, which\\nmakes it easy to achieve load balancing and movement\\nof computation to data, if needed. Thus stateless com-\\nputations are a good ﬁt for ﬁne-grained simulation and\\ndata processing, such as extracting features from images\\nor videos. In contrast stateful computations are a good ﬁt\\nfor implementing parameter servers, performing repeated\\ncomputation on GPU-backed data, or running third-party\\nsimulators that do not expose their state.\\nDynamic execution. Several components of RL appli-\\ncations require dynamic execution, as the order in which\\ncomputations ﬁnish is not always known in advance (e.g.,\\nthe order in which simulations ﬁnish), and the results of a\\ncomputation can determine future computations (e.g., the\\nresults of a simulation will determine whether we need to\\nperform more simulations).\\nWe make two ﬁnal comments. First, to achieve high\\nutilization in large clusters, such a framework must handle\\nmillions of tasks per second.∗Second, such a framework\\nis not intended for implementing deep neural networks\\nor complex simulators from scratch. Instead, it should\\nenable seamless integration with existing simulators [13,\\n11, 59] and deep learning frameworks [7, 18, 46, 29].\\n∗Assume 5ms single-core tasks and a cluster of 200 32-core nodes.\\nThis cluster can run (1s/5ms)×32×200 = 1.28M tasks/sec.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 3}, page_content='Name\\nDescription\\nfutures = f.remote(args)\\nExecute function f remotely. f.remote() can take objects or futures as inputs\\nand returns one or more futures. This is non-blocking.\\nobjects = ray.get(futures)\\nReturn the values associated with one or more futures. This is blocking.\\nready futures = ray.wait(futures,k,timeout)\\nReturn the futures whose corresponding tasks have completed as soon as either\\nk have completed or the timeout expires.\\nactor = Class.remote(args)\\nInstantiate class Class as a remote actor, and return a handle to it. Call a method\\nfutures = actor.method.remote(args)\\non the remote actor and return one or more futures. Both are non-blocking.\\nTable 1: Ray API\\n3\\nProgramming and Computation Model\\nRay implements a dynamic task graph computation\\nmodel, i.e., it models an application as a graph of depen-\\ndent tasks that evolves during execution. On top of this\\nmodel, Ray provides both an actor and a task-parallel\\nprogramming abstraction. This uniﬁcation differentiates\\nRay from related systems like CIEL, which only pro-\\nvides a task-parallel abstraction, and from Orleans [14] or\\nAkka [1], which primarily provide an actor abstraction.\\n3.1\\nProgramming Model\\nTasks. A task represents the execution of a remote func-\\ntion on a stateless worker. When a remote function is\\ninvoked, a future representing the result of the task is\\nreturned immediately. Futures can be retrieved using\\nray.get() and passed as arguments into other remote func-\\ntions without waiting for their result. This allows the user\\nto express parallelism while capturing data dependencies.\\nTable 1 shows Ray’s API.\\nRemote functions operate on immutable objects and\\nare expected to be stateless and side-effect free: their\\noutputs are determined solely by their inputs. This implies\\nidempotence, which simpliﬁes fault tolerance through\\nfunction re-execution on failure.\\nActors. An actor represents a stateful computation. Each\\nactor exposes methods that can be invoked remotely and\\nare executed serially. A method execution is similar to a\\ntask, in that it executes remotely and returns a future, but\\ndiffers in that it executes on a stateful worker. A handle\\nto an actor can be passed to other actors or tasks, making\\nit possible for them to invoke methods on that actor.\\nTasks (stateless)\\nActors (stateful)\\nFine-grained load balancing\\nCoarse-grained load balancing\\nSupport for object locality\\nPoor locality support\\nHigh overhead for small updates\\nLow overhead for small updates\\nEfﬁcient failure handling\\nOverhead from checkpointing\\nTable 2: Tasks vs. actors tradeoffs.\\nTable 2 summarizes the properties of tasks and actors.\\nTasks enable ﬁne-grained load balancing through leverag-\\ning load-aware scheduling at task granularity, input data\\nlocality, as each task can be scheduled on the node stor-\\ning its inputs, and low recovery overhead, as there is no\\nneed to checkpoint and recover intermediate state. In con-\\ntrast, actors provide much more efﬁcient ﬁne-grained up-\\ndates, as these updates are performed on internal rather\\nthan external state, which typically requires serialization\\nand deserialization. For example, actors can be used to\\nimplement parameter servers [32] and GPU-based itera-\\ntive computations (e.g., training). In addition, actors can\\nbe used to wrap third-party simulators and other opaque\\nhandles that are hard to serialize.\\nTo satisfy the requirements for heterogeneity and ﬂex-\\nibility (Section 2), we augment the API in three ways.\\nFirst, to handle concurrent tasks with heterogeneous du-\\nrations, we introduce ray.wait(), which waits for the\\nﬁrst k available results, instead of waiting for all results\\nlike ray.get(). Second, to handle resource-heterogeneous\\ntasks, we enable developers to specify resource require-\\nments so that the Ray scheduler can efﬁciently manage re-\\nsources. Third, to improve ﬂexibility, we enable nested re-\\nmote functions, meaning that remote functions can invoke\\nother remote functions. This is also critical for achiev-\\ning high scalability (Section 4), as it enables multiple pro-\\ncesses to invoke remote functions in a distributed fashion.\\n3.2\\nComputation Model\\nRay employs a dynamic task graph computation\\nmodel [21], in which the execution of both remote func-\\ntions and actor methods is automatically triggered by the\\nsystem when their inputs become available. In this sec-\\ntion, we describe how the computation graph (Figure 4) is\\nconstructed from a user program (Figure 3). This program\\nuses the API in Table 1 to implement the pseudocode\\nfrom Figure 2.\\nIgnoring actors ﬁrst, there are two types of nodes in\\na computation graph: data objects and remote function\\ninvocations, or tasks. There are also two types of edges:\\ndata edges and control edges. Data edges capture the de-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 4}, page_content='@ray.remote\\ndef create_policy():\\n# Initialize the policy randomly.\\nreturn policy\\n@ray.remote(num_gpus=1)\\nclass Simulator(object):\\ndef __init__(self):\\n# Initialize the environment.\\nself.env = Environment()\\ndef rollout(self, policy, num_steps):\\nobservations = []\\nobservation = self.env.current_state()\\nfor _ in range(num_steps):\\naction = policy(observation)\\nobservation = self.env.step(action)\\nobservations.append(observation)\\nreturn observations\\n@ray.remote(num_gpus=2)\\ndef update_policy(policy, *rollouts):\\n# Update the policy.\\nreturn policy\\n@ray.remote\\ndef train_policy():\\n# Create a policy.\\npolicy_id = create_policy.remote()\\n# Create 10 actors.\\nsimulators = [Simulator.remote() for _ in range(10)]\\n# Do 100 steps of training.\\nfor _ in range(100):\\n# Perform one rollout on each actor.\\nrollout_ids = [s.rollout.remote(policy_id)\\nfor s in simulators]\\n# Update the policy with the rollouts.\\npolicy_id =\\nupdate_policy.remote(policy_id, *rollout_ids)\\nreturn ray.get(policy_id)\\nFigure 3: Python code implementing the example in Figure 2\\nin Ray. Note that @ray.remote indicates remote functions and\\nactors. Invocations of remote functions and actor methods return\\nfutures, which can be passed to subsequent remote functions or\\nactor methods to encode task dependencies. Each actor has an\\nenvironment object self.env shared between all of its methods.\\npendencies between data objects and tasks. More pre-\\ncisely, if data object D is an output of task T, we add a\\ndata edge from T to D. Similarly, if D is an input to T,\\nwe add a data edge from D to T. Control edges capture\\nthe computation dependencies that result from nested re-\\nmote functions (Section 3.1): if task T1 invokes task T2,\\nthen we add a control edge from T1 to T2.\\nActor method invocations are also represented as nodes\\nin the computation graph. They are identical to tasks\\nwith one key difference. To capture the state dependency\\nacross subsequent method invocations on the same actor,\\nwe add a third type of edge: a stateful edge. If method\\nMj is called right after method Mi on the same actor,\\nthen we add a stateful edge from Mi to Mj. Thus, all\\npolicy1\\nT1\\ncreate_policy\\nT2\\nupdate_policy\\nA11\\nrollout\\nA12\\nrollout\\npolicy2\\nT3\\nupdate_policy\\nrollout11\\nrollout12\\nA21\\nrollout\\nA22\\nrollout\\nrollout22\\nA10\\nSimulator\\nA20\\nSimulator\\n…\\n…\\n…\\ndata edges\\nstateful edges\\nobject\\ntask/method\\ncontrol edges\\nrollout21\\nT0\\ntrain_policy\\nFigure 4: The task graph corresponding to an invocation of\\ntrain policy.remote() in Figure 3. Remote function calls and the\\nactor method calls correspond to tasks in the task graph. The\\nﬁgure shows two actors. The method invocations for each actor\\n(the tasks labeled A1i and A2i) have stateful edges between them\\nindicating that they share the mutable actor state. There are con-\\ntrol edges from train policy to the tasks that it invokes. To train\\nmultiple policies in parallel, we could call train policy.remote()\\nmultiple times.\\nmethods invoked on the same actor object form a chain\\nthat is connected by stateful edges (Figure 4). This chain\\ncaptures the order in which these methods were invoked.\\nStateful edges help us embed actors in an otherwise\\nstateless task graph, as they capture the implicit data de-\\npendency between successive method invocations sharing\\nthe internal state of an actor. Stateful edges also enable\\nus to maintain lineage. As in other dataﬂow systems [64],\\nwe track data lineage to enable reconstruction. By explic-\\nitly including stateful edges in the lineage graph, we can\\neasily reconstruct lost data, whether produced by remote\\nfunctions or actor methods (Section 4.2.3).\\n4\\nArchitecture\\nRay’s architecture comprises (1) an application layer im-\\nplementing the API, and (2) a system layer providing high\\nscalability and fault tolerance.\\n4.1\\nApplication Layer\\nThe application layer consists of three types of processes:\\n• Driver: A process executing the user program.\\n• Worker: A stateless process that executes tasks\\n(remote functions) invoked by a driver or another'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 5}, page_content='Local Scheduler\\nActor\\nDriver\\nObject Store\\nGlobal \\nScheduler\\nGlobal \\nScheduler\\nObject Table\\nTask Table\\nFunction Table\\nEvent Logs\\nGlobal Control Store (GCS)\\nLocal Scheduler\\nDriver\\nWorker\\nObject Store\\nNode\\nGlobal \\nScheduler\\nWeb UI\\nDebugging \\nTools\\nProfiling Tools\\nError Diagnosis\\nLocal Scheduler\\nWorker\\nWorker\\nObject Store\\nNode\\nNode\\nApp Layer\\nSystem Layer (backend)\\nFigure 5: Ray’s architecture consists of two parts: an applica-\\ntion layer and a system layer. The application layer implements\\nthe API and the computation model described in Section 3, the\\nsystem layer implements task scheduling and data management\\nto satisfy the performance and fault-tolerance requirements.\\nworker. Workers are started automatically and as-\\nsigned tasks by the system layer. When a remote\\nfunction is declared, the function is automatically\\npublished to all workers. A worker executes tasks\\nserially, with no local state maintained across tasks.\\n• Actor: A stateful process that executes, when in-\\nvoked, only the methods it exposes. Unlike a worker,\\nan actor is explicitly instantiated by a worker or a\\ndriver. Like workers, actors execute methods seri-\\nally, except that each method depends on the state\\nresulting from the previous method execution.\\n4.2\\nSystem Layer\\nThe system layer consists of three major components: a\\nglobal control store, a distributed scheduler, and a dis-\\ntributed object store. All components are horizontally\\nscalable and fault-tolerant.\\n4.2.1\\nGlobal Control Store (GCS)\\nThe global control store (GCS) maintains the entire con-\\ntrol state of the system, and it is a unique feature of our\\ndesign. At its core, GCS is a key-value store with pub-\\nsub functionality. We use sharding to achieve scale, and\\nper-shard chain replication [61] to provide fault tolerance.\\nThe primary reason for the GCS and its design is to main-\\ntain fault tolerance and low latency for a system that can\\ndynamically spawn millions of tasks per second.\\nFault tolerance in case of node failure requires a solu-\\ntion to maintain lineage information. Existing lineage-\\nbased solutions [64, 63, 40, 28] focus on coarse-grained\\nparallelism and can therefore use a single node (e.g., mas-\\nter, driver) to store the lineage without impacting perfor-\\nmance. However, this design is not scalable for a ﬁne-\\ngrained and dynamic workload like simulation. Therefore,\\nwe decouple the durable lineage storage from the other\\nsystem components, allowing each to scale independently.\\nMaintaining low latency requires minimizing over-\\nheads in task scheduling, which involves choosing where\\nto execute, and subsequently task dispatch, which in-\\nvolves retrieving remote inputs from other nodes. Many\\nexisting dataﬂow systems [64, 40, 48] couple these by\\nstoring object locations and sizes in a centralized sched-\\nuler, a natural design when the scheduler is not a bottle-\\nneck. However, the scale and granularity that Ray targets\\nrequires keeping the centralized scheduler off the critical\\npath. Involving the scheduler in each object transfer is pro-\\nhibitively expensive for primitives important to distributed\\ntraining like allreduce, which is both communication-\\nintensive and latency-sensitive. Therefore, we store the\\nobject metadata in the GCS rather than in the scheduler,\\nfully decoupling task dispatch from task scheduling.\\nIn summary, the GCS signiﬁcantly simpliﬁes Ray’s\\noverall design, as it enables every component in the sys-\\ntem to be stateless. This not only simpliﬁes support for\\nfault tolerance (i.e., on failure, components simply restart\\nand read the lineage from the GCS), but also makes it\\neasy to scale the distributed object store and scheduler in-\\ndependently, as all components share the needed state via\\nthe GCS. An added beneﬁt is the easy development of de-\\nbugging, proﬁling, and visualization tools.\\n4.2.2\\nBottom-Up Distributed Scheduler\\nAs discussed in Section 2, Ray needs to dynamically\\nschedule millions of tasks per second, tasks which may\\ntake as little as a few milliseconds. None of the clus-\\nter schedulers we are aware of meet these requirements.\\nMost cluster computing frameworks, such as Spark [64],\\nCIEL [40], and Dryad [28] implement a centralized sched-\\nuler, which can provide locality but at latencies in the tens\\nof ms. Distributed schedulers such as work stealing [12],\\nSparrow [45] and Canary [47] can achieve high scale, but\\nthey either don’t consider data locality [12], or assume\\ntasks belong to independent jobs [45], or assume the com-\\nputation graph is known [47].\\nTo satisfy the above requirements, we design a two-\\nlevel hierarchical scheduler consisting of a global sched-\\nuler and per-node local schedulers. To avoid overloading\\nthe global scheduler, the tasks created at a node are sub-\\nmitted ﬁrst to the node’s local scheduler. A local sched-\\nuler schedules tasks locally unless the node is overloaded\\n(i.e., its local task queue exceeds a predeﬁned threshold),\\nor it cannot satisfy a task’s requirements (e.g., lacks a\\nGPU). If a local scheduler decides not to schedule a task\\nlocally, it forwards it to the global scheduler. Since this\\nscheduler attempts to schedule tasks locally ﬁrst (i.e., at\\nthe leaves of the scheduling hierarchy), we call it a bottom-\\nup scheduler.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 6}, page_content='Global \\nScheduler\\nLocal Scheduler\\nGlobal \\nScheduler\\nWorker\\nDriver\\nWorker\\n…\\nGlobal Control        State (GCS)\\nLocal Scheduler\\nWorker\\nWorker\\nWorker\\nSubmit \\ntasks\\nSchedule \\ntasks\\nLoad\\ninfo\\nNode 1\\nNode N\\nFigure 6: Bottom-up distributed scheduler. Tasks are submitted\\nbottom-up, from drivers and workers to a local scheduler and\\nforwarded to the global scheduler only if needed (Section 4.2.2).\\nThe thickness of each arrow is proportional to its request rate.\\nThe global scheduler considers each node’s load and\\ntask’s constraints to make scheduling decisions. More pre-\\ncisely, the global scheduler identiﬁes the set of nodes that\\nhave enough resources of the type requested by the task,\\nand of these nodes selects the node which provides the\\nlowest estimated waiting time. At a given node, this time\\nis the sum of (i) the estimated time the task will be queued\\nat that node (i.e., task queue size times average task ex-\\necution), and (ii) the estimated transfer time of task’s\\nremote inputs (i.e., total size of remote inputs divided by\\naverage bandwidth). The global scheduler gets the queue\\nsize at each node and the node resource availability via\\nheartbeats, and the location of the task’s inputs and their\\nsizes from GCS. Furthermore, the global scheduler com-\\nputes the average task execution and the average transfer\\nbandwidth using simple exponential averaging. If the\\nglobal scheduler becomes a bottleneck, we can instantiate\\nmore replicas all sharing the same information via GCS.\\nThis makes our scheduler architecture highly scalable.\\n4.2.3\\nIn-Memory Distributed Object Store\\nTo minimize task latency, we implement an in-memory\\ndistributed storage system to store the inputs and outputs\\nof every task, or stateless computation. On each node, we\\nimplement the object store via shared memory. This al-\\nlows zero-copy data sharing between tasks running on the\\nsame node. As a data format, we use Apache Arrow [2].\\nIf a task’s inputs are not local, the inputs are replicated\\nto the local object store before execution. Also, a task\\nwrites its outputs to the local object store. Replication\\neliminates the potential bottleneck due to hot data ob-\\njects and minimizes task execution time as a task only\\nreads/writes data from/to the local memory. This in-\\ncreases throughput for computation-bound workloads, a\\nproﬁle shared by many AI applications. For low latency,\\nwe keep objects entirely in memory and evict them as\\nneeded to disk using an LRU policy.\\nAs with existing cluster computing frameworks, such\\nas Spark [64], and Dryad [28], the object store is limited\\nto immutable data. This obviates the need for complex\\nconsistency protocols (as objects are not updated), and\\nsimpliﬁes support for fault tolerance. In the case of node\\nfailure, Ray recovers any needed objects through lineage\\nre-execution. The lineage stored in the GCS tracks both\\nstateless tasks and stateful actors during initial execution;\\nwe use the former to reconstruct objects in the store.\\nFor simplicity, our object store does not support dis-\\ntributed objects, i.e., each object ﬁts on a single node. Dis-\\ntributed objects like large matrices or trees can be imple-\\nmented at the application level as collections of futures.\\n4.2.4\\nImplementation\\nRay is an active open source project† developed at the Uni-\\nversity of California, Berkeley. Ray fully integrates with\\nthe Python environment and is easy to install by simply\\nrunning pip install ray. The implementation com-\\nprises ≈40K lines of code (LoC), 72% in C++ for the\\nsystem layer, 28% in Python for the application layer. The\\nGCS uses one Redis [50] key-value store per shard, with\\nentirely single-key operations. GCS tables are sharded\\nby object and task IDs to scale, and every shard is chain-\\nreplicated [61] for fault tolerance. We implement both\\nthe local and global schedulers as event-driven, single-\\nthreaded processes. Internally, local schedulers maintain\\ncached state for local object metadata, tasks waiting for\\ninputs, and tasks ready for dispatch to a worker. To trans-\\nfer large objects between different object stores, we stripe\\nthe object across multiple TCP connections.\\n4.3\\nPutting Everything Together\\nFigure 7 illustrates how Ray works end-to-end with a\\nsimple example that adds two objects a and b, which\\ncould be scalars or matrices, and returns result c. The\\nremote function add() is automatically registered with the\\nGCS upon initialization and distributed to every worker\\nin the system (step 0 in Figure 7a).\\nFigure 7a shows the step-by-step operations triggered\\nby a driver invoking add.remote(a,b), where a and b are\\nstored on nodes N1 and N2, respectively. The driver sub-\\nmits add(a, b) to the local scheduler (step 1), which for-\\nwards it to a global scheduler (step 2).‡ Next, the global\\nscheduler looks up the locations of add(a, b)’s arguments\\nin the GCS (step 3) and decides to schedule the task on\\nnode N2, which stores argument b (step 4). The local\\nscheduler at node N2 checks whether the local object\\nstore contains add(a, b)’s arguments (step 5). Since the\\n†https://github.com/ray-project/ray\\n‡Note that N1 could also decide to schedule the task locally.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 7}, page_content='Object store\\n@ray.remote\\ndef add(a, b):\\nreturn a + b\\n@ray.remote\\ndef add(a, b):\\nreturn a + b\\nidc = add.remote(a, b)\\nc = ray.get(idc)\\nN1\\nDriver \\nObject Table\\nFunction Table\\n@ray.remote\\ndef add(a, b):\\nreturn a + b\\nN2\\nWorker \\nida\\nN1\\nidb\\nN2\\nGlobal Control Store (GCS)\\n4\\n5\\n6\\nLocal Scheduler\\nObject store\\nida a\\n1\\n2\\n8\\nGlobal Scheduler\\n7\\n9\\nida a\\nidb b\\n0\\n3\\nLocal Scheduler\\n(a) Executing a task remotely\\nLocal Scheduler\\nidb b\\n@ray.remote\\ndef add(a, b):\\nreturn a + b\\n@ray.remote\\ndef add(a, b):\\nreturn a + b\\nidc = add.remote(a, b)\\nc = ray.get(idc)\\nN1\\nDriver \\nObject Table\\nFunction Table\\n@ray.remote\\ndef add(a, b):\\nreturn a + b\\nN2\\nWorker \\nida\\nN1\\nidb\\nN2\\nGlobal Control Store (GCS)\\nLocal Scheduler\\nida a\\n1\\nida a\\nidc c\\nidc\\nN2, N1\\n4\\n2\\n5\\n7\\n3\\nGlobal Scheduler\\nidc c\\n6\\n(b) Returning the result of a remote task\\nFigure 7: An end-to-end example that adds a and b and returns\\nc. Solid lines are data plane operations and dotted lines are\\ncontrol plane operations. (a) The function add() is registered\\nwith the GCS by node 1 (N1), invoked on N1, and executed\\non N2. (b) N1 gets add()’s result using ray.get(). The Object\\nTable entry for c is created in step 4 and updated in step 6 after\\nc is copied to N1.\\nlocal store doesn’t have object a, it looks up a’s location\\nin the GCS (step 6). Learning that a is stored at N1, N2’s\\nobject store replicates it locally (step 7). As all arguments\\nof add() are now stored locally, the local scheduler in-\\nvokes add() at a local worker (step 8), which accesses the\\narguments via shared memory (step 9).\\nFigure 7b shows the step-by-step operations triggered\\nby the execution of ray.get() at N1, and of add() at N2,\\nrespectively. Upon ray.get(idc)’s invocation, the driver\\nchecks the local object store for the value c, using the\\nfuture idc returned by add() (step 1). Since the local\\nobject store doesn’t store c, it looks up its location in the\\nGCS. At this time, there is no entry for c, as c has not\\nbeen created yet. As a result, N1’s object store registers a\\ncallback with the Object Table to be triggered when c’s\\nentry has been created (step 2). Meanwhile, at N2, add()\\ncompletes its execution, stores the result c in the local\\nobject store (step 3), which in turn adds c’s entry to the\\nGCS (step 4). As a result, the GCS triggers a callback\\nto N1’s object store with c’s entry (step 5). Next, N1\\nreplicates c from N2 (step 6), and returns c to ray.get()\\n(step 7), which ﬁnally completes the task.\\nWhile this example involves a large number of RPCs,\\n100KB\\n1MB\\n10MB 100MB\\nObject size\\n10-5\\n10-4\\n10-3\\n10-2\\n10-1\\nMean task latency (s)\\nLocality Aware\\nUnaware\\n(a) Ray locality scheduling\\n10 20 30 40 50 60\\n100\\nnumber of nodes\\n0.0\\n0.4\\n0.8\\n1.2\\n1.6\\nMillions of tasks/s\\n(b) Ray scalability\\nFigure 8: (a) Tasks leverage locality-aware placement. 1000\\ntasks with a random object dependency are scheduled onto one\\nof two nodes. With locality-aware policy, task latency remains\\nindependent of the size of task inputs instead of growing by 1-2\\norders of magnitude. (b) Near-linear scalability leveraging the\\nGCS and bottom-up distributed scheduler. Ray reaches 1 million\\ntasks per second throughput with 60 nodes. x ∈{70,80,90}\\nomitted due to cost.\\nin many cases this number is much smaller, as most tasks\\nare scheduled locally, and the GCS replies are cached by\\nthe global and local schedulers.\\n5\\nEvaluation\\nIn our evaluation, we study the following questions:\\n1. How well does Ray meet the latency, scalability,\\nand fault tolerance requirements listed in Section 2?\\n(Section 5.1)\\n2. What overheads are imposed on distributed primi-\\ntives (e.g., allreduce) written using Ray’s API? (Sec-\\ntion 5.1)\\n3. In the context of RL workloads, how does Ray com-\\npare against specialized systems for training, serv-\\ning, and simulation? (Section 5.2)\\n4. What advantages does Ray provide for RL applica-\\ntions, compared to custom systems? (Section 5.3)\\nAll experiments were run on Amazon Web Services.\\nUnless otherwise stated, we use m4.16xlarge CPU in-\\nstances and p3.16xlarge GPU instances.\\n5.1\\nMicrobenchmarks\\nLocality-aware task placement. Fine-grain load bal-\\nancing and locality-aware placement are primary beneﬁts\\nof tasks in Ray. Actors, once placed, are unable to move\\ntheir computation to large remote objects, while tasks can.\\nIn Figure 8a, tasks placed without data locality awareness\\n(as is the case for actor methods), suffer 1-2 orders of\\nmagnitude latency increase at 10-100MB input data sizes.\\nRay uniﬁes tasks and actors through the shared object\\nstore, allowing developers to use tasks for e.g., expensive\\npostprocessing on output produced by simulation actors.\\nEnd-to-end scalability. One of the key beneﬁts of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 8}, page_content='1KB 10KB100KB 1MB 10MB100MB 1GB\\nobject size\\n0\\n5000\\n10000\\n15000\\n20000\\nIOPS\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nthroughput (GB/s)\\nFigure 9:\\nObject store write throughput and IOPS. From a\\nsingle client, throughput exceeds 15GB/s (red) for large objects\\nand 18K IOPS (cyan) for small objects on a 16 core instance\\n(m4.4xlarge). It uses 8 threads to copy objects larger than 0.5MB\\nand 1 thread for small objects. Bar plots report throughput with\\n1, 2, 4, 8, 16 threads. Results are averaged over 5 runs.\\nthe Global Control Store (GCS) and the bottom-up dis-\\ntributed scheduler is the ability to horizontally scale the\\nsystem to support a high throughput of ﬁne-grained tasks,\\nwhile maintaining fault tolerance and low-latency task\\nscheduling. In Figure 8b, we evaluate this ability on an\\nembarrassingly parallel workload of empty tasks, increas-\\ning the cluster size on the x-axis. We observe near-perfect\\nlinearity in progressively increasing task throughput. Ray\\nexceeds 1 million tasks per second throughput at 60 nodes\\nand continues to scale linearly beyond 1.8 million tasks\\nper second at 100 nodes. The rightmost datapoint shows\\nthat Ray can process 100 million tasks in less than a\\nminute (54s), with minimum variability. As expected, in-\\ncreasing task duration reduces throughput proportionally\\nto mean task duration, but the overall scalability remains\\nlinear. While many realistic workloads may exhibit more\\nlimited scalability due to object dependencies and inher-\\nent limits to application parallelism, this demonstrates the\\nscalability of our overall architecture under high load.\\nObject store performance. To evaluate the perfor-\\nmance of the object store (Section 4.2.3), we track two\\nmetrics: IOPS (for small objects) and write throughput\\n(for large objects). In Figure 9, the write throughput from\\na single client exceeds 15GB/s as object size increases.\\nFor larger objects, memcpy dominates object creation\\ntime. For smaller objects, the main overheads are in seri-\\nalization and IPC between the client and object store.\\nGCS fault tolerance. To maintain low latency while\\nproviding strong consistency and fault tolerance, we build\\na lightweight chain replication [61] layer on top of Redis.\\nFigure 10a simulates recording Ray tasks to and reading\\ntasks from the GCS, where keys are 25 bytes and values\\nare 512 bytes. The client sends requests as fast as it can,\\nhaving at most one in-ﬂight request at a time. Failures are\\nreported to the chain master either from the client (having\\nreceived explicit errors, or timeouts despite retries) or\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nTime since start (s)\\n103\\n103\\n104\\n104\\nLatency (μs)\\nwrite\\nread\\nnode dead\\n(a) A timeline for GCS read and write latencies as viewed from\\na client submitting tasks. The chain starts with 2 replicas. We\\nmanually trigger reconﬁguration as follows. At t ≈4.2s, a chain\\nmember is killed; immediately after, a new chain member joins,\\ninitiates state transfer, and restores the chain to 2-way replication.\\nThe maximum client-observed latency is under 30ms despite\\nreconﬁgurations.\\n0\\n10000\\n20000\\n30000\\n40000\\n50000\\n60000\\nElasped Time (seconds)\\n0\\n2000\\n4000\\n6000\\n8000\\nGCS Used Memory (MB)\\n50 million no-op tasks\\nRay, no GCS flush\\nRay, GCS flush\\n(b) The Ray GCS maintains a constant memory footprint with\\nGCS ﬂushing. Without GCS ﬂushing, the memory footprint\\nreaches a maximum capacity and the workload fails to complete\\nwithin a predetermined duration (indicated by the red cross).\\nFigure 10: Ray GCS fault tolerance and ﬂushing.\\nfrom any server in the chain (having received explicit\\nerrors). Overall, reconﬁgurations caused a maximum\\nclient-observed delay of under 30ms (this includes both\\nfailure detection and recovery delays).\\nGCS ﬂushing. Ray is equipped to periodically ﬂush\\nthe contents of GCS to disk. In Figure 10b we submit 50\\nmillion empty tasks sequentially and monitor GCS mem-\\nory consumption. As expected, it grows linearly with the\\nnumber of tasks tracked and eventually reaches the mem-\\nory capacity of the system. At that point, the system be-\\ncomes stalled and the workload fails to ﬁnish within a rea-\\nsonable amount of time. With periodic GCS ﬂushing, we\\nachieve two goals. First, the memory footprint is capped\\nat a user-conﬁgurable level (in the microbenchmark we\\nemploy an aggressive strategy where consumed memory\\nis kept as low as possible). Second, the ﬂushing mecha-\\nnism provides a natural way to snapshot lineage to disk\\nfor long-running Ray applications.\\nRecovering from task failures. In Figure 11a, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 9}, page_content='0\\n50\\n100\\n150\\n200\\nTime since start (s)\\n0\\n500\\n1000\\n1500\\n2000\\nThroughput (tasks/s)\\n0\\n20\\n40\\n60\\nNumber of nodes\\nOriginal tasks\\nRe-executed tasks\\n(a) Task reconstruction\\n100\\n200\\n300\\n400\\n500\\n600\\nTime since start (s)\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\nThroughput (tasks/s)\\nOriginal tasks\\nRe-executed tasks\\nCheckpoint tasks\\n(b) Actor reconstruction\\nFigure 11: Ray fault-tolerance. (a) Ray reconstructs lost task\\ndependencies as nodes are removed (dotted line), and recovers\\nto original throughput when nodes are added back. Each task\\nis 100ms and depends on an object generated by a previously\\nsubmitted task. (b) Actors are reconstructed from their last\\ncheckpoint. At t = 200s, we kill 2 of the 10 nodes, causing 400\\nof the 2000 actors in the cluster to be recovered on the remaining\\nnodes (t = 200–270s).\\ndemonstrate Ray’s ability to transparently recover from\\nworker node failures and elastically scale, using the\\ndurable GCS lineage storage. The workload, run on\\nm4.xlarge instances, consists of linear chains of 100ms\\ntasks submitted by the driver. As nodes are removed (at\\n25s, 50s, 100s), the local schedulers reconstruct previous\\nresults in the chain in order to continue execution. Over-\\nall per-node throughput remains stable throughout.\\nRecovering from actor failures. By encoding actor\\nmethod calls as stateful edges directly in the dependency\\ngraph, we can reuse the same object reconstruction mech-\\nanism as in Figure 11a to provide transparent fault tol-\\nerance for stateful computation. Ray additionally lever-\\nages user-deﬁned checkpoint functions to bound the re-\\nconstruction time for actors (Figure 11b). With minimal\\noverhead, checkpointing enables only 500 methods to be\\nre-executed, versus 10k re-executions without checkpoint-\\ning. In the future, we hope to further reduce actor recon-\\nstruction time, e.g., by allowing users to annotate meth-\\nods that do not mutate state.\\nAllreduce. Allreduce is a distributed communication\\n10MB\\n100MB\\n1GB\\nObject size\\n100\\n101\\n102\\n103\\n104\\nIteration time (milliseconds)\\nOpenMPI\\nRay*\\nRay\\n(a) Ray vs OpenMPI\\n+0\\n+1\\n+5\\n+10\\nAdded scheduler latency (ms\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\nIteration time (milliseconds)\\nRay ring reduce latency\\n(16 nodes, 100MB)\\n(b) Ray scheduler ablation\\nFigure 12: (a) Mean execution time of allreduce on 16 m4.16xl\\nnodes. Each worker runs on a distinct node. Ray* restricts Ray\\nto 1 thread for sending and 1 thread for receiving. (b) Ray’s low-\\nlatency scheduling is critical for allreduce.\\nprimitive important to many machine learning workloads.\\nHere, we evaluate whether Ray can natively support a\\nring allreduce [57] implementation with low enough over-\\nhead to match existing implementations [53]. We ﬁnd that\\nRay completes allreduce across 16 nodes on 100MB in\\n∼200ms and 1GB in ∼1200ms, surprisingly outperform-\\ning OpenMPI (v1.10), a popular MPI implementation,\\nby 1.5× and 2× respectively (Figure 12a). We attribute\\nRay’s performance to its use of multiple threads for net-\\nwork transfers, taking full advantage of the 25Gbps con-\\nnection between nodes on AWS, whereas OpenMPI se-\\nquentially sends and receives data on a single thread [22].\\nFor smaller objects, OpenMPI outperforms Ray by switch-\\ning to a lower overhead algorithm, an optimization we\\nplan to implement in the future.\\nRay’s scheduler performance is critical to implement-\\ning primitives such as allreduce. In Figure 12b, we inject\\nartiﬁcial task execution delays and show that performance\\ndrops nearly 2× with just a few ms of extra latency. Sys-\\ntems with centralized schedulers like Spark and CIEL typ-\\nically have scheduler overheads in the tens of millisec-\\nonds [62, 38], making such workloads impractical. Sched-\\nuler throughput also becomes a bottleneck since the num-\\nber of tasks required by ring reduce scales quadratically\\nwith the number of participants.\\n5.2\\nBuilding blocks\\nEnd-to-end applications (e.g., AlphaGo [54]) require a\\ntight coupling of training, serving, and simulation. In this\\nsection, we isolate each of these workloads to a setting\\nthat illustrates a typical RL application’s requirements.\\nDue to a ﬂexible programming model targeted to RL, and\\na system designed to support this programming model,\\nRay matches and sometimes exceeds the performance of\\ndedicated systems for these individual workloads.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 10}, page_content='4\\n8\\n16\\n32\\n64\\nNum GPUs (V100)\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\n6000\\n7000\\nMean images / s\\nHorovod + TF\\nDistributed TF\\nRay + TF\\nFigure 13: Images per second reached when distributing the\\ntraining of a ResNet-101 TensorFlow model (from the ofﬁcial\\nTF benchmark). All experiments were run on p3.16xl instances\\nconnected by 25Gbps Ethernet, and workers allocated 4 GPUs\\nper node as done in Horovod [53]. We note some measurement\\ndeviations from previously reported, likely due to hardware\\ndifferences and recent TensorFlow performance improvements.\\nWe used OpenMPI 3.0, TF 1.8, and NCCL2 for all runs.\\n5.2.1\\nDistributed Training\\nWe implement data-parallel synchronous SGD leverag-\\ning the Ray actor abstraction to represent model replicas.\\nModel weights are synchronized via allreduce (5.1) or pa-\\nrameter server, both implemented on top of the Ray API.\\nIn Figure 13, we evaluate the performance of the\\nRay (synchronous) parameter-server SGD implementa-\\ntion against state-of-the-art implementations [53], us-\\ning the same TensorFlow model and synthetic data gen-\\nerator for each experiment. We compare only against\\nTensorFlow-based systems to accurately measure the over-\\nhead imposed by Ray, rather than differences between the\\ndeep learning frameworks themselves. In each iteration,\\nmodel replica actors compute gradients in parallel, send\\nthe gradients to a sharded parameter server, then read the\\nsummed gradients from the parameter server for the next\\niteration.\\nFigure 13 shows that Ray matches the performance of\\nHorovod and is within 10% of distributed TensorFlow\\n(in distributed replicated mode). This is due to\\nthe ability to express the same application-level optimiza-\\ntions found in these specialized systems in Ray’s general-\\npurpose API. A key optimization is the pipelining of gra-\\ndient computation, transfer, and summation within a sin-\\ngle iteration. To overlap GPU computation with network\\ntransfer, we use a custom TensorFlow operator to write\\ntensors directly to Ray’s object store.\\n5.2.2\\nServing\\nModel serving is an important component of end-to-end\\napplications. Ray focuses primarily on the embedded\\nserving of models to simulators running within the same\\ndynamic task graph (e.g., within an RL application on\\nRay). In contrast, systems like Clipper [19] focus on\\nserving predictions to external clients.\\nIn this setting, low latency is critical for achieving high\\nutilization. To show this, in Table 3 we compare the\\nSystem\\nSmall Input\\nLarger Input\\nClipper\\n4400 ± 15 states/sec\\n290 ± 1.3 states/sec\\nRay\\n6200 ± 21 states/sec\\n6900 ± 150 states/sec\\nTable 3: Throughput comparisons for Clipper [19], a dedicated\\nserving system, and Ray for two embedded serving workloads.\\nWe use a residual network and a small fully connected network,\\ntaking 10ms and 5ms to evaluate, respectively. The server is\\nqueried by clients that each send states of size 4KB and 100KB\\nrespectively in batches of 64.\\nserver throughput achieved using a Ray actor to serve\\na policy versus using the open source Clipper system\\nover REST. Here, both client and server processes are co-\\nlocated on the same machine (a p3.8xlarge instance). This\\nis often the case for RL applications but not for the general\\nweb serving workloads addressed by systems like Clipper.\\nDue to its low-overhead serialization and shared memory\\nabstractions, Ray achieves an order of magnitude higher\\nthroughput for a small fully connected policy model that\\ntakes in a large input and is also faster on a more expensive\\nresidual network policy model, similar to one used in\\nAlphaGo Zero, that takes smaller input.\\n5.2.3\\nSimulation\\nSimulators used in RL produce results with variable\\nlengths (“timesteps”) that, due to the tight loop with train-\\ning, must be used as soon as they are available. The task\\nheterogeneity and timeliness requirements make simu-\\nlations hard to support efﬁciently in BSP-style systems.\\nTo demonstrate, we compare (1) an MPI implementation\\nthat submits 3n parallel simulation runs on n cores in 3\\nrounds, with a global barrier between rounds§, to (2) a\\nRay program that issues the same 3n tasks while concur-\\nrently gathering simulation results back to the driver. Ta-\\nble 4 shows that both systems scale well, yet Ray achieves\\nup to 1.8× throughput. This motivates a programming\\nmodel that can dynamically spawn and collect the results\\nof ﬁne-grained simulation tasks.\\nSystem, programming model\\n1 CPU\\n16 CPUs\\n256 CPUs\\nMPI, bulk synchronous\\n22.6K\\n208K\\n2.16M\\nRay, asynchronous tasks\\n22.3K\\n290K\\n4.03M\\nTable 4: Timesteps per second for the Pendulum-v0 simulator\\nin OpenAI Gym [13]. Ray allows for better utilization when\\nrunning heterogeneous simulations at scale.\\n§Note that experts can use MPI’s asynchronous primitives to get\\naround barriers—at the expense of increased program complexity —we\\nnonetheless chose such an implementation to simulate BSP.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 11}, page_content='5.3\\nRL Applications\\nWithout a system that can tightly couple the training, sim-\\nulation, and serving steps, reinforcement learning algo-\\nrithms today are implemented as one-off solutions that\\nmake it difﬁcult to incorporate optimizations that, for ex-\\nample, require a different computation structure or that\\nutilize different architectures. Consequently, with imple-\\nmentations of two representative reinforcement learning\\napplications in Ray, we are able to match and even out-\\nperform custom systems built speciﬁcally for these algo-\\nrithms. The primary reason is the ﬂexibility of Ray’s pro-\\ngramming model, which can express application-level op-\\ntimizations that would require substantial engineering ef-\\nfort to port to custom-built systems, but are transparently\\nsupported by Ray’s dynamic task graph execution engine.\\n5.3.1\\nEvolution Strategies\\nTo evaluate Ray on large-scale RL workloads, we imple-\\nment the evolution strategies (ES) algorithm and com-\\npare to the reference implementation [49]—a system spe-\\ncially built for this algorithm that relies on Redis for mes-\\nsaging and low-level multiprocessing libraries for data-\\nsharing. The algorithm periodically broadcasts a new pol-\\nicy to a pool of workers and aggregates the results of\\nroughly 10000 tasks (each performing 10 to 1000 simula-\\ntion steps).\\nAs shown in Figure 14a, an implementation on Ray\\nscales to 8192 cores. Doubling the cores available yields\\nan average completion time speedup of 1.6×. Conversely,\\nthe special-purpose system fails to complete at 2048 cores,\\nwhere the work in the system exceeds the processing\\ncapacity of the application driver. To avoid this issue, the\\nRay implementation uses an aggregation tree of actors,\\nreaching a median time of 3.7 minutes, more than twice\\nas fast as the best published result (10 minutes).\\nInitial parallelization of a serial implementation using\\nRay required modifying only 7 lines of code. Performance\\nimprovement through hierarchical aggregation was easy\\nto realize with Ray’s support for nested tasks and actors.\\nIn contrast, the reference implementation had several hun-\\ndred lines of code dedicated to a protocol for communi-\\ncating tasks and data between workers, and would require\\nfurther engineering to support optimizations like hierar-\\nchical aggregation.\\n5.3.2\\nProximal Policy Optimization\\nWe implement Proximal Policy Optimization (PPO) [51]\\nin Ray and compare to a highly-optimized reference im-\\nplementation [5] that uses OpenMPI communication prim-\\nitives. The algorithm is an asynchronous scatter-gather,\\nwhere new tasks are assigned to simulation actors as they\\n256\\n1024\\n8192\\nNumber of CPUs\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\nMean time to solve (minutes)\\nx x x\\nReference ES\\nRay ES\\n(a) Evolution Strategies\\n8x1\\n64x8\\n512x64\\nCPUs x GPUs\\n0\\n100\\n200\\n300\\n400\\n500\\nMean time to solve (minutes)\\nMPI PPO\\nRay PPO\\n(b) PPO\\nFigure 14: Time to reach a score of 6000 in the Humanoid-\\nv1 task [13]. (a) The Ray ES implementation scales well to\\n8192 cores and achieves a median time of 3.7 minutes, over\\ntwice as fast as the best published result. The special-purpose\\nsystem failed to run beyond 1024 cores. ES is faster than PPO\\non this benchmark, but shows greater runtime variance. (b)\\nThe Ray PPO implementation outperforms a specialized MPI\\nimplementation [5] with fewer GPUs, at a fraction of the cost.\\nThe MPI implementation required 1 GPU for every 8 CPUs,\\nwhereas the Ray version required at most 8 GPUs (and never\\nmore than 1 GPU per 8 CPUs).\\nreturn rollouts to the driver. Tasks are submitted un-\\ntil 320000 simulation steps are collected (each task pro-\\nduces between 10 and 1000 steps). The policy update per-\\nforms 20 steps of SGD with a batch size of 32768. The\\nmodel parameters in this example are roughly 350KB.\\nThese experiments were run using p2.16xlarge (GPU) and\\nm4.16xlarge (high CPU) instances.\\nAs shown in Figure 14b, the Ray implementation out-\\nperforms the optimized MPI implementation in all exper-\\niments, while using a fraction of the GPUs. The reason\\nis that Ray is heterogeneity-aware and allows the user to\\nutilize asymmetric architectures by expressing resource\\nrequirements at the granularity of a task or actor. The Ray\\nimplementation can then leverage TensorFlow’s single-\\nprocess multi-GPU support and can pin objects in GPU\\nmemory when possible. This optimization cannot be eas-\\nily ported to MPI due to the need to asynchronously gather\\nrollouts to a single GPU process. Indeed, [5] includes two\\ncustom implementations of PPO, one using MPI for large\\nclusters and one that is optimized for GPUs but that is re-\\nstricted to a single node. Ray allows for an implementa-\\ntion suitable for both scenarios.\\nRay’s ability to handle resource heterogeneity also de-\\ncreased PPO’s cost by a factor of 4.5 [4], since CPU-only\\ntasks can be scheduled on cheaper high-CPU instances.\\nIn contrast, MPI applications often exhibit symmetric ar-\\nchitectures, in which all processes run the same code and\\nrequire identical resources, in this case preventing the\\nuse of CPU-only machines for scale-out. Furthermore,\\nthe MPI implementation requires on-demand instances\\nsince it does not transparently handle failure. Assum-\\ning 4× cheaper spot instances, Ray’s fault tolerance and\\nresource-aware scheduling together cut costs by 18×.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 12}, page_content='6\\nRelated Work\\nDynamic task graphs.\\nRay is closely related to\\nCIEL [40] and Dask [48]. All three support dynamic\\ntask graphs with nested tasks and implement the futures\\nabstraction. CIEL also provides lineage-based fault toler-\\nance, while Dask, like Ray, fully integrates with Python.\\nHowever, Ray differs in two aspects that have important\\nperformance consequences. First, Ray extends the task\\nmodel with an actor abstraction. This is necessary for\\nefﬁcient stateful computation in distributed training and\\nserving, to keep the model data collocated with the com-\\nputation. Second, Ray employs a fully distributed and de-\\ncoupled control plane and scheduler, instead of relying on\\na single master storing all metadata. This is critical for ef-\\nﬁciently supporting primitives like allreduce without sys-\\ntem modiﬁcation. At peak performance for 100MB on 16\\nnodes, allreduce on Ray (Section 5.1) submits 32 rounds\\nof 16 tasks in 200ms. Meanwhile, Dask reports a maxi-\\nmum scheduler throughput of 3k tasks/s on 512 cores [3].\\nWith a centralized scheduler, each round of allreduce\\nwould then incur a minimum of ∼5ms of scheduling\\ndelay, translating to up to 2× worse completion time (Fig-\\nure 12b). Even with a decentralized scheduler, coupling\\nthe control plane information with the scheduler leaves\\nthe latter on the critical path for data transfer, adding an\\nextra roundtrip to every round of allreduce.\\nDataﬂow systems. Popular dataﬂow systems, such\\nas MapReduce [20], Spark [65], and Dryad [28] have\\nwidespread adoption for analytics and ML workloads,\\nbut their computation model is too restrictive for a ﬁne-\\ngrained and dynamic simulation workload. Spark and\\nMapReduce implement the BSP execution model, which\\nassumes that tasks within the same stage perform the\\nsame computation and take roughly the same amount of\\ntime. Dryad relaxes this restriction but lacks support for\\ndynamic task graphs. Furthermore, none of these systems\\nprovide an actor abstraction, nor implement a distributed\\nscalable control plane and scheduler. Finally, Naiad [39]\\nis a dataﬂow system that provides improved scalability\\nfor some workloads, but only supports static task graphs.\\nMachine learning frameworks. TensorFlow [7] and\\nMXNet [18] target deep learning workloads and efﬁ-\\nciently leverage both CPUs and GPUs.\\nWhile they\\nachieve great performance for training workloads consist-\\ning of static DAGs of linear algebra operations, they have\\nlimited support for the more general computation required\\nto tightly couple training with simulation and embedded\\nserving. TensorFlow Fold [33] provides some support for\\ndynamic task graphs, as well as MXNet through its inter-\\nnal C++ APIs, but neither fully supports the ability to mod-\\nify the DAG during execution in response to task progress,\\ntask completion times, or faults. TensorFlow and MXNet\\nin principle achieve generality by allowing the program-\\nmer to simulate low-level message-passing and synchro-\\nnization primitives, but the pitfalls and user experience in\\nthis case are similar to those of MPI. OpenMPI [22] can\\nachieve high performance, but it is relatively hard to pro-\\ngram as it requires explicit coordination to handle hetero-\\ngeneous and dynamic task graphs. Furthermore, it forces\\nthe programmer to explicitly handle fault tolerance.\\nActor systems. Orleans [14] and Akka [1] are two ac-\\ntor frameworks well suited to developing highly available\\nand concurrent distributed systems. However, compared\\nto Ray, they provide less support for recovery from data\\nloss. To recover stateful actors, the Orleans developer\\nmust explicitly checkpoint actor state and intermediate re-\\nsponses. Stateless actors in Orleans can be replicated for\\nscale-out, and could therefore act as tasks, but unlike in\\nRay, they have no lineage. Similarly, while Akka explic-\\nitly supports persisting actor state across failures, it does\\nnot provide efﬁcient fault tolerance for stateless computa-\\ntion (i.e., tasks). For message delivery, Orleans provides\\nat-least-once and Akka provides at-most-once semantics.\\nIn contrast, Ray provides transparent fault tolerance and\\nexactly-once semantics, as each method call is logged in\\nthe GCS and both arguments and results are immutable.\\nWe ﬁnd that in practice these limitations do not affect the\\nperformance of our applications. Erlang [10] and C++ Ac-\\ntor Framework [17], two other actor-based systems, have\\nsimilarly limited support for fault tolerance.\\nGlobal control store and scheduling. The concept\\nof logically centralizing the control plane has been pre-\\nviously proposed in software deﬁned networks (SDNs)\\n[16], distributed ﬁle systems (e.g., GFS [23]), resource\\nmanagement (e.g., Omega [52]), and distributed frame-\\nworks (e.g., MapReduce [20], BOOM [9]), to name a\\nfew. Ray draws inspiration from these pioneering efforts,\\nbut provides signiﬁcant improvements. In contrast with\\nSDNs, BOOM, and GFS, Ray decouples the storage of\\nthe control plane information (e.g., GCS) from the logic\\nimplementation (e.g., schedulers). This allows both stor-\\nage and computation layers to scale independently, which\\nis key to achieving our scalability targets. Omega uses\\na distributed architecture in which schedulers coordinate\\nvia globally shared state. To this architecture, Ray adds\\nglobal schedulers to balance load across local schedulers,\\nand targets ms-level, not second-level, task scheduling.\\nRay implements a unique distributed bottom-up sched-\\nuler that is horizontally scalable, and can handle dynami-\\ncally constructed task graphs. Unlike Ray, most existing\\ncluster computing systems [20, 64, 40] use a centralized\\nscheduler architecture. While Sparrow [45] is decentral-\\nized, its schedulers make independent decisions, limiting\\nthe possible scheduling policies, and all tasks of a job are\\nhandled by the same global scheduler. Mesos [26] im-\\nplements a two-level hierarchical scheduler, but its top-\\nlevel scheduler manages frameworks, not individual tasks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 13}, page_content='Canary [47] achieves impressive performance by having\\neach scheduler instance handle a portion of the task graph,\\nbut does not handle dynamic computation graphs.\\nCilk [12] is a parallel programming language whose\\nwork-stealing scheduler achieves provably efﬁcient load-\\nbalancing for dynamic task graphs. However, with no cen-\\ntral coordinator like Ray’s global scheduler, this fully par-\\nallel design is also difﬁcult to extend to support data lo-\\ncality and resource heterogeneity in a distributed setting.\\n7\\nDiscussion and Experiences\\nBuilding Ray has been a long journey. It started two years\\nago with a Spark library to perform distributed training\\nand simulations. However, the relative inﬂexibility of the\\nBSP model, the high per-task overhead, and the lack of an\\nactor abstraction led us to develop a new system. Since we\\nreleased Ray roughly one year ago, several hundreds of\\npeople have used it and several companies are running it\\nin production. Here we discuss our experience developing\\nand using Ray, and some early user feedback.\\nAPI. In designing the API, we have emphasized mini-\\nmalism. Initially we started with a basic task abstraction.\\nLater, we added the wait() primitive to accommodate roll-\\nouts with heterogeneous durations and the actor abstrac-\\ntion to accommodate third-party simulators and amortize\\nthe overhead of expensive initializations. While the re-\\nsulting API is relatively low-level, it has proven both pow-\\nerful and simple to use. We have already used this API to\\nimplement many state-of-the-art RL algorithms on top of\\nRay, including A3C [36], PPO [51], DQN [37], ES [49],\\nDDPG [55], and Ape-X [27]. In most cases it took us\\njust a few tens of lines of code to port these algorithms to\\nRay. Based on early user feedback, we are considering\\nenhancing the API to include higher level primitives and\\nlibraries, which could also inform scheduling decisions.\\nLimitations. Given the workload generality, special-\\nized optimizations are hard. For example, we must make\\nscheduling decisions without full knowledge of the com-\\nputation graph. Scheduling optimizations in Ray might\\nrequire more complex runtime proﬁling. In addition, stor-\\ning lineage for each task requires the implementation of\\ngarbage collection policies to bound storage costs in the\\nGCS, a feature we are actively developing.\\nFault tolerance. We are often asked if fault tolerance\\nis really needed for AI applications. After all, due to the\\nstatistical nature of many AI algorithms, one could sim-\\nply ignore failed rollouts. Based on our experience, our\\nanswer is “yes”. First, the ability to ignore failures makes\\napplications much easier to write and reason about. Sec-\\nond, our particular implementation of fault tolerance via\\ndeterministic replay dramatically simpliﬁes debugging as\\nit allows us to easily reproduce most errors. This is par-\\nticularly important since, due to their stochasticity, AI al-\\ngorithms are notoriously hard to debug. Third, fault toler-\\nance helps save money since it allows us to run on cheap\\nresources like spot instances on AWS. Of course, this\\ncomes at the price of some overhead. However, we found\\nthis overhead to be minimal for our target workloads.\\nGCS and Horizontal Scalability. The GCS dramati-\\ncally simpliﬁed Ray development and debugging. It en-\\nabled us to query the entire system state while debugging\\nRay itself, instead of having to manually expose internal\\ncomponent state. In addition, the GCS is also the backend\\nfor our timeline visualization tool, used for application-\\nlevel debugging.\\nThe GCS was also instrumental to Ray’s horizontal\\nscalability. In Section 5, we were able to scale by adding\\nmore shards whenever the GCS became a bottleneck. The\\nGCS also enabled the global scheduler to scale by sim-\\nply adding more replicas. Due to these advantages, we\\nbelieve that centralizing control state will be a key design\\ncomponent of future distributed systems.\\n8\\nConclusion\\nNo general-purpose system today can efﬁciently support\\nthe tight loop of training, serving, and simulation. To ex-\\npress these core building blocks and meet the demands of\\nemerging AI applications, Ray uniﬁes task-parallel and\\nactor programming models in a single dynamic task graph\\nand employs a scalable architecture enabled by the global\\ncontrol store and a bottom-up distributed scheduler. The\\nprogramming ﬂexibility, high throughput, and low laten-\\ncies simultaneously achieved by this architecture is partic-\\nularly important for emerging artiﬁcial intelligence work-\\nloads, which produce tasks diverse in their resource re-\\nquirements, duration, and functionality. Our evaluation\\ndemonstrates linear scalability up to 1.8 million tasks per\\nsecond, transparent fault tolerance, and substantial perfor-\\nmance improvements on several contemporary RL work-\\nloads. Thus, Ray provides a powerful combination of ﬂex-\\nibility, performance, and ease of use for the development\\nof future AI applications.\\n9\\nAcknowledgments\\nThis research is supported in part by NSF CISE Expedi-\\ntions Award CCF-1730628 and gifts from Alibaba, Ama-\\nzon Web Services, Ant Financial, Arm, CapitalOne, Eric-\\nsson, Facebook, Google, Huawei, Intel, Microsoft, Sco-\\ntiabank, Splunk and VMware as well as by NSF grant\\nDGE-1106400. We are grateful to our anonymous review-\\ners and our shepherd, Miguel Castro, for thoughtful feed-\\nback, which helped improve the quality of this paper.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 14}, page_content='References\\n[1] Akka. https://akka.io/.\\n[2] Apache Arrow. https://arrow.apache.org/.\\n[3] Dask Benchmarks.\\nhttp://matthewrocklin.com/blog/\\nwork/2017/07/03/scaling.\\n[4] EC2 Instance Pricing.\\nhttps://aws.amazon.com/ec2/\\npricing/on-demand/.\\n[5] OpenAI Baselines: high-quality implementations of reinforce-\\nment learning algorithms.\\nhttps://github.com/openai/\\nbaselines.\\n[6] TensorFlow\\nServing.\\nhttps://www.tensorflow.org/\\nserving/.\\n[7] ABADI, M., BARHAM, P., CHEN, J., CHEN, Z., DAVIS, A.,\\nDEAN, J., DEVIN, M., GHEMAWAT, S., IRVING, G., ISARD, M.,\\nET AL. TensorFlow: A system for large-scale machine learning.\\nIn Proceedings of the 12th USENIX Symposium on Operating\\nSystems Design and Implementation (OSDI). Savannah, Georgia,\\nUSA (2016).\\n[8] AGARWAL, A., BIRD, S., COZOWICZ, M., HOANG, L., LANG-\\nFORD, J., LEE, S., LI, J., MELAMED, D., OSHRI, G., RIBAS,\\nO., SEN, S., AND SLIVKINS, A. A multiworld testing decision\\nservice. arXiv preprint arXiv:1606.03966 (2016).\\n[9] ALVARO, P., CONDIE, T., CONWAY, N., ELMELEEGY, K.,\\nHELLERSTEIN, J. M., AND SEARS, R. BOOM Analytics: ex-\\nploring data-centric, declarative programming for the cloud. In\\nProceedings of the 5th European conference on Computer systems\\n(2010), ACM, pp. 223–236.\\n[10] ARMSTRONG,\\nJ.,\\nVIRDING,\\nR.,\\nWIKSTR ¨OM,\\nC.,\\nAND\\nWILLIAMS, M. Concurrent programming in ERLANG.\\n[11] BEATTIE, C., LEIBO, J. Z., TEPLYASHIN, D., WARD, T.,\\nWAINWRIGHT, M., K ¨UTTLER, H., LEFRANCQ, A., GREEN, S.,\\nVALD´ES, V., SADIK, A., ET AL. DeepMind Lab. arXiv preprint\\narXiv:1612.03801 (2016).\\n[12] BLUMOFE, R. D., AND LEISERSON, C. E. Scheduling mul-\\ntithreaded computations by work stealing. J. ACM 46, 5 (Sept.\\n1999), 720–748.\\n[13] BROCKMAN, G., CHEUNG, V., PETTERSSON, L., SCHNEIDER,\\nJ., SCHULMAN, J., TANG, J., AND ZAREMBA, W. OpenAI gym.\\narXiv preprint arXiv:1606.01540 (2016).\\n[14] BYKOV, S., GELLER, A., KLIOT, G., LARUS, J. R., PANDYA,\\nR., AND THELIN, J. Orleans: Cloud computing for everyone. In\\nProceedings of the 2nd ACM Symposium on Cloud Computing\\n(2011), ACM, p. 16.\\n[15] CARBONE, P., EWEN, S., F ´ORA, G., HARIDI, S., RICHTER,\\nS., AND TZOUMAS, K. State management in Apache Flink:\\nConsistent stateful distributed stream processing. Proc. VLDB\\nEndow. 10, 12 (Aug. 2017), 1718–1729.\\n[16] CASADO, M., FREEDMAN, M. J., PETTIT, J., LUO, J., MCKE-\\nOWN, N., AND SHENKER, S. Ethane: Taking control of the enter-\\nprise. SIGCOMM Comput. Commun. Rev. 37, 4 (Aug. 2007), 1–12.\\n[17] CHAROUSSET, D., SCHMIDT, T. C., HIESGEN, R.,\\nAND\\nW ¨AHLISCH, M. Native actors: A scalable software platform for\\ndistributed, heterogeneous environments. In Proceedings of the\\n2013 workshop on Programming based on actors, agents, and de-\\ncentralized control (2013), ACM, pp. 87–96.\\n[18] CHEN, T., LI, M., LI, Y., LIN, M., WANG, N., WANG, M.,\\nXIAO, T., XU, B., ZHANG, C., AND ZHANG, Z. MXNet: A\\nﬂexible and efﬁcient machine learning library for heterogeneous\\ndistributed systems. In NIPS Workshop on Machine Learning\\nSystems (LearningSys’16) (2016).\\n[19] CRANKSHAW, D., WANG, X., ZHOU, G., FRANKLIN, M. J.,\\nGONZALEZ, J. E., AND STOICA, I. Clipper: A low-latency\\nonline prediction serving system. In 14th USENIX Symposium\\non Networked Systems Design and Implementation (NSDI 17)\\n(Boston, MA, 2017), USENIX Association, pp. 613–627.\\n[20] DEAN, J., AND GHEMAWAT, S. MapReduce: Simpliﬁed data\\nprocessing on large clusters. Commun. ACM 51, 1 (Jan. 2008),\\n107–113.\\n[21] DENNIS, J. B., AND MISUNAS, D. P. A preliminary architecture\\nfor a basic data-ﬂow processor. In Proceedings of the 2Nd An-\\nnual Symposium on Computer Architecture (New York, NY, USA,\\n1975), ISCA ’75, ACM, pp. 126–132.\\n[22] GABRIEL, E., FAGG, G. E., BOSILCA, G., ANGSKUN, T., DON-\\nGARRA, J. J., SQUYRES, J. M., SAHAY, V., KAMBADUR, P.,\\nBARRETT, B., LUMSDAINE, A., CASTAIN, R. H., DANIEL,\\nD. J., GRAHAM, R. L., AND WOODALL, T. S. Open MPI: Goals,\\nconcept, and design of a next generation MPI implementation. In\\nProceedings, 11th European PVM/MPI Users’ Group Meeting\\n(Budapest, Hungary, September 2004), pp. 97–104.\\n[23] GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google\\nﬁle system. 29–43.\\n[24] GONZALEZ, J. E., XIN, R. S., DAVE, A., CRANKSHAW, D.,\\nFRANKLIN, M. J., AND STOICA, I. GraphX: Graph processing\\nin a distributed dataﬂow framework. In Proceedings of the 11th\\nUSENIX Conference on Operating Systems Design and Implemen-\\ntation (Berkeley, CA, USA, 2014), OSDI’14, USENIX Associa-\\ntion, pp. 599–613.\\n[25] GU*, S., HOLLY*, E., LILLICRAP, T., AND LEVINE, S. Deep re-\\ninforcement learning for robotic manipulation with asynchronous\\noff-policy updates. In IEEE International Conference on Robotics\\nand Automation (ICRA 2017) (2017).\\n[26] HINDMAN, B., KONWINSKI, A., ZAHARIA, M., GHODSI, A.,\\nJOSEPH, A. D., KATZ, R., SHENKER, S., AND STOICA, I.\\nMesos: A platform for ﬁne-grained resource sharing in the data\\ncenter. In Proceedings of the 8th USENIX Conference on Net-\\nworked Systems Design and Implementation (Berkeley, CA, USA,\\n2011), NSDI’11, USENIX Association, pp. 295–308.\\n[27] HORGAN, D., QUAN, J., BUDDEN, D., BARTH-MARON, G.,\\nHESSEL, M., VAN HASSELT, H., AND SILVER, D. Distributed\\nprioritized experience replay. International Conference on Learn-\\ning Representations (2018).\\n[28] ISARD, M., BUDIU, M., YU, Y., BIRRELL, A., AND FETTERLY,\\nD. Dryad: Distributed data-parallel programs from sequential\\nbuilding blocks. In Proceedings of the 2nd ACM SIGOPS/EuroSys\\nEuropean Conference on Computer Systems 2007 (New York, NY,\\nUSA, 2007), EuroSys ’07, ACM, pp. 59–72.\\n[29] JIA, Y., SHELHAMER, E., DONAHUE, J., KARAYEV, S., LONG,\\nJ., GIRSHICK, R., GUADARRAMA, S., AND DARRELL, T. Caffe:\\nConvolutional architecture for fast feature embedding.\\narXiv\\npreprint arXiv:1408.5093 (2014).\\n[30] JORDAN, M. I., AND MITCHELL, T. M.\\nMachine learning:\\nTrends, perspectives, and prospects. Science 349, 6245 (2015),\\n255–260.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 15}, page_content='[31] LEIBIUSKY, J., EISBRUCH, G., AND SIMONASSI, D. Getting\\nStarted with Storm. O’Reilly Media, Inc., 2012.\\n[32] LI, M., ANDERSEN, D. G., PARK, J. W., SMOLA, A. J.,\\nAHMED, A., JOSIFOVSKI, V., LONG, J., SHEKITA, E. J., AND\\nSU, B.-Y. Scaling distributed machine learning with the parame-\\nter server. In Proceedings of the 11th USENIX Conference on Op-\\nerating Systems Design and Implementation (Berkeley, CA, USA,\\n2014), OSDI’14, pp. 583–598.\\n[33] LOOKS, M., HERRESHOFF, M., HUTCHINS, D., AND NORVIG,\\nP. Deep learning with dynamic computation graphs. arXiv preprint\\narXiv:1702.02181 (2017).\\n[34] LOW, Y., GONZALEZ, J., KYROLA, A., BICKSON, D.,\\nGUESTRIN, C., AND HELLERSTEIN, J. GraphLab: A new frame-\\nwork for parallel machine learning. In Proceedings of the Twenty-\\nSixth Conference on Uncertainty in Artiﬁcial Intelligence (Arling-\\nton, Virginia, United States, 2010), UAI’10, pp. 340–349.\\n[35] MALEWICZ, G., AUSTERN, M. H., BIK, A. J., DEHNERT, J. C.,\\nHORN, I., LEISER, N., AND CZAJKOWSKI, G. Pregel: A system\\nfor large-scale graph processing. In Proceedings of the 2010 ACM\\nSIGMOD International Conference on Management of Data (New\\nYork, NY, USA, 2010), SIGMOD ’10, ACM, pp. 135–146.\\n[36] MNIH, V., BADIA, A. P., MIRZA, M., GRAVES, A., LILLICRAP,\\nT. P., HARLEY, T., SILVER, D., AND KAVUKCUOGLU, K. Asyn-\\nchronous methods for deep reinforcement learning. In Interna-\\ntional Conference on Machine Learning (2016).\\n[37] MNIH, V., KAVUKCUOGLU, K., SILVER, D., RUSU, A. A.,\\nVENESS, J., BELLEMARE, M. G., GRAVES, A., RIEDMILLER,\\nM., FIDJELAND, A. K., OSTROVSKI, G., ET AL. Human-level\\ncontrol through deep reinforcement learning. Nature 518, 7540\\n(2015), 529–533.\\n[38] MURRAY, D. A Distributed Execution Engine Supporting Data-\\ndependent Control Flow. University of Cambridge, 2012.\\n[39] MURRAY, D. G., MCSHERRY, F., ISAACS, R., ISARD, M.,\\nBARHAM, P., AND ABADI, M. Naiad: A timely dataﬂow system.\\nIn Proceedings of the Twenty-Fourth ACM Symposium on Operat-\\ning Systems Principles (New York, NY, USA, 2013), SOSP ’13,\\nACM, pp. 439–455.\\n[40] MURRAY, D. G., SCHWARZKOPF, M., SMOWTON, C., SMITH,\\nS., MADHAVAPEDDY, A., AND HAND, S. CIEL: A universal exe-\\ncution engine for distributed data-ﬂow computing. In Proceedings\\nof the 8th USENIX Conference on Networked Systems Design and\\nImplementation (Berkeley, CA, USA, 2011), NSDI’11, USENIX\\nAssociation, pp. 113–126.\\n[41] NAIR, A., SRINIVASAN, P., BLACKWELL, S., ALCICEK, C.,\\nFEARON, R., MARIA, A. D., PANNEERSHELVAM, V., SULEY-\\nMAN, M., BEATTIE, C., PETERSEN, S., LEGG, S., MNIH, V.,\\nKAVUKCUOGLU, K., AND SILVER, D. Massively parallel meth-\\nods for deep reinforcement learning, 2015.\\n[42] NG, A., COATES, A., DIEL, M., GANAPATHI, V., SCHULTE, J.,\\nTSE, B., BERGER, E., AND LIANG, E. Autonomous inverted he-\\nlicopter ﬂight via reinforcement learning. Experimental Robotics\\nIX (2006), 363–372.\\n[43] NISHIHARA, R., MORITZ, P., WANG, S., TUMANOV, A., PAUL,\\nW., SCHLEIER-SMITH, J., LIAW, R., NIKNAMI, M., JORDAN,\\nM. I., AND STOICA, I. Real-time machine learning: The missing\\npieces. In Workshop on Hot Topics in Operating Systems (2017).\\n[44] OPENAI.\\nOpenAI Dota 2 1v1 bot.\\nhttps://openai.com/\\nthe-international/, 2017.\\n[45] OUSTERHOUT, K., WENDELL, P., ZAHARIA, M., AND STOICA,\\nI. Sparrow: Distributed, low latency scheduling. In Proceedings\\nof the Twenty-Fourth ACM Symposium on Operating Systems\\nPrinciples (New York, NY, USA, 2013), SOSP ’13, ACM, pp. 69–\\n84.\\n[46] PASZKE, A., GROSS, S., CHINTALA, S., CHANAN, G., YANG,\\nE., DEVITO, Z., LIN, Z., DESMAISON, A., ANTIGA, L., AND\\nLERER, A. Automatic differentiation in PyTorch.\\n[47] QU, H., MASHAYEKHI, O., TEREI, D., AND LEVIS, P. Canary:\\nA scheduling architecture for high performance cloud computing.\\narXiv preprint arXiv:1602.01412 (2016).\\n[48] ROCKLIN, M. Dask: Parallel computation with blocked algo-\\nrithms and task scheduling. In Proceedings of the 14th Python in\\nScience Conference (2015), K. Huff and J. Bergstra, Eds., pp. 130\\n– 136.\\n[49] SALIMANS, T., HO, J., CHEN, X., AND SUTSKEVER, I. Evolu-\\ntion strategies as a scalable alternative to reinforcement learning.\\narXiv preprint arXiv:1703.03864 (2017).\\n[50] SANFILIPPO, S. Redis: An open source, in-memory data structure\\nstore. https://redis.io/, 2009.\\n[51] SCHULMAN, J., WOLSKI, F., DHARIWAL, P., RADFORD, A.,\\nAND KLIMOV, O. Proximal policy optimization algorithms. arXiv\\npreprint arXiv:1707.06347 (2017).\\n[52] SCHWARZKOPF, M., KONWINSKI, A., ABD-EL-MALEK, M.,\\nAND WILKES, J. Omega: Flexible, scalable schedulers for large\\ncompute clusters. In Proceedings of the 8th ACM European Con-\\nference on Computer Systems (New York, NY, USA, 2013), Eu-\\nroSys ’13, ACM, pp. 351–364.\\n[53] SERGEEV, A., AND DEL BALSO, M.\\nHorovod: fast and\\neasy distributed deep learning in tensorﬂow.\\narXiv preprint\\narXiv:1802.05799 (2018).\\n[54] SILVER, D., HUANG, A., MADDISON, C. J., GUEZ, A.,\\nSIFRE, L., VAN DEN DRIESSCHE, G., SCHRITTWIESER, J.,\\nANTONOGLOU, I., PANNEERSHELVAM, V., LANCTOT, M.,\\nET AL. Mastering the game of Go with deep neural networks and\\ntree search. Nature 529, 7587 (2016), 484–489.\\n[55] SILVER, D., LEVER, G., HEESS, N., DEGRIS, T., WIERSTRA,\\nD., AND RIEDMILLER, M. Deterministic policy gradient algo-\\nrithms. In ICML (2014).\\n[56] SUTTON, R. S., AND BARTO, A. G. Reinforcement Learning:\\nAn Introduction. MIT press Cambridge, 1998.\\n[57] THAKUR, R., RABENSEIFNER, R., AND GROPP, W. Optimiza-\\ntion of collective communication operations in MPICH. The Inter-\\nnational Journal of High Performance Computing Applications\\n19, 1 (2005), 49–66.\\n[58] TIAN, Y., GONG, Q., SHANG, W., WU, Y., AND ZITNICK, C. L.\\nELF: An extensive, lightweight and ﬂexible research platform\\nfor real-time strategy games. Advances in Neural Information\\nProcessing Systems (NIPS) (2017).\\n[59] TODOROV, E., EREZ, T., AND TASSA, Y. Mujoco: A physics\\nengine for model-based control. In Intelligent Robots and Systems\\n(IROS), 2012 IEEE/RSJ International Conference on (2012), IEEE,\\npp. 5026–5033.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-10-02T00:16:18+00:00', 'source': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ray-distributed-framework-AI-app.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-10-02T00:16:18+00:00', 'trapped': '', 'modDate': 'D:20181002001618Z', 'creationDate': 'D:20181002001618Z', 'page': 16}, page_content='[60] VAN DEN BERG, J., MILLER, S., DUCKWORTH, D., HU, H.,\\nWAN, A., FU, X.-Y., GOLDBERG, K., AND ABBEEL, P. Su-\\nperhuman performance of surgical tasks by robots using iterative\\nlearning from human-guided demonstrations. In Robotics and Au-\\ntomation (ICRA), 2010 IEEE International Conference on (2010),\\nIEEE, pp. 2074–2081.\\n[61]\\nVAN RENESSE, R., AND SCHNEIDER, F. B. Chain replication for\\nsupporting high throughput and availability. In Proceedings of the\\n6th Conference on Symposium on Opearting Systems Design &\\nImplementation - Volume 6 (Berkeley, CA, USA, 2004), OSDI’04,\\nUSENIX Association.\\n[62] VENKATARAMAN, S., PANDA, A., OUSTERHOUT, K., GHODSI,\\nA., ARMBRUST, M., RECHT, B., FRANKLIN, M., AND STOICA,\\nI. Drizzle: Fast and adaptable stream processing at scale. In\\nProceedings of the Twenty-Sixth ACM Symposium on Operating\\nSystems Principles (2017), SOSP ’17, ACM.\\n[63] WHITE, T. Hadoop: The Deﬁnitive Guide. O’Reilly Media, Inc.,\\n2012.\\n[64] ZAHARIA, M., CHOWDHURY, M., DAS, T., DAVE, A., MA, J.,\\nMCCAULEY, M., FRANKLIN, M. J., SHENKER, S., AND STO-\\nICA, I. Resilient distributed datasets: A fault-tolerant abstrac-\\ntion for in-memory cluster computing. In Proceedings of the 9th\\nUSENIX conference on Networked Systems Design and Implemen-\\ntation (2012), USENIX Association, pp. 2–2.\\n[65] ZAHARIA, M., XIN, R. S., WENDELL, P., DAS, T., ARMBRUST,\\nM., DAVE, A., MENG, X., ROSEN, J., VENKATARAMAN, S.,\\nFRANKLIN, M. J., GHODSI, A., GONZALEZ, J., SHENKER, S.,\\nAND STOICA, I. Apache Spark: A uniﬁed engine for big data\\nprocessing. Commun. ACM 59, 11 (Oct. 2016), 56–65.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Scaling Transformer-Based Novel View Synthesis Models with\\nToken Disentanglement and Synthetic Data\\nNithin Gopalakrishnan Nair1∗\\nSrinivas Kaza2∗\\nXuan Luo2\\nVishal M. Patel1\\nStephen Lombardi2\\nJungyeon Park2\\n1 Johns Hopkins University\\n2 Google\\n{ngopala2,vpatel36}@jhu.edu\\n{srinivaskaza,xuluo,salombardi,jungyeonp}@google.com\\nhttps://scaling3dnvs.github.io\\nLVSM\\nOURS\\nLVSM\\nOURS\\nLVSM\\nOURS\\nFigure 1. Overview. Our method performs feed-forward novel-view synthesis from a series of input images, such as the pairs shown\\nabove. We demonstrate strong results in terms of quality and generalization capacity, performing well across a variety of common novel-\\nview synthesis datasets, including scenes that are out-of-distribution.\\nAbstract\\nLarge transformer-based models have made significant\\nprogress in generalizable novel view synthesis (NVS) from\\nsparse input views, generating novel viewpoints without the\\nneed for test-time optimization.\\nHowever, these models\\nare constrained by the limited diversity of publicly avail-\\nable scene datasets, making most real-world (in-the-wild)\\nscenes out-of-distribution. To overcome this, we incorpo-\\nrate synthetic training data generated from diffusion mod-\\nels, which improves generalization across unseen domains.\\nWhile synthetic data offers scalability, we identify artifacts\\nintroduced during data generation as a key bottleneck af-\\nfecting reconstruction quality. To address this, we propose\\na token disentanglement process within the transformer ar-\\nchitecture, enhancing feature separation and ensuring more\\neffective learning. This refinement not only improves re-\\narXiv:2509.06950v1  [cs.GR]  8 Sep 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='construction quality over standard transformers but also\\nenables scalable training with synthetic data. As a result,\\nour method outperforms existing models on both in-dataset\\nand cross-dataset evaluations, achieving state-of-the-art re-\\nsults across multiple benchmarks while significantly reduc-\\ning computational costs.\\n1. Introduction\\nNovel view synthesis (NVS) [20, 27] is a well-studied and\\nimportant problem in computer vision, where the task is to\\ngenerate unseen perspectives of a scene from a given set of\\nimages. Many approaches utilize volumetric [2, 5, 27, 28]\\nor differentiable rendering [20] to optimize for each scene\\nindividually, achieving high-quality NVS from arbitrary\\nviewpoints.\\nMore recently, advancements have enabled\\ntraining a single model that generalizes to novel scenes\\nwithout requiring per-scene optimization.\\nMost existing\\nmethods address NVS by incorporating hand-crafted 3D\\npriors and architectural biases [4, 16, 39]. While these de-\\nsign choices provide structure, they limit scalability with\\ndata and hinder generalization.\\nRecently, Large View Synthesis Model (LVSM) [19]\\nproposed a promising foundation for an NVS model scal-\\nable with large datasets.\\nLVSM introduces an architec-\\nture that doesn’t require 3D inductive biases for scene re-\\nconstruction.\\nIt employs a decoder-only transformer ar-\\nchitecture that achieves state-of-the-art results by a sig-\\nnificant margin, with the performance improving with in-\\ncreased compute. However, we observed during our exper-\\niments that the decoder-only design causes an inherent fea-\\nture alignment problem which causes the target and source\\nfeatures to look similar at all layers. Thus, part of the trans-\\nformer’s computational capacity is spent modifying source\\ntoken information that is ultimately discarded at the end\\nof the transformer block, reducing efficiency. This design\\nchoice also makes LVSM susceptible to unwanted noise\\nor compression artifacts that may be present in the source\\nviews. In addition, we noticed that LVSM presents limited\\ncross-domain performance when tested on datasets outside\\nthe training dataset domains.\\nMoreover, these issues are not unique to LVSM; many\\nNVS models face similar challenges due to data scarcity\\nin 3D vision. All existing multi-view 3D scene datasets\\n[24, 25, 49] combined contain fewer than 100,000 scenes,\\nseverely limiting the performance of NVS models on in-\\nthe-wild cases beyond the training distribution. One pos-\\nsible solution for alleviating this 3D data scarcity is using\\nsynthetic data from generative models. Recent research has\\nexplored adapting pre-trained image [33, 34] and video dif-\\nfusion models [14, 15] for multi-view dataset generation\\n*Equal contribution. Nair designed the methodology, conducted pre-\\nrebuttal experiments, and drafted the initial manuscript. Kaza helped ad-\\nvise the project, led the rebuttal, and conducted camera-ready experiments.\\n[10, 26, 36, 44].\\nHowever, previous feed-forward mod-\\nels trained using synthetic data perform worse than those\\ntrained with real data. We hypothesize that the inability of\\nsynthetic data to improve reconstruction quality stems from\\ntwo types of degradation artifacts in scenes generated by\\ndiffusion models [15, 29, 38] (1) artifacts influenced by the\\ninitial noise of the diffusion process and (2) artifacts intro-\\nduced during decoding, as most diffusion-based scene syn-\\nthesis models operate in latent space and rely on a diffusion\\nVAE [33]. We address both issues, leading to improved per-\\nformance when using synthetic data. We provide a detailed\\nexplanation of our data pipeline in Section 4.2.\\nIn this work, we tackle a key challenge in developing\\na feed-forward NVS model that performs well on out-of-\\ndistribution data – the need for a scalable and efficient\\ntransformer-based NVS architecture. We introduce the To-\\nken Distentangled (Tok-D) transformer block, which ap-\\nplies layer-wise modulation of source and target tokens, ex-\\nplicitly distinguishing the two at each layer. These model\\nmodifications improve out-of-distribution training, which\\nintroduces the possibility of training on synthetic data. We\\nuse the CAT3D model to generate a large dataset of syn-\\nthetic multi-view samples. We then employ a novel data\\ngeneration strategy that significantly improves the quality\\nof these synthetic samples. We show that the Tok-D trans-\\nformer block can be trained with synthetic data augmenta-\\ntion, unlike the baseline LVSM method which suffers from\\nthe inclusion of synthetic data.\\n• We enhance the scalability of transformer architectures\\nfor NVS, enabling more efficient modeling.\\n• We introduce a new training scheme that is less suscepti-\\nble to artifacts from synthetic data.\\n• We improve the training efficiency of transformer for\\nNVS by introducing a new transformer block.\\n• Our approach achieves state-of-the-art results across mul-\\ntiple benchmarks for scene level NVS.\\n2. Related Works\\n2.1. Offline Novel View Synthesis\\nThe advent of neural rendering in recent years has substan-\\ntially improved the quality of NVS. Early neural scene rep-\\nresentations focused on the 4D plenoptic function [11, 23]\\nthat represents the lightfield of a scene [1, 37, 39]. Other\\nmethods modeled the geometry of the scene (e.g.\\nas a\\nsigned distance function) separately from material proper-\\nties [40, 45]. Either way, a differentiable rendering process\\nwas used to render these neural representations into 2D im-\\nages [27]. Most of these methods focused on fitting neu-\\nral fields to sparse observations of a scene at test time—\\nwe refer to this as test-time or offline optimization. There\\nis a substantial amount of heterogeneity in these methods,\\nboth in terms of the rendering method and the scene repre-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Tok-D Transformer Blocks\\nSource Patchify \\nPatchify\\nSource tokens\\nTarget tokens\\nMulti-View Diﬀusion Model\\n   Input View\\n                          Generated Views\\nSynthetic Data Generation\\nNVS Model Training\\nUnPatchify\\nFigure 2. An illustration of the architecture. We use CAT3D, a multi-view diffusion model, to generate synthetic views conditioned on\\nrandom spline camera trajectories and a random image. From the two random views form the generated views as the source views and\\nthe input conditioning view to be the target of our large reconstruction network. Our large reconstruction model uses a special transformer\\nblock which we name Tok-D Transformer. When real data is available, we just use the reconstruction transformer.\\nsentation used. Multi-layer perceptrons (MLPs) [27], vox-\\nels [9, 26], hashing-based representations [3, 28], triplanes\\n[5], and, most recently, Gaussian splats [17, 20, 21, 31]\\nhave been used as scene representations.\\nThese meth-\\nods have trade-offs between reconstruction quality, training\\ntime, inference time, memory/space requirements, capacity\\nto model view-dependent effects, etc. Some of these offline\\nmethods can even fit dynamic scenes. These test-time opti-\\nmization methods demonstrate compelling results given the\\nsparsity of the observations provided. However, they often\\nstruggle to incorporate priors learned from larger datasets.\\n2.2. Online Novel View Synthesis\\nSometimes referred to as “feed-forward” or “generaliz-\\nable” NVS models, these methods attempt to directly pro-\\nduce 3D representations from input images. Early efforts\\ninclude the image-based rendering-inspired IBRNet [41],\\nwhich directly produces 2D images based on epipolar cor-\\nrespondences on the viewing ray. The Large Reconstruc-\\ntion Model (LRM) [16] family of methods attempt to pro-\\nduce a triplane that represents an object, in some cases with\\nnear-real time performance. PixelSplat [4], MVSplat [4],\\nand GS-LRM [47] attempt to predict 3DGS [20] representa-\\ntions, which exploit the sparse Gaussian splat representation\\nand fast rasterization to achieve quasi-interactive inference.\\nThese methods are trained on large datasets of real-world\\nscenes, which helps them outperform even test-time opti-\\nmization methods. Quark [8] couples an easily-rasterizable\\nlayered depth map representation with a render-and-refine\\nstrategy to achieve state-of-the-art quality at a much higher\\nresolution. Other efforts in this space include GPNR [39]\\nand SRT [35], which are parameterized in a similar fash-\\nion to IBRNet [41] and attempt to scale up the image and\\nray transformers. LRF [22] attempts to perform 3D recon-\\nstruction in the latent space of a VAE, bypassing learning\\n3D representation altogether [48]. Finally, the LVSM [19]\\nremoves all 3D priors by simply using one transformer to\\nperform NVS. LVSM performs favorably compared to both\\ngeometry-free and geometry-based feed-forward models.\\n2.3. Synthetic Data\\nRecent efforts have leveraged synthetic data to train exist-\\ning feed-forward NVS methods and investigate its efficacy\\nas a training dataset. However, it is important to note that\\nthe synthetic data in many of these efforts are generated\\nprocedurally from systems like Blender, whereas ours are\\ngenerated from a multi-view diffusion model. Two recent\\nworks LRM-Zero [43] and MegaSynth [18] are examples\\nof models trained either entirely or mostly on procedurally\\ngenerated synthetic data. In LRM-Zero, they demonstrate\\nthat the LRM model can be trained entirely on synthetic\\ndata. However, the synthetic-data-only model shows a sub-\\nstantial decrease in reconstruction quality compared to the\\nreal-world-data equivalent. Improving training data diver-\\nsity using synthetic data for 4D generation has also been\\nexplored in CAT4D [42].\\n3. Background\\nLVSM is a feed-forward NVS method that has no 3D in-\\nductive bias. Since our model builds upon its architecture,\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Feedforward \\nNetwork\\nMulti-head \\nAttention\\nLayernorm\\nEmbedding \\nLayer\\n𝐑d\\n𝐑k\\nPre-Modulate\\nLinear Layer\\n𝐑6d\\nStyle \\nembed\\nPost-Modulate\\nPre-Modulate\\nPost-Modulate\\n 0\\n 1\\nStyle Tokens\\nFeedforward \\nNetwork\\nMulti-head \\nAttention\\nLayernorm\\n 0\\n 1\\nEmbedding \\nLayer\\n 𝐑d\\n𝐑k\\nModulate\\nLinear Layer\\n𝐑2d\\nStyle Tokens\\nStyle \\nembed\\nFigure 3. An illustration of the Tok-D transformer block. Our transformer blocks that differentiates between source and target tokens.\\nTok-D transformer modulates the input to all transformer blocks. Tok-D plus transformer modulates the attention and MLP layers.\\nwe outline the details here for clarity. Where i denotes the\\nimage index and j denotes the token index, source images\\npatches are written as Is\\nij ∈Rp×p×3, source Pl¨ucker coordi-\\nnates patches P s\\nij ∈Rp×p×6, and target Pl¨ucker coordinates\\nP t\\nj ∈Rp×p×6. The source images and pl¨ucker embeddings\\nare tokenized together using a linear layer embedder.\\n  S _ {ij} =Line\\nar( [ I\\n^s_{ij}, P^s_{ij}]) \\n(1)\\nThe target Pl¨ucker coordinates are also embedded using a\\nlinear layer.\\n  T _ {ij} =Li n\\near( P^t_{ij}) \\n(2)\\nFinally, the transformer network is trained to reconstruct the\\ntarget output tokens Ot\\nj from the Pl¨ucker patch embeddings.\\n  \\nO ^ t_{j} =M(T_{j}|S_{ij}) \\n(3)\\nThe target output tokens are detokenized using a linear\\nlayer which is converted to target image embeddings Tj ∈\\nRp×p×3\\n  T _{j} =Line\\nar([O^t_{j}]) \\n(4)\\nThe target patches are unpatchified to get the target image\\nT ∈RH×W ×3 (see Figure 2). The training is supervised\\nusing MSE loss and perceptual loss designed to reconstruct.\\nTransformer Block Consider a transformer block at\\nlayer l, which includes a Multi-head Self Attention layer\\n(SelfAttnl), a Feed-forward network (FFNl), and a Layer\\nNorm operation (LNl). For an input [xs\\nl , xt\\nl], where xs\\nl and\\nxt\\nl represent the source and target tokens, the data flow as\\nfollows:\\n  [\\n\\\\ m at\\nhb f  {x\\n} ^ s_\\nl,  \\\\mathbf {x}^t\\n_ l ] \\n&= \\n[\\\\m\\nath\\nb f  {\\nx} ^ s_l\\n,  \\\\m\\nat h bf {x}^t_l] \\n+  \\\\t\\next {SelfAttn}_l([\\\\mathbf {x}^s_l, \\\\mathbf {x}^t_l]) \\\\\\\\ \\\\notag [\\\\mathbf {x}^s_l, \\\\mathbf {x}^t_l] &= [\\\\mathbf {x}^s_l, \\\\mathbf {x}^t_l] + \\\\text {FFN}_l(LN_l([\\\\mathbf {x}^s_l, \\\\mathbf {x}^t_l])).\\nGiven the basic self attention based transformer blocks in\\nLVSM. At the end of the optimization process there arises a\\nneed for all token outputs of a particular layer to be aligned\\nsince they are processed by the same set of weights. Hence,\\nLVSM inherently has a chance to infuse noise or atifacts\\nthat maybe present in the source images to the target. More-\\nover this alignment also causes some part of the computa-\\ntional power of the model being used to model source token\\ninformation although those tokens are discarded at the last\\nlayer. Hence, we call for a need to distinguish between the\\nsource and target tokens of the transformer network.\\n4. Method\\nOur proposed method consists of two major contributions.\\nFirst, our Token-Disentangled (Tok-D) transformer block\\nis specialized for NVS and distinguishes information from\\nthe source and target views, leading to more efficient allo-\\ncation of representation capacity. Second, to address the\\nscarcity of multi-view data, we generate synthetic data us-\\ning CAT3D [10] and propose a model training scheme that\\nis robust to artifacts in this synthetic data. In this section,\\nwe describe each component in detail.\\n4.1. Token-Disentangled Transformer\\nIn LVSM, the transformer blocks process source and target\\ntokens in the same manner, even though the source consists\\nof images and Pl¨ucker rays, while the target includes only\\nPl¨ucker rays. Additionally, source and target image quality\\ncan differ when training with synthetic data. To address this,\\nwe introduce the Token-Disentangled (Tok-D) Transformer\\nblock (see Figure 3), which enables differentiated process-\\ning of source and target tokens through modulation. The\\nTok-D Transformer uses an indicator variable (δ), where\\nδ = 1 for target tokens and δ = 0 for source tokens, to\\nmodulate tokens based on their origin. This mechanism ex-\\ntracts distinct style vectors and computes specific scale and\\nbias parameters for each layer and token type, allowing for\\nprecise and adaptive token modulation.\\n  \\\\ma t hbf {style} &= \\\\\\ntex\\nt {Line a r} ( \\\\tex t  {E mbed} (\\\\de lta  )) \\\\\\\\ \\\\notag \\\\\\ntex\\nt  {M\\nod } _l(\\\\ma\\nt hbf \\n{ x })\\n &=  (1+\\\\m\\na thb\\nf  { \\\\sig\\nma }\\n_l)\\\\mathbf {x} + \\\\mathbf {\\\\mu }_l, \\\\text {where\\\\ } [\\\\mathbf {\\\\sigma }_l, \\\\mathbf {\\\\mu }_l] = \\\\text {Linear}_l(\\\\mathbf {style}) \\\\\\\\ \\\\notag [\\\\mathbf {x}^s_l, \\\\mathbf {x}_l^t] &= \\\\text {Mod}^{s,t}_l([\\\\mathbf {x}^s_l, \\\\mathbf {x}_l^t]) = [\\\\text {Mod}^s_l(\\\\mathbf {x}^s_l),\\\\text {Mod}^t_l(\\\\mathbf {x}^t_l)]\\nModulating the input of each transformer block improves\\nperforamnce.\\nDrawing inspiration from DiT [30], we\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='extend this modulation to the Attention and MLP lay-\\ners, achieving further improvements. This modulation is\\ntermed pre-modulation if applied before a layer and post-\\nmodulation if after. Pre-modulation includes both scaling\\nand shifting, and post-modulation involves only scaling.\\n  [\\\\\\nh a t {\\n\\\\m a thbf {\\nx} }^s_\\nl ,  \\\\\\nhat\\n {\\\\\\nmat\\nh b f \\n{x } }_l\\n^ t ] \\n&=  \\\\te\\nxt {M\\nod} ^ {s,t}_{l1}([\\\\\\nm a thb\\nf {\\nx}^s\\n_ l , \\\\\\nma t hbf {x\\n}_ l^t]\\n)  \\\\\\\\\\n \\\\n\\nota\\ng  [\\\\\\nma t hbf\\n { x}\\n^s _ l, \\n\\\\ma th\\nbf { x}_l^t] &= [\\\\\\nm a thb\\nf {x}^s_l, \\\\mathbf {x}_l^t] + [\\\\sigma ^{s}_{l1}, \\\\sigma ^{t}_{l1}] \\\\odot \\\\text {SelfAttn}([\\\\hat {\\\\mathbf {x}}^s_l, \\\\hat {\\\\mathbf {x}}_l^t]) \\\\\\\\ \\\\notag [\\\\hat {\\\\mathbf {x}}^s_l, \\\\hat {\\\\mathbf {x}}_l^t] &= \\\\text {Mod}^{s,t}_{l2}([\\\\mathbf {x}^s_l, \\\\mathbf {x}_l^t]) \\\\\\\\ \\\\notag [\\\\mathbf {x}^s_l, \\\\mathbf {x}_l^t] &= [\\\\mathbf {x}^s_l, \\\\mathbf {x}_l^t] + [\\\\sigma ^{s}_{l2}, \\\\sigma ^{t}_{l2}] \\\\odot \\\\text {FFN}_l(\\\\text {LN}_l([\\\\hat {\\\\mathbf {x}}^s_l, \\\\hat {\\\\mathbf {x}}_l^t]))\\nwhere ⊙denotes element-wise multiplication which scales\\nthe corresponding source and target tokens.\\nOur Tok-D transformer block enhances the distinction\\nbetween source and target tokens, as reflected in their dis-\\ntinct feature representations (Figure 6, Section 5.4). This\\nspecialization highlights the superior representational ca-\\npacity of our model. Furthermore, when trained on syn-\\nthetic data (Section 4.2), out-of-distribution artifacts can\\nintroduce quality disparities between source and target to-\\nkens. By leveraging its token-aware architecture, our model\\ndemonstrates greater robustness to these artifacts, resulting\\nin improved performance, as shown in Section 5.3.\\n4.2. Synthetic Data Generation & Training Scheme\\nTraining a naive transformer model with synthetic data can\\nlead to degraded performance rather than improvement due\\nto two key factors: (1) The model struggles to distinguish\\nbetween tokens from source images and target images, al-\\nlowing artifacts from one to propagate into the other dur-\\ning alignment. (2) The model is trained to generate novel\\nviews from sparse input views, and if the target is a syn-\\nthetic image with artifacts, it may learn a distribution bi-\\nased toward unrealistic images. While these issues might\\nnot arise with perfect synthetic data, in-practice synthetic\\ndatasets often contain noise, making the model vulnerable\\nto errors through either mechanism. However, for image-to-\\nmultiview synthesis models like CAT3D, we propose a sim-\\nple yet effective solution: assigning the conditioned image\\nas the target view and the generated views as input views.\\nFormally let Ic, Cc denote the input image and camera\\nconditioning used for the multiview diffusion model. We\\nsample additional random spline camera trajectory poses\\nCtgt relative to this particular view, and use the state-of-\\nthe-art multi-view diffusion model CAT3D to generate the\\ntarget views Isrc conditioned on the input conditioning and\\ntarget poses\\n  I_ { gen} \\\\sim DM( I_{ gen}|C_{gen},C_c,I_c) \\n(8)\\nHere DM represents inferencing through the state of the art\\ndiffusion model, After obtaining the generated views, we\\nsample 2 generated views Isrc\\n  I_{ src} , C_{sr c} \\\\sim I_{gen},C_{gen} \\n(9)\\nand their camera poses as the source images Isrc, Csrc\\nand utilize the conditioned image and its camera as the tar-\\nget Ic, Cc. Sampling the source and target images this way\\nforces the transformer to always generate a realistic image,\\nmaking our model robust to artifacts from synthetic data.\\n5. Experiments\\n5.1. Implementation Details\\nTraining details We perform all experiments on 8 H100\\nGPUs. We use the AdamW optimizer with β parameters\\n0.9 and 0.95, and we use weight decay with a rate of 0.05\\nfor all layers except the normalization layers. Moreover, we\\nuse a linear learning rate scheduler with with a peak learn-\\ning rate of 2e−4, and a warmup of 2500 iterations. In total,\\nall experiments have 100k training iterations. In addition,\\nwe use exponential moving averaging (EMA) with a rate of\\n0.99 for stabilizing the training process. Although previous\\nworks required gradient clipping for a stable training pro-\\ncess, our training processes were smooth without a need for\\nan explicit gradient clipping.\\nTraining and Evaluation Datasets For scene-level synthe-\\nsis model training, we use Re10K [49], ACID [25] and\\nDL3DV [24] with their originally released train and test\\nsplits. We also perform an experiment where the model\\nis trained together with a mix of all of these datasets. For\\nscene-level synthesis, we follow LVSM and train using 2\\ninput views and test using 6 target views fed one at a time.\\nFor DL3DV dataset evaluation, we choose the farthest cam-\\nera from a randomly selected target view as the input view.\\nThe training and evaluation of DL3DV dataset for in dis-\\ntribution metrics is done using 2 input views and 2 target\\nviews. For cross dataset testing, we use 2 input views and 6\\ntarget views for DL3DV dataset. We use a batch size of 64\\nfor our experiments.\\nSynthetic Data For generating the synthetic data we use the\\nstate-of-the-art 3D generation model CAT3D. CAT3D was\\ntrained using a single scene dataset Re10K and three object-\\nbased datasets: Objaverse [7], MVImgNet [46] and Co3D\\n[32]. To create synthetic data, we use two variants: one with\\n1 conditioning view and 7 generated views, and another\\nwith 3 conditioning views and 5 generated views. We match\\nthe focal lengths of Re10K and DL3DV during generation.\\nFor the camera trajectory, we sample a random spline tra-\\njectory with a random position rotation matrix, converting it\\ninto ray maps before passing it into the network. As CAT3D\\nis originally trained with a resolution of 512, we convert the\\nimages and camera parameters to a resolution of 256 before\\npassing them through our network.\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Input Views\\nLVSM\\nOurs\\nGT\\nFigure 4. Qualitative results on in-distribution datasets. We illustrate the cases Tok-D transformer works better than LVSM. We notice\\nthat we obtain substantial improvement in cases where the novel views needs to reconstruct regions present only in one of the views as\\nshown in the highlighted regions in the images. The results presented here are taken from our in-distribution trained model. We present\\ntwo diffrent views to show that this problem is persistent across views.\\nTable 1. Quantitative comparisons for in-distribution scene synthesis at 256 resolution. LVSM and our method are trained with a\\nbatch size of 64. LVSM results are taken from the original paper rather than our re-implementation. Our method outperforms the previous\\nSOTA method across all exisiting datasets. (\\n,\\n,\\n) denotes the first, second and third best results.\\nMethod\\nVenue\\nRealEstate10k [49]\\nACID [25]\\nDL3DV [24]\\nPSNR \\\\delimiter \"3222378 \\nSSIM \\\\delimiter \"3222378 \\nLPIPS \\\\delimiter \"3223379 \\nPSNR \\\\delimiter \"3222378 \\nSSIM \\\\delimiter \"3222378 \\nLPIPS \\\\delimiter \"3223379 \\nPSNR \\\\delimiter \"3222378 \\nSSIM \\\\delimiter \"3222378 \\nLPIPS \\\\delimiter \"3223379 \\nGPNR [39]\\nCVPR’23\\n24.11\\n0.793\\n0.255\\n25.28\\n0.764\\n0.332\\n-\\n-\\n-\\nPixelSplat [4]\\nCVPR’24\\n25.89\\n0.858\\n0.142\\n28.14\\n0.839\\n0.533\\n-\\n-\\n-\\nMVSplat [6]\\nECCV’25\\n26.39\\n0.869\\n0.128\\n28.25\\n0.843\\n0.144\\n17.54\\n0.529\\n0.402\\nDepthSplat [44]\\nCVPR’25\\n27.44\\n0.887\\n0.119\\n-\\n-\\n-\\n19.05\\n0.610\\n0.313\\nLVSM [19]\\nICLR’25\\n28.89\\n0.894\\n0.108\\n29.19\\n0.836\\n0.095\\n19.91\\n0.600\\n0.273\\nOurs\\n30.02\\n0.919\\n0.058\\n29.47\\n0.846\\n0.086\\n21.55\\n0.643\\n0.208\\n5.2. Scene Synthesis\\nWe evaluate our method qualitatively and quantitatively\\nfor scene synthesis using very recent feed-forward meth-\\nods GPNR, PixelSplat, MVSplat, DepthSplat and LVSM.\\nThese methods were chosen because they outperform con-\\nventional approaches in 2-view reconstruction. Quantita-\\ntive results are shown in Table 1. We observe that Tok-D-\\nPlus outperforms LVSM by 1.2 dB on the Re10K evaluation\\nbenchmark when both models are trained with 8 GPUs. Fur-\\nthermore, despite using only 8 GPUs, our method still sur-\\npasses LVSM trained with 64 GPUs by a margin of 0.2 dB.\\nMoreover we obtain an improvement of 1.6dB over LVSM\\nin a more diverse scene-level dataset, DL3DV [24] dataset\\nas well. We also observe that our performance improvement\\nis 0.2 in ACID dataset. We emphasize that this happens be-\\ncause ACID has a relatively smaller training and testing set\\nand the dataset is generally clean and easier to reconstruct.\\nWe also provide the corresponding qualitative comparisons\\non Re10K and DL3DV dataset in Figure 4 . Comparing the\\nmain results we find that our method usually outperforms\\nLVSM when the generated content is only visible in one of\\nthe source views. When the camera is far from both views\\nand the information is present only in one of the views, our\\nmethod is still able to extract the relevant content from the\\ncorresponding input image. As can be seen from rows 1\\nand 2, the reconstruction form LVSM fails to reconstruct\\nobjects present in only one of the views, whereas Tok-D\\ntransformer can effectively reconstruct these regions.\\n5.3. Cross-Dataset Scene Synthesis\\nTo analyze the generalization capacity of our method, we\\nevaluate our method trained with Re10K dataset on two dif-\\nferent datasets: ACID and DL3DV [24]. ACID is a dataset\\nwith aerial views similar to Re10K. DL3DV [24] is a di-\\nverse dataset comprising natural scenes and various indoor\\nand outdoor settings. The scene geometry and appearance\\nof DL3DV [24] is very different from Re10k. We test the\\nRe10K and ACID datasets at a resolution of 256×265. For\\ntesting on DL3DV [24], we choose a resolution of 256×448\\nto maintain the original aspect ratio in the DL3DV [24]\\ndataset and well as maintain consistent evaluation settings\\nwith DepthSplat.\\nWe choose 2 source views and 6 tar-\\nget views for all of these datasets. Looking closely at the\\nquantitative results on Table 1 and Table 2, we find that the\\nmodel trained on Re10K underperformed the in-distribution\\ntrained model by a small margin. The drop is higher in the\\ncase of DL3DV due to resolution and diversity differences\\nin the datasets. Next we add a small portion of synthetic\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='LVSM\\nOurs\\nGT\\nInput\\nFigure 5. Out-of-distribution Evaluation: We evaluate our the version of our method fine-tuned on synthetic data and LVSM on DL3DV\\nand ACID (i.e. out-of-distribution datasets). We also evaluate the model with resolutions that were not used during training. We notice that\\nLVSM’s visual quality degrades when substantial camera motion reveals previously-occluded regions.\\nTable 2. Quantitative comparisons for scaling up with synthetic data. We evaluate LVSM and our method, which are both trained with\\na batch size of 64. A mixture of synthesized DL3DV and Re10K data is used for the synthetic tab. For MVSplat and DepthSplat we include\\nthe numbers reported in their papers\\nMethod\\nTraining Dataset\\nRealEstate10k [49]\\nACID [25]\\nDL3DV [24]\\nRe10K[49]\\nSynthetic\\nPSNR \\\\delimiter \"3222378 \\nSSIM \\\\delimiter \"3222378 \\nLPIPS \\\\delimiter \"3223379 \\nPSNR \\\\delimiter \"3222378 \\nSSIM \\\\delimiter \"3222378 \\nLPIPS \\\\delimiter \"3223379 \\nPSNR \\\\delimiter \"3222378 \\nSSIM \\\\delimiter \"3222378 \\nLPIPS \\\\delimiter \"3223379 \\nMVSplat [6]\\n✓\\n26.39\\n0.869\\n0.128\\n28.15\\n0.147\\n0.841\\n17.72\\n0.534\\n0.371\\nDepthsplat [44]\\n✓\\n27.44\\n0.887\\n0.119\\n-\\n-\\n-\\n18.90\\n0.640\\n0.317\\nLVSM [19]\\n✓\\n28.89\\n0.894\\n0.108\\n28.29\\n0.809\\n0.104\\n20.52\\n0.621\\n0.223\\nLVSM [19]\\n✓\\n✓\\n28.49\\n0.892\\n0.070\\n28.16\\n0.802\\n0.107\\n20.21\\n0.595\\n0.240\\nOurs\\n✓\\n30.02\\n0.910\\n0.058\\n29.31\\n0.838\\n0.091\\n21.18\\n0.652\\n0.205\\nOurs\\n✓\\n✓\\n29.97\\n0.920\\n0.058\\n29.37\\n0.839\\n0.091\\n21.27\\n0.657\\n0.202\\ndata comprising about half the size of Re10K dataset and\\nperform training with the new framework. We also retrain\\nLVSM for the same setting. We find that LVSM’s perfor-\\nmance drops rather than improving when synthetic data is\\nadded. We emphasize that this arises due to the introduc-\\ntion of artifacts during feature alignment. In contrast, we\\nobserve an improvement in quality on our method when a\\nsmall amount of synthetic data is added.\\n5.4. Analysis and Discussion\\nVisualization of source and target features. To visually\\nillustrate the representation alignment problem mentioned\\nin the previous sections, we visualize the 3 channel PCA\\nof each transformer block output after unpatchifying for all\\n24 layers of LVSM and our method in Figure 6. The first\\nrow shows the first 6 layer outputs, second row shows layer\\n6-12, and so on. We can see that for a particular scene the\\nsource and target layer tokens are aligned at all layers even\\nthough the training objective is to reconstruct the target.\\nThis causes inefficient usage of the transformer parameters\\nto maintain the source information throughout the layers.\\nMoreover this also makes the model prone to noise in the\\nsource data. However, with our Tok-D transformer there is\\nno alignment and the source information is infused much\\nearlier, leaving more room for the transformer blocks to re-\\nconstruct the target. Another important observation is that\\nalthough both source image and Pl¨ucker coordinates are fed\\nas input to the source, the source tokens look similar to the\\nPl¨ucker coordinates. Whereas in our case the image compo-\\nnents in the source PCA components leading to much more\\neffective information extraction from each source token.\\nImpact of additional real data. Incorporating synthetic\\ndata into the training process facilitates the introduction of\\ndiverse scenes and camera motion, enhancing model gener-\\nalizability. While the proposed Tok-D transformer demon-\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='(a)\\nTarget\\n(b)\\nSource\\n(c) LVSM Target PCA\\n(d) LVSM Source PCA\\n(e) Ours Source PCA\\n(f) Ours Target PCA\\nFigure 6. A visualization of the principal components of transformer layer outputs for source and target of LVSM. The 24 images\\nin each subfigure show the layer output of each layer of the transformer. LVSM features for source and target images looks similar even\\nthough the source is conditioned with image and Pl¨ucker coordinates and target is conditioned with Pl¨ucker coordinates alone. This leads\\nto inefficient transformer usage requiring explicit alignment of source and target features across different layers\\nTable 3. Ablation studies on scaling up with more real data. Although including synthetic data in training is helpful for improving\\nquality, including additional real data significantly improves reconstruction quality.\\nMethod\\nTraining Dataset\\nRealEstate10k [49]\\nACID [25]\\nDL3DV [24]\\nRe10K [49]+ Synthetic\\nDL3DV [24]\\nPSNR \\\\delimiter \"3222378 \\nSSIM \\\\delimiter \"3222378 \\nLPIPS \\\\delimiter \"3223379 \\nPSNR \\\\delimiter \"3222378 \\nSSIM \\\\delimiter \"3222378 \\nLPIPS \\\\delimiter \"3223379 \\nPSNR \\\\delimiter \"3222378 \\nSSIM \\\\delimiter \"3222378 \\nLPIPS \\\\delimiter \"3223379 \\nLVSM [19]\\n✓\\n28.49\\n0.892\\n0.070\\n28.16\\n0.802\\n0.107\\n20.21\\n0.595\\n0.240\\nLVSM [19]\\n✓\\n✓\\n28.10\\n0.892\\n0.073\\n28.79\\n0.826\\n0.096\\n21.37\\n0.665\\n0.196\\nOurs\\n✓\\n29.97\\n0.920\\n0.058\\n29.37\\n0.839\\n0.091\\n21.27\\n0.657\\n0.202\\nOurs\\n✓\\n✓\\n29.78\\n0.917\\n0.0604\\n30.13\\n0.857\\n0.082\\n23.14\\n0.726\\n0.156\\nTable 4. Ablation analysis We analyze the performance improve-\\nment of our design choices. Pre and Post demonstrate the effects\\nof including or not including pre/post-modulation.\\nPre\\nPost\\nWhole\\nAttn\\nMLP\\nPSNR \\\\delimiter \"3222378 \\nSSIM \\\\delimiter \"3222378 \\nLPIPS \\\\delimiter \"3223379 \\n28.50\\n0.893\\n0.070\\n✓\\n✓\\n29.69\\n0.911\\n0.063\\n✓\\n✓\\n✓\\n28.51\\n0.894\\n0.070\\n✓\\n✓\\n✓\\n✓\\n30.02\\n0.918\\n0.058\\nstrates reduced sensitivity to synthetic data artifacts and in-\\ncreased generative diversity, its photorealistic reconstruc-\\ntion performance remains comparable to the baseline model\\ntrained solely on real data. To investigate the impact of\\naugmenting the training dataset with additional real data,\\nwe integrated the DL3DV dataset into the existing exper-\\nimental setup. This modification resulted in a significant\\nimprovement in photorealistic reconstruction, as evidenced\\nby a substantial increase in PSNR on the ACID dataset. Fur-\\nthermore, the relative performance gains observed with our\\nmodel, compared to LVSM, were considerably greater, sug-\\ngesting a reduced susceptibility to noise.\\n5.5. Ablation Studies\\nWe analyze the impact of various design choices in the net-\\nwork. Specifically, we examine three aspects: (1) The effect\\nof modulation in different parts of the network, (2) The role\\nof EMA in performance, (3) Number of input views.\\nImpact of modulation at different locations of Tok-D\\ntransformer. We examine the effect of modulating differ-\\nent parts of the network. For this, we consider four differ-\\nent cases. We present the corresponding results in Table 4.\\nHaving a common modulation premodulation worked better\\nthan separate premodulation for both layers.\\nImpact of EMA. We also observe that performing Expo-\\nnential moving average (EMA) [13] during training results\\nin a performance boost for the base model. For the sake of\\nTable 5. Effect of EMA on runtime performance and quality.\\nComparison performed on Re10k.\\nMethod\\nTrain\\nRender\\nGFLOPs\\nNo EMA\\nWith EMA\\n(ms)\\n(ms)\\nPSNR\\nSSIM\\nLPIPS\\nPSNR\\nSSIM\\nLPIPS\\nLVSM-1024\\n706.1\\n171.6\\n2896.88\\n27.68\\n0.88\\n0.077\\n28.65\\n0.90\\n0.070\\nOurs\\n734.6\\n174.4\\n2900.78\\n28.75\\n0.90\\n0.064\\n30.02\\n0.92\\n0.058\\nTable 6. Effect of adding more source views. Our method works\\nwell as additional source views are introduced.\\nMethod\\n2 views\\n4 views\\n8 views\\nPSNR\\nSSIM\\nLPIPS\\nPSNR\\nSSIM\\nLPIPS\\nPSNR\\nSSIM\\nLPIPS\\nOurs\\n30.02\\n0.92\\n0.058\\n31.51\\n0.94\\n0.048\\n33.09\\n0.94\\n0.042\\nconsistency, we show the results of our model and our re-\\nimplementation of LVSM with 1024 channels trained with\\nand without EMA in Table 5.\\nImpact of number of source frames. Our model scales\\nwith the number of input views and results in better re-\\nconstruction quality when more input views are fed to the\\nmodel to the model as presented in Table 6.\\n6. Conclusion\\nIn this paper, we introduce a new approach to scaling up\\nNVS by addressing two key limitations in existing mod-\\nels: efficiency and diversity.\\nTo enhance the efficiency\\nof transformer-based NVS models, we propose the Token-\\nDisentangled (Tok-D) Transformer, which reduces redun-\\ndancies and improves data efficiency, enabling higher re-\\nconstruction quality with less compute. Additionally, the\\nTok-D Transformer mitigates training artifacts through its\\ndisentangling property, allowing for effective scaling us-\\ning synthetic data. Incorporating synthetic data into train-\\ning significantly improves cross-dataset performance com-\\npared to existing models. By integrating the Tok-D Trans-\\nformer and synthetic data, we achieve state-of-the-art re-\\nsults across three large-scale NVS benchmarks, surpassing\\nprevious methods with lower computational cost and by a\\nsubstantial margin.\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='References\\n[1] Benjamin Attal, Jia-Bin Huang, Michael Zollh¨ofer, Johannes\\nKopf, and Changil Kim. Learning neural light fields with\\nray-space embedding. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition, pages\\n19819–19829, 2022. 2\\n[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\\nSrinivasan, and Peter Hedman.\\nZip-nerf:\\nAnti-aliased\\ngrid-based neural radiance fields.\\nIn Proceedings of the\\nIEEE/CVF International Conference on Computer Vision,\\npages 19697–19705, 2023. 2\\n[3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\\nSrinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-\\nbased neural radiance fields. ICCV, 2023. 3\\n[4] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and\\nVincent Sitzmann. pixelsplat: 3d gaussian splats from image\\npairs for scalable generalizable 3d reconstruction. In Pro-\\nceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition, pages 19457–19467, 2024. 2, 3, 6\\n[5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\\nHao Su. Tensorf: Tensorial radiance fields. In European\\nConference on Computer Vision (ECCV), 2022. 2, 3\\n[6] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang,\\nMarc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei\\nCai. Mvsplat: Efficient 3d gaussian splatting from sparse\\nmulti-view images. In European Conference on Computer\\nVision, pages 370–386. Springer, 2024. 6, 7\\n[7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\\nA universe of annotated 3d objects.\\nIn Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 13142–13153, 2023. 5\\n[8] John Flynn, Michael Broxton, Lukas Murmann, Lucy Chai,\\nMatthew DuVall, Cl´ement Godard, Kathryn Heal, Srinivas\\nKaza, Stephen Lombardi, Xuan Luo, et al. Quark: Real-time,\\nhigh-resolution, and general neural view synthesis.\\nACM\\nTransactions on Graphics (TOG), 43(6):1–20, 2024. 3\\n[9] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\\nRadiance fields without neural networks. In Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 5501–5510, 2022. 3\\n[10] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur\\nBrussee, Ricardo Martin Brualla, Pratul Srinivasan, Jonathan\\nBarron, and Ben Poole. Cat3d: Create anything in 3d with\\nmulti-view diffusion models. Advances in Neural Informa-\\ntion Processing Systems, 37:75468–75494, 2024. 2, 4\\n[11] Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and\\nMichael F. Cohen. The lumigraph. In Proceedings of the\\n23rd Annual Conference on Computer Graphics and Inter-\\nactive Techniques, page 43–54, New York, NY, USA, 1996.\\nAssociation for Computing Machinery. 2\\n[12] Albert Gu and Tri Dao. Mamba: Linear-time sequence mod-\\neling with selective state spaces, 2024. 12\\n[13] David Haynes, Steven Corns, and Ganesh Kumar Venayag-\\namoorthy.\\nAn exponential moving average algorithm.\\nIn\\n2012 IEEE Congress on Evolutionary Computation, pages\\n1–8. IEEE, 2012. 8\\n[14] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\\nQifeng Chen. Latent video diffusion models for high-fidelity\\nlong video generation.\\narXiv preprint arXiv:2211.13221,\\n2022. 2\\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\\nfusion probabilistic models. Advances in neural information\\nprocessing systems, 33:6840–6851, 2020. 2\\n[16] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,\\nDifan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao\\nTan. LRM: Large reconstruction model for single image to\\n3d. In The Twelfth International Conference on Learning\\nRepresentations, 2024. 2, 3\\n[17] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and\\nShenghua Gao. 2d gaussian splatting for geometrically accu-\\nrate radiance fields. In Special Interest Group on Computer\\nGraphics and Interactive Techniques Conference Conference\\nPapers, page 1–11. ACM, 2024. 3\\n[18] Hanwen Jiang, Zexiang Xu, Desai Xie, Ziwen Chen, Haian\\nJin, Fujun Luan, Zhixin Shu, Kai Zhang, Sai Bi, Xin Sun, Ji-\\nuxiang Gu, Qixing Huang, Georgios Pavlakos, and Hao Tan.\\nMegasynth: Scaling up 3d scene reconstruction with syn-\\nthesized data. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR), pages\\n16441–16452, 2025. 3\\n[19] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi,\\nTianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang\\nXu. LVSM: A large view synthesis model with minimal 3d\\ninductive bias. In The Thirteenth International Conference\\non Learning Representations, 2025. 2, 3, 6, 7, 8\\n[20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk¨uhler,\\nand George Drettakis.\\n3d gaussian splatting for real-time\\nradiance field rendering. ACM Trans. Graph., 42(4):139–1,\\n2023. 2, 3\\n[21] Shakiba Kheradmand,\\nDelio Vicini,\\nGeorge Kopanas,\\nDmitry Lagun, Kwang Moo Yi, Mark Matthews, and An-\\ndrea Tagliasacchi. Stochasticsplats: Stochastic rasterization\\nfor sorting-free 3d gaussian splatting, 2025. 3\\n[22] Diederik P Kingma, Max Welling, et al. Auto-encoding vari-\\national bayes, 2013. 3\\n[23] Marc Levoy and Pat Hanrahan. Light field rendering. In\\nProceedings of the 23rd Annual Conference on Computer\\nGraphics and Interactive Techniques, page 31–42, New\\nYork, NY, USA, 1996. Association for Computing Machin-\\nery. 2\\n[24] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin,\\nKun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu,\\net al.\\nDl3dv-10k: A large-scale scene dataset for deep\\nlearning-based 3d vision. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npages 22160–22169, 2024. 2, 5, 6, 7, 8\\n[25] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh\\nMakadia, Noah Snavely, and Angjoo Kanazawa. Infinite na-\\nture: Perpetual view generation of natural scenes from a sin-\\ngle image. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision, pages 14458–14467, 2021.\\n2, 5, 6, 7, 8\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='[26] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\\nChristian Theobalt. Neural sparse voxel fields. Advances\\nin Neural Information Processing Systems, 33:15651–15663,\\n2020. 2, 3\\n[27] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\\nRepresenting scenes as neural radiance fields for view syn-\\nthesis. In European conference on computer vision, 2020. 2,\\n3\\n[28] Thomas M¨uller, Alex Evans, Christoph Schied, and Alexan-\\nder Keller. Instant neural graphics primitives with a mul-\\ntiresolution hash encoding. ACM transactions on graphics\\n(TOG), 41(4):1–15, 2022. 2, 3\\n[29] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\\ndenoising diffusion probabilistic models.\\nIn International\\nconference on machine learning, pages 8162–8171. PMLR,\\n2021. 2\\n[30] William Peebles and Saining Xie. Scalable diffusion models\\nwith transformers. In Proceedings of the IEEE/CVF inter-\\nnational conference on computer vision, pages 4195–4205,\\n2023. 4\\n[31] Lukas Radl, Michael Steiner, Mathias Parger, Alexan-\\nder Weinrauch, Bernhard Kerbl, and Markus Steinberger.\\nStopThePop: Sorted Gaussian Splatting for View-Consistent\\nReal-time Rendering.\\nACM Transactions on Graphics, 4\\n(43), 2024. 3\\n[32] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,\\nLuca Sbordone, Patrick Labatut, and David Novotny. Com-\\nmon objects in 3d:\\nLarge-scale learning and evaluation\\nof real-life 3d category reconstruction.\\nIn Proceedings of\\nthe IEEE/CVF international conference on computer vision,\\npages 10901–10911, 2021. 5\\n[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\\nPatrick Esser, and Bj¨orn Ommer.\\nHigh-resolution image\\nsynthesis with latent diffusion models.\\nIn Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 10684–10695, 2022. 2\\n[34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\\net al. Photorealistic text-to-image diffusion models with deep\\nlanguage understanding.\\nAdvances in neural information\\nprocessing systems, 35:36479–36494, 2022. 2\\n[35] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs\\nBergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario\\nLuˇci´c, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene\\nrepresentation transformer: Geometry-free novel view syn-\\nthesis through set-latent scene representations. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 6229–6238, 2022. 3\\n[36] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,\\nand Xiao Yang. MVDream: Multi-view diffusion for 3d gen-\\neration. In The Twelfth International Conference on Learn-\\ning Representations, 2024. 2\\n[37] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh\\nTenenbaum, and Fredo Durand. Light field networks: Neu-\\nral scene representations with single-evaluation rendering.\\nAdvances in Neural Information Processing Systems, 34:\\n19313–19325, 2021. 2\\n[38] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\\ngenerative modeling through stochastic differential equa-\\ntions. In International Conference on Learning Represen-\\ntations, 2021. 2\\n[39] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and\\nAmeesh Makadia. Generalizable patch-based neural render-\\ning.\\nIn European Conference on Computer Vision, pages\\n156–174. Springer, 2022. 2, 3, 6\\n[40] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\\nKomura, and Wenping Wang. Neus: Learning neural im-\\nplicit surfaces by volume rendering for multi-view recon-\\nstruction. Advances in Neural Information Processing Sys-\\ntems, 34:27171–27183, 2021. 2\\n[41] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P\\nSrinivasan, Howard Zhou, Jonathan T Barron, Ricardo\\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr-\\nnet: Learning multi-view image-based rendering. In Pro-\\nceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition, pages 4690–4699, 2021. 3\\n[42] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi\\nZheng, Jonathan T Barron, and Aleksander Holynski. Cat4d:\\nCreate anything in 4d with multi-view video diffusion mod-\\nels.\\nIn Proceedings of the Computer Vision and Pattern\\nRecognition Conference, pages 26057–26068, 2025. 3\\n[43] Desai Xie, Sai Bi, Zhixin Shu, Kai Zhang, Zexiang Xu, Yi\\nZhou, Soren Pirk, Arie Kaufman, Xin Sun, and Hao Tan.\\nLRM-zero: Training large reconstruction models with syn-\\nthesized data. In The Thirty-eighth Annual Conference on\\nNeural Information Processing Systems, 2024. 3\\n[44] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann\\nBlum, Daniel Barath, Andreas Geiger, and Marc Pollefeys.\\nDepthsplat: Connecting gaussian splatting and depth. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR), pages 16453–16463, 2025.\\n2, 6, 7\\n[45] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\\nume rendering of neural implicit surfaces, 2021. 2\\n[46] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu,\\nChongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu,\\nZhangyang Xiong, Tianyou Liang, et al.\\nMvimgnet: A\\nlarge-scale dataset of multi-view images. In Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 9150–9161, 2023. 5\\n[47] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao,\\nKalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large recon-\\nstruction model for 3d gaussian splatting. European Confer-\\nence on Computer Vision, 2024. 3\\n[48] Chaoyi Zhou, Xi Liu, Feng Luo, and Siyu Huang. Latent\\nradiance fields with 3d-aware 2d representations.\\nIn The\\nThirteenth International Conference on Learning Represen-\\ntations, 2025. 3\\n[49] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\\nand Noah Snavely. Stereo magnification: learning view syn-\\nthesis using multiplane images. ACM Trans. Graph., 37(4),\\n2018. 2, 5, 6, 7, 8\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='[50] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang,\\nWenyu Liu, and Xinggang Wang. Vision mamba: Efficient\\nvisual representation learning with bidirectional state space\\nmodel, 2024. 12\\n7. Design choices\\nWe provide further details of the exact transformer model\\nused here. Transformer blocks We find the claims regard-\\ning the naive transformer architecture to be unstable for im-\\nage generative tasks to be true. We use QK-Norm to stabi-\\nlize the transformer block. We use 24 transformer blocks\\nwith an embedding dimension of 1024. In addition to this,\\ndifferent from LVSM, we use attention biases at all layers\\nand include the bias for the last transformer block, as we\\nfind this design choice particularly stable with linear learn-\\ning rate decay. We use a patch size of 8 for all experiments.\\n7.1. Enhancing 3D generative models for 3D consis-\\ntent generation\\nThe use of diffusion models has been widely explored for\\ngenerating 3D scenes.\\nMultiple works in the literature\\nadapt pretrained text-to-image and image-to-video models\\nfor 3D-consistent scene generation. Most of these works\\ncondition the diffusion model on camera parameters and\\nlearn the conditional distribution of multiple views given the\\ncamera poses. Given the ability to cherry-pick and sample\\nthrough the diffusion model multiple times, these models\\nproduce high-quality results. However, existing 3D scene\\ngeneration models cannot mass-produce synthetic data for\\nfine-tuning substream models for high-fidelity generation.\\nUntil now, no generalizable models with high-fidelity re-\\nsults have been proposed that can directly utilize the data\\ngenerated by diffusion models. We argue that this draw-\\nback is caused by a lack of analysis of the inference-time\\ngeneration process of diffusion models. Although extensive\\nstudies have been performed on different training strategies\\nfor 3D-consistent generation using diffusion models, much\\nless effort has been put into improving inference-time gen-\\neration quality.\\nMost 3D generative models generate N views of a scene,\\neach of dimension (H × W × C), in parallel to preserve\\n3D consistency. The generation process starts with random\\nisotropic Gaussian noise of dimension N × H × W × C,\\nwhich undergoes a diffusion process of T steps. This either\\nconverts it into a latent representation, which is then de-\\ncoded by a VAE decoder to produce multiview images, or\\ngenerates images directly. These multiview images are fur-\\nther used to train a NeRF or a Gaussian Splat model to gen-\\nerate novel views of the scene. When the diffusion model\\ngenerates high-quality, 3D-consistent images, this frame-\\nwork works perfectly. However, in reality, diffusion models\\nare sensitive to input noise. Even for the simple case of\\nimage generation, different noise inputs produce different\\nquality results. Recent works have shed light on inference-\\ntime scaling laws for generation, claiming that the quality of\\ndiffusion model outputs can be controlled by selecting the\\ncorrect input noise via rejection sampling. Similar claims\\nhave been made for video generation models, where per-\\nformance improves significantly by refining the input noise\\nschedule.\\nTo understand this, consider a toy example: Suppose we\\nwant to generate an image (I1) using the diffusion model\\nconditioned on a text prompt. Starting with Gaussian noise\\nN1, if we want to generate another image (I2) close to (I1),\\nthe desired noise is most likely closer to N1. Previous works\\nhave demonstrated enhanced video generation results by se-\\nlecting starting noises that are close across different frames.\\nIn our case, we use the image-to-multiview variant of\\nCAT3D as the base model for generating multiview images.\\nFor choosing the initial noise, we follow a specific heuris-\\ntic. Specifically, we ensure that the noise across different\\nviews remains 3D-consistent. CAT3D is a multiview diffu-\\nsion model that generates eight views simultaneously, con-\\nditioned on the camera poses. CAT3D allows conditioning\\non a particular view to generate the remaining views. Given\\nthe view to be conditioned, we select a random noise for\\nthis view, denoted as V1, with its noise represented as N1\\nand the corresponding rotation-translation matrices denoted\\nas R1, t1. To estimate the starting noise for other views, we\\nperform a warping operation on N1, denoted by:\\n  N _i = war p(N_1, inv([R_i,t_i])[R_1,t_1]) t\\n(10)\\nwhere the warp operation transforms the coordinates of\\nN1 to Ni. However, we noticed that such a warping op-\\neration fails in regions outside the scene. To handle these\\ncases while enhancing consistency, we marginally modify\\nthe noise. Specifically, for these cases, we assign the noise\\nas:\\n  N _2 =  \\\\ a lpha N _1 + (1-\\\\alpha ) \\\\mathcal {N}(0,I) \\n(11)\\nThus, our effective starting noise is defined as:\\nNfinal =\\n(\\nN1, overlapping regions\\nN2, non overlapping regions\\nWe perform the effective noising operation parallely with\\nrespect to the reference view. First we take view 1, warp to\\nview 2. then add noise, then we Although we use CAT3D,\\nour method is generalizable across any 3D scene generation\\nmodel.\\nUnderstanding the value that synthetic data from gen-\\nerative models can bring, we propose a method to en-\\nhance diffusion-based 3D generative models to produce\\nhigh-quality, 3D-consistent results.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='7.2. Loss functions\\nSimilar to LVSM, we utilize Mean Square Error (MSE) loss\\nfor training our network. Instead of using Perceptual Loss,\\nwe utilize LPIPS loss for training. Given the ground-truth\\ntarget view of dimension ˆI ∈RH×W ×C and the recon-\\nstructed target view I, the effective objective function used\\nfor optimization is defined as:\\n  L = MS E(I , \\\\ h at {I}) + \\\\lambda \\\\cdot LPIPS(I, \\\\hat {I}) \\n(12)\\nwhere λ is a scaling factor set to 0.5 for all experiments.\\n7.3. Emergent Properties\\nOne surprising emergent property of our newly proposed\\ntransformer block is its ability to disentangle the source\\nand target tokens, which allows it to scale better for syn-\\nthetic data compared to a naive transformer block.\\nWe\\npresent these results in Figure X, where we observe sig-\\nnificant improvements. We hypothesize that this emergent\\nproperty arises because synthetic data is generally prone to\\nartifacts and out-of-distribution noise. When transformer\\nblocks cannot distinguish between source and target tokens,\\nthe model learns using both real and synthetic data, leading\\nto reconstructions that inherit these artifacts. However, in\\nour case, only the relevant information from clean images\\nis used for backpropagation, allowing the model to utilize\\nuseful context from synthetic data while discarding artifacts\\nduring token fusion for target view generation.\\n8. Limitations\\nOur model struggles when regions occluded in the source\\nimages become visible in the target view. As shown in Fig-\\nure 17, when a new object enters the scene, the model hallu-\\ncinates the affected region. We argue that this phenomenon\\nis inherently ill-posed and lacks a definitive solution. Ad-\\nditionally, the model uses a token size of 8 for all blocks,\\nresulting in 1024 tokens per source image, which demands\\nsignificant memory.\\nWe leave further architectural opti-\\nmizations, such as hierarchical transformers and more ef-\\nficient networks like linear attention and state-space models\\n(e.g., Mamba [12], [50]), for future work.\\n9. Failure cases of our method\\nWe notice that our method contains two main failure modes\\n(1) when an new object comes into the view in between the\\nconditioned frames. (2) When too many shiny artifacts are\\npresent in the image\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Figure 7. Figure illustrating results from DL3DV dataset trained with our synthetic data. The first 2 images represent the input views.\\nthird presents results of LVSM, Fourth represents our results and fifth the ground truth\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Figure 8. Figure illustrating results from Re10k dataset trained with our synthetic data. The first 2 images represent the input views.\\nthird presents results of LVSM, Fourth represents our results and fifth the ground truth\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='Figure 9. Figure illustrating results from ACID dataset trained with our synthetic data. The first 2 images represent the input views.\\nthird presents results of LVSM, Fourth represents our results and fifth the ground truth\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Figure 10. Figure illustrating the regions where our method works better than LVSM for Re10K dataset. The figures are in the order\\nRow 1:- LVSM, Row 2:- OURS Row 3:- GT, Row 4:- Difference between LVSM and Ours\\nFigure 11. Figure illustrating the regions where our method works better than LVSM for Re10K dataset. The figures are in the order\\nRow 1:- LVSM, Row 2:- OURS Row 3:- GT, Row 4:- Difference between LVSM and Ours\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Figure 12. Figure illustrating the regions where our method works better than LVSM for Re10K dataset. The figures are in the order\\nRow 1:- LVSM, Row 2:- OURS Row 3:- GT, Row 4:- Difference between LVSM and Ours\\nFigure 13. Figure illustrating the regions where our method works better than LVSM for Re10K dataset. The figures are in the order\\nRow 1:- LVSM, Row 2:- OURS Row 3:- GT, Row 4:- Difference between LVSM and Ours\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='Figure 14. Figure illustrating the regions where our method works better than LVSM for Re10K dataset. The figures are in the order\\nRow 1:- LVSM, Row 2:- OURS Row 3:- GT, Row 4:- Difference between LVSM and Ours\\nFigure 15. Figure illustrating the regions where our method works better than LVSM for Re10K dataset. The figures are in the order\\nRow 1:- LVSM, Row 2:- OURS Row 3:- GT, Row 4:- Difference between LVSM and\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Figure 16. Figure illustrating the regions where our method works better than LVSM for Re10K dataset. The figures are in the order\\nRow 1:- LVSM, Row 2:- OURS Row 3:- GT, Row 4:- Difference between LVSM and Ours\\nFigure 17. Figure illustrating failure cases of our method. Our method fails to perform well if there are occluded objects coming into\\nthe scene. Figure ordering is OURS, GT, DIFF\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content='Figure 18. Figure illustrating failure cases of our method. Our method fails to perform well if there are occluded objects coming into\\nthe scene. Figure ordering is OURS, GT, DIFF\\nFigure 19. Figure illustrating failure cases of our method. Our method fails to perform well if there are occluded objects coming into\\nthe scene. Figure ordering is OURS, GT, DIFF\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'file_path': '..\\\\data\\\\pdf\\\\scaling-transformer-based-synthesis-models-token-disentanglement.pdf', 'total_pages': 21, 'format': 'PDF 1.6', 'title': 'Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data', 'author': 'Nithin Gopalakrishnan Nair; Srinivas Kaza; Xuan Luo; Vishal M. Patel; Stephen Lombardi; Jungyeon Park', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='Figure 20. Figure illustrating failure cases of our method. Our method fails to perform well if there are occluded objects coming into\\nthe scene. Figure ordering is OURS, GT, DIFF\\nFigure 21. Figure illustrating failure cases of our method. Our method fails to perform well if there are occluded objects coming into\\nthe scene moreover, our method also fails to reconstruct properly when there are some shiny obejcts in the scene. Figure ordering is OURS,\\nGT, DIFF\\n21')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## load all the pdf files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\", ## Pattern to match files\n",
    "    loader_cls=PyMuPDFLoader, ## Loader class to use\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a52468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3855c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\excel\\\\CollectionCentersMuthootFinal.xls'}, page_content='S. No. Sl. No. State City Axis Dhanlaxmi HDFC ICICI IDBI Indusind Total 1 1 UP Agra 0 1 1 2 2 Guj Ahmedabad 1 1 1 0 1 4 3 3 RAJSTHAN Ajmer 0 0 0 1 1 4 4 UP Allahabad 0 0 1 1 5 5 HARYANA Ambala 0 0 1 0 1 6 6 Punjab Amritsar 0 0 7 7 Guj Anand 0 0 0 0 8 8 WEST BENGAL Asansol 0 1 0 1 9 9 MAHARASHTRA Aurangabad 0 0 0 10 10 KARNATAKA Bangalore 1 1 1 3 11 11 UP Bareilly 0 0 0 12 12 KARNATAKA Belgaum 0 1 0 0 1 13 13 BIHAR Bhagalpur 0 0 0 0 0 14 14 Guj Bharuch 1 0 1 0 2 15 15 Punjab Bhatinda 0 0 0 0 16 16 Guj Bhavnagar 0 1 1 17 17 CHATTISGARH Bhilai 0 0 0 0 18 18 RAJSTHAN Bhilwara 0 1 0 0 1 19 19 MP Bhopal 1 1 20 20 ORISSA Bhubaneswar 1 0 1 0 2 21 21 KARNATAKA Bijapur 1 0 0 0 0 0 1 22 22 RAJSTHAN Bikaner 0 1 0 0 1 23 23 WEST BENGAL Burdwan 0 1 0 0 1 24 24 KERALA Calicut 1 1 0 0 2 25 25 Punjab Chandigarh 1 1 1 3 26 26 TAMILNADU Chennai 1 1 1 1 1 1 6 27 27 TAMILNADU Coimbatore 1 1 28 28 ORISSA Cuttack 0 1 0 0 1 29 29 WEST BENGAL Davanagere 0 1 0 0 0 1 30 30 UTTARANCHAL Dehradun 0 1 1 2 31 31 BIHAR Dhanbad 0 1 0 1 32 32 WEST BENGAL Durgapur 1 0 0 1 33 33 TAMILNADU Erode 0 1 0 0 1 34 34 HARYANA Faridabad 0 0 1 1 35 35 Guj Gandhinagar 0 0 0 0 0 36 36 UP Ghaziabad 0 0 0 37 37 UP Gorakhpur 0 1 0 1 38 38 KARNATAKA Gulbarga 1 0 0 0 0 0 1 39 39 AP Guntur 0 0 1 0 1 40 40 HARYANA Gurgaon 1 0 0 1 1 3 41 41 ASSAM Guwahati 0 1 1 42 42 MP Gwalior 0 1 0 1 44 43 HARYANA Hisar 0 1 0 1 45 44 KARNATAKA Hubli 0 1 1 46 45 AP Hyderabad 1 1 1 3 47 46 MP Indore 1 0 1 2 48 47 MP Jabalpur 1 0 0 1 49 48 RAJSTHAN Jaipur 1 1 1 3 50 49 Punjab Jalandhar 0 0 51 50 Jammu and Kashmir Jammu 0 0 0 0 52 51 Guj Jamnagar 1 0 0 1 2 53 52 BIHAR Jamshedpur 0 1 1 54 53 RAJSTHAN Jodhpur 1 1 55 54 AP Kakinada 0 1 0 0 1 56 55 UP Kanpur 0 1 1 1 3 57 56 HARYANA Karnal 1 0 0 0 1 58 57 KERALA Kochi/Ernakulam 1 1 1 0 3 59 58 MAHARASHTRA Kolhapur 0 1 1 60 59 WEST BENGAL Kolkata 1 1 1 1 1 1 6 61 60 RAJSTHAN Kota 0 0 1 1 62 61 UP Lucknow 1 1 2 63 62 Punjab Ludhiana 0 1 1 64 63 TAMILNADU Madurai 1 1 0 2 65 64 KARNATAKA Mangalore 1 0 0 1 66 65 UP Meerut 1 0 0 0 1 67 66 PUNJAB Mohali 0 0 0 0 1 1 68 67 MAHARASHTRA Mumbai 1 1 1 1 1 1 6 69 68 UP Muzaffarnagar 0 0 0 0 0 70 69 KARNATAKA Mysore 1 1 2 71 70 MAHARASHTRA Nagpur 0 1 1 2 72 71 MAHARASHTRA Nashik 1 0 1 73 72 Guj Navsari 0 1 0 1 74 73 AP Nellore 0 1 0 0 1 75 74 NEW DELHI NewDelhi/Delhi 1 1 1 1 1 1 6 76 75 UP Noida 1 0 0 1 0 2 77 76 GOA Panaji 1 0 1 78 77 Punjab Patiala 0 0 0 79 78 BIHAR Patna 0 0 0 80 79 PONDICHERRY Pondicherry 0 1 1 0 2 81 80 MAHARASHTRA Pune 1 1 1 3 82 81 KARNATAKA Raichur 1 0 0 0 0 0 1 83 82 CHATTISGARH Raipur 0 1 0 1 84 83 AP Rajahmundry 0 1 0 0 0 1 85 84 Guj Rajkot 0 1 1 86 85 JHARKHAND Ranchi 0 1 1 87 86 HARYANA Rohtak 0 0 0 0 88 87 TAMILNADU Salem 1 1 2 89 88 MEGHALAYA Shilong 0 1 0 0 0 1 90 89 HIMACHAL PRADESH Shimla 0 0 0 0 91 90 Karnataka Shimoga 0 0 0 0 92 91 MAHARASHTRA Sholapur 0 1 0 0 0 1 93 92 WEST BENGAL Siliguri 1 0 0 0 1 94 93 HARYANA Sirsa 0 0 0 0 0 0 95 94 Guj Surat 1 1 1 3 95 Guj Surendranagar 0 1 0 0 0 1 96 MAHARASHTRA Thane (mumbai) 1 0 0 0 0 1 97 KERALA Thiruvananthapuram 1 0 0 1 98 AP Tirupati 0 1 0 0 0 1 99 TAMILNADU Tirupur 0 0 0 0 x 100 TAMILNADU Trichur 1 1 0 0 2 101 TAMILNADU Trichy 1 1 0 2 102 RAJSTHAN Udaipur 0 0 1 1 103 KARNATAKA Udupi 0 1 0 0 0 1 104 Guj Vadodara 1 1 1 3 105 Guj Valsad 0 0 0 106 UP Varanasi 1 0 1 107 AP Vijayawada 1 1 108 AP Visakhapatnam 1 1 Total 25 13 40 33 14 21 146\\n\\nS. No. Sl. No. State City Axis Dhanlaxmi HDFC ICICI IDBI Indusind Total 1 1 UP Agra 1 1 2 2 Guj Ahmedabad 1 1 1 1 4 3 3 RAJSTHAN Ajmer 1 1 4 4 UP Allahabad 1 1 5 5 HARYANA Ambala 1 1 6 6 Punjab Amritsar 1 1 7 7 Guj Anand 1 1 8 8 WEST BENGAL Asansol 1 1 9 9 MAHARASHTRA Aurangabad 1 1 10 10 KARNATAKA Bangalore 1 1 1 1 4 11 11 UP Bareilly 1 1 12 12 KARNATAKA Belgaum 1 1 13 13 BIHAR Bhagalpur 1 1 14 14 Guj Bharuch 1 1 15 15 Punjab Bhatinda 1 1 16 16 Guj Bhavnagar 1 1 2 17 17 CHATTISGARH Bhilai 1 1 18 18 RAJSTHAN Bhilwara 1 1 19 19 MP Bhopal 1 1 20 20 ORISSA Bhubaneswar 1 1 2 21 21 KARNATAKA Bijapur 1 1 22 22 RAJSTHAN Bikaner 1 1 23 23 WEST BENGAL Burdwan 1 1 24 24 KERALA Calicut 1 1 2 25 25 Punjab Chandigarh 1 1 1 3 26 26 TAMILNADU Chennai 1 1 1 1 1 1 6 27 27 TAMILNADU Coimbatore 1 1 2 28 28 ORISSA Cuttack 1 1 29 29 KARNATAKA Davanagere 1 1 30 30 UTTARANCHAL Dehradun 1 1 2 31 31 BIHAR Dhanbad 1 1 32 32 WEST BENGAL Durgapur 1 1 33 33 TAMILNADU Erode 1 1 34 34 HARYANA Faridabad 1 1 2 35 35 Guj Gandhinagar 1 1 36 36 UP Ghaziabad 1 1 2 37 37 UP Gorakhpur 1 1 38 38 KARNATAKA Gulbarga 1 1 39 39 AP Guntur 1 1 40 40 HARYANA Gurgaon 1 1 1 3 41 41 ASSAM Guwahati 1 1 42 42 MP Gwalior 1 1 44 43 HARYANA Hisar 1 1 45 44 KARNATAKA Hubli 1 1 46 45 AP Hyderabad 1 1 1 3 47 46 MP Indore 1 1 2 48 47 MP Jabalpur 1 1 49 48 RAJSTHAN Jaipur 1 1 1 1 4 50 49 Punjab Jalandhar 1 1 51 50 Jammu and Kashmir Jammu 1 1 52 51 Guj Jamnagar 1 1 2 53 52 BIHAR Jamshedpur 1 1 54 53 RAJSTHAN Jodhpur 1 1 55 54 AP Kakinada 1 1 56 55 UP Kanpur 1 1 1 3 57 56 HARYANA Karnal 1 1 58 57 KERALA Kochi/Ernakulam 1 1 1 3 59 58 MAHARASHTRA Kolhapur 1 1 60 59 WEST BENGAL Kolkata 1 1 1 1 1 1 6 61 60 RAJSTHAN Kota 1 1 62 61 UP Lucknow 1 1 1 3 63 62 Punjab Ludhiana 1 1 64 63 TAMILNADU Madurai 1 1 2 65 64 KARNATAKA Mangalore 1 1 66 65 UP Meerut 1 1 67 66 PUNJAB Mohali 1 1 68 67 MAHARASHTRA Mumbai 1 1 1 1 1 1 6 69 68 UP Muzaffarnagar 1 1 70 69 KARNATAKA Mysore 1 1 2 71 70 MAHARASHTRA Nagpur 1 1 2 72 71 MAHARASHTRA Nashik 1 1 73 72 Guj Navsari 1 1 74 73 AP Nellore 1 1 75 74 NEW DELHI NewDelhi/Delhi 1 1 1 1 1 1 6 76 75 UP Noida 1 1 2 77 76 GOA Panaji 1 1 78 77 Punjab Patiala 1 1 79 78 BIHAR Patna 1 1 80 79 PONDICHERRY Pondicherry 1 1 2 81 80 MAHARASHTRA Pune 1 1 1 3 82 81 KARNATAKA Raichur 1 1 83 82 CHATTISGARH Raipur 1 1 84 83 AP Rajahmundry 1 1 85 84 Guj Rajkot 1 1 1 3 86 85 JHARKHAND Ranchi 1 1 87 86 HARYANA Rohtak 1 1 88 87 TAMILNADU Salem 1 1 2 89 88 MEGHALAYA Shilong 1 1 90 89 HIMACHAL PRADESH Shimla 1 1 91 90 Karnataka Shimoga 1 1 92 91 MAHARASHTRA Sholapur 1 1 93 92 WEST BENGAL Siliguri 1 1 94 93 HARYANA Sirsa 1 1 95 94 Guj Surat 1 1 1 1 4 95 Guj Surendranagar 1 1 96 MAHARASHTRA Thane (mumbai) 1 1 97 KERALA Thiruvananthapuram 1 1 98 AP Tirupati 1 1 99 TAMILNADU Tirupur 1 1 x 100 KERALA Trichur 1 1 2 101 TAMILNADU Trichy 1 1 2 102 RAJSTHAN Udaipur 1 1 103 KARNATAKA Udupi 1 1 104 Guj Vadodara 1 1 2 105 Guj Valsad 1 1 106 UP Varanasi 1 1 107 AP Vijayawada 1 1 108 AP Visakhapatnam 1 1 Total 28 19 42 32 20 33 174\\n\\nS. No. Sl. No. State City Axis sol id Branch Address Contact Person name and number 1 2 Guj Ahmedabad 1 003 TRISHUL-OPPOSITE SAMARTHESHWAR TEMPLELAW GARDEN, ELLIS BRIDGEAHMEDABAD 380006 GUJARAT Parag Muchalla-(079) 66306116/173/174/9427231432, 9825041432 2 10 KARNATAKA Bangalore 1 009 NO. 9, M.G. ROAD,BLOCK ABANGALORE 560001 KARNATAKA 080-25370638/ 646 /mithun 9880490023 3 16 Guj Bhavnagar 1 200 PLOT NO. 4/B, VASUNDHARA COMPLEXOPP. DAKSHINAMURTHY SCHOOL,WAGHAWADI ROAD BHAVNAGAR 364002 GUJARAT Jatin Gandhi 9998141181 8980802002-AMIT OPHD 4 20 ORISSA Bhubaneswar 1 024 ARCHBISHOPS HOUSESATYANAGARBHUBANESHWAR 751007 ODISHA 9437106311-SWARNAPRABHA 5 21 KARNATAKA Bijapur 1 525 1ST FLOOR, V K G COMPLEXM G ROADBIJAPUR 586101 KARNATAKA Shrinivasan- 08352-224194/95/96/9916871327-srinivas 6 24 KERALA Calicut 1 136 AXIS BANK LTD, MARINA MALLYMCA CROSS ROADKOZHIKODE 673001 KERALA Venkat- 9995390776 7 26 TAMILNADU Chennai 1 006 82 DR RADHAKRISHNAN SALAIMYLAPORE, CHENNAICHENNAI 600004 TAMIL NADU Jakir Hussain H-9884159985/7845845758JEROD 044-28306828-jitendra 8 34 HARYANA Faridabad 1 348 SHOP NO.-6, CROWN COMPLEX,NEIGHBOURHOOD NO.2, 1-2 CHOWK, N.I.T.,FARIDABAD 121001 HARYANA 9562366665- santosh; 09999077205-SHARADA 0129-4039739/40/41 9 36 UP Ghaziabad 1 095 PLOT NO 3,N-30 AMBEDKAR ROAD NEHRUNAGAR GHAZIABAD (U.P.)GHAZIABAD 201001 UTTAR PRADESH 0120-2757115/120 9582800952 10 38 KARNATAKA Gulbarga 1 342 JAWALI COMPLEXSUPER MARKET ,GULBARGAGULBARGA 585101 KARNATAKA OPRH 8095503422 11 40 HARYANA Gurgaon 1 056 SCO-29, SECTOR-14,NEAR HUDA OFFICE,OLD DELHI-GURGAON RDGURGAON 122001 HARYANA 124-2308751-ganesh/9811207977-amit 9811683279-manish ophd(leave till 11-12-11) 12 46 MP Indore 1 043 KAMAL PALACE, 1 YESHWANT COLONYYESHWANT NIWAS ROADINDORE 452003 MADHYA PRADESH Arpita-07314295208/207 9893296689-ANIDYA BHATNAGAR 13 47 MP Jabalpur 1 128 GRD & 1ST FLR,PANCHRATNA BLGPLOT NO.902,MODEL RD,WRIGHT TOWNJABALPUR 482002 MADHYA PRADESH Gauri tiwari- 9893912928 09669984999-yogendra 14 51 Guj Jamnagar 1 175 JAIDEV ARCADE,GRND FLR,PARK CLY MAINMAIN RD,NR JOGGERS PARKJAMNAGAR 361008 GUJARAT varsha- 9228406338 9662277285-ZARA 15 56 HARYANA Karnal 1 394 3/250,1,SHAKTI COLONY,MALL ROAD,KARNALKARNAL 132001 HARYANA 9896750720-vikas 16 57 KERALA Kochi/Ernakulam 1 081 41/419, GROUND FLOOR CHICAGO PLAZA,RAJAJI ROAD, ERNAKULAMKOCHI 682035 KERALA Venkitachalam Anand 9895398014 17 59 WEST BENGAL Kolkata 1 005 7, SHAKESPEARE SARANIKOLKATAKOLKATA 700071 WEST BENGAL 033-22824869 Siddhart- 9830463389, 22824989 9831005141- arpit 18 64 KARNATAKA Mangalore 1 077 ESSEL TOWER , GROUND FLOORBUNTS HOSTEL CIRCLEMANGALORE 575003 KARNATAKA Saujanya-8242410981/8050113676 19 65 UP Meerut 1 177 HOTEL CRYSTAL PALACE, G-2/47 CIVILLINES, BOUNDARY ROAD, MEERUT DIST.MEERUT 250001 UTTAR PRADESH Siddharth Mehta-9536901772 0121-2664526/2664519/2664404 20 67 MAHARASHTRA Mumbai 1 004 UNIVERSAL INSURANCE BUILDING,GROUND FLOORSIR P M ROAD, FORTMUMBAI 400001 MAHARASHTRA Nachiket-022-66107265 Pravin - 9833260932 21 71 MAHARASHTRA Nashik 1 115 MAZDA TOWERS, TRYAMBAK NAKA F.P.NO.183, C.T.SNO.620/9 GPO ROAD, NASHIKNASHIK 422001 MAHARASHTRA nilesh patil-0253-6627404, 9822974396 nilesh 22 74 NEW DELHI NewDelhi/Delhi 1 007 STATESMAN HOUSE,148, BARAKHAMBA ROADNEW DELHI 110001 DELHI Neha.Munjal - 011-47425118 neha-9999113382 23 75 UP Noida 1 022 B2-B3, SECTOR 16NOIDA, U.P.NOIDA 201301 UTTAR PRADESH Priyanka Upadhyay 9818954356/0120-2510751/52 24 81 KARNATAKA Raichur 1 412 MPL NO.1-10-151 & 152/1KUBERA PALACE,STATION ROADRAICHUR 584101 KARNATAKA 9449920895 rasish OPRH 8095504122 // 9449920895 25 92 WEST BENGAL Siliguri 1 035 SPECTRUM HOUSESEVOKE ROAD, SILIGURI,SILIGURI 734401 WEST BENGAL 9832011628- tapasnath 0353-2642745 / 8001600352/kirti- 9832474864 26 93 HARYANA Sirsa 1 609 PLOT NO 17/339 & PLOT NO. 17/340, SANGWANNURSING HOME CPLX,SANGWAN CHOWK, DABWALI ROADSIRSA 125055 HARYANA 01666-235192/3/4 8053106092 27 96 MAHARASHTRA Thane (mumbai) 1 061 DHIRAJ BAUG, (NEAR HARI NIWAS CIRCLE)LBS MARG, THANE (WEST).THANE 400602 MAHARASHTRA 022-66905300/5301 022-66905304-PANKAJA/5301 28 97 KERALA Thiruvananthapuram 1 113 TC2/2421 , CONDOR BUILDING ,PATTOM POTHIRUVANANTHAPURAM 695004 KERALA ARATHI K 0471-4400721/720 Total 28\\n\\nSr No City Type Branch Code BM Name BM E-mail Id Contact No BOM Name BOM E-mail ID Contact No Additional Staff E-mail ID Contact No Address Landline 1 Ahmedabad Branch 143 Vishal Upmanyu vishal.upmanyu@dhanbank.co.in 9909008373 Anandsinh Jhala anandsinh.jhala@dhanbank.co.in 9099976003 Heena Shah heenatshah@dhanbank.co.in 9825038099 DHANLAXMI BANK, 3, MOTILAL CHAMBERS , INCOME TAX CIRCLE, ,NEAR \\'SALES INDIA\\',ASHRAM ROAD ,143- AHMEDABAD ,AHMEDABAD DIST, GUJARAT - 380 009 079 - 64502690 , 64502692, 64502694 2 Bhopal Branch 226 Nand Kishore Piryani nandkishore.piryani@dhanbank.co.in 9584883000 Pradyumn Sharma pradyumn.sharma@dhanbank.co.in 9981175763 Amitkumar Singh amitkumar.singh@dhanbank,co.in 9926794894 DHANLAXMI BANK , GROUND FLOOR, VNV PLAZA, PLOT NO:6, , ZONE -2, MAHARANA PRATAP NAGAR , BHOPAL ,MADHYA PRADESH - 462 011 , 0755 - 6459927, 6459937 3 Chandigarh Branch 219 Nandit Sood nandit.sood@dhanbank.co.in 8054017788 Malvika Gadhok malvika.gadhok@dhanbank.co.in 9988800565 Kuldeep Kumar kuldeep.kumar@dhanbank.co.in 9779279790 Gr Floor, Rai Building, SCO 93&94, Sector - 17 B&C, Near K C Cinema, Chandigarh - 160017 0172 - 6538924/ 25/ 26 4 Chennai Branch 54 Dilip Kumar J dilipkumar.j@dhanbank.co.in 9176832859 Mr.Ckm Kumar kumar.ckm@dhanbank.co.in 9176773613 Bharanidharan T bharanidharan.t@dhanbank.co.in 9884388629 DHANLAXMI BANK LTD, P.B.NO.359 , 104 & 107,OM SAKTHI TOWERS ,MOUNT ROAD, ,ANNA SALAI, CHENNAI ,TAMIL NADU - 600 002 044-64530014 / 119 / 124 RPC 141 J Srinivasa Narasimhan srinivasanarasimhan@dhanbank.co.in 9176904126 G. Pugazhendhi pugazhendhi.g@dhanbank.co.in 9884388629 Prasanna B prasanna.b@dhanbank.co.in 5 Coimbatore Branch 62 Mr.Doctor Hakkim Khan doctorhakkim.khan@dhanbank.co.in 9626248104 Pradeep babu pradeepbabu.r@dhanbank.co.in 9626251659 461, Vivekananda Road, Ram Nagar, 62- Coimbatore, Tamil Nadu -641 009 RPC 993 Chidhambaranatha Pillai.M chidambaranatha pillai m/dlb 9176832844 Antony irudayaraja antonyirudayarajaa@dhanbank.co.in 9626248133 Santha meena c santhameena.c@dhanbank.co.in 9944030233 6 Hyderabad Branch 188 Suhasini A suhasinia@dhanbank.co.in 9160020305 Ravidra Maley ravidra.maley@dhanbank.co.in 9160020378 Premanth premanth.y@dhanbank.ci.in 9160020309 23/A, Sai sushma homes, main road, S.R. Nagar, opp. S.r. nagar Police station Hyderabad - 500 038 040-64636991/ 992/993 RPC 245 Madhavi madhavi.mallikarjune@dhanbank.co.in 9160020309 Hima Bindu K himabinduk@dhanbank.co.in 9160020315 7 Jaipur Branch 218 Sneh Walia sneh.walia@dhanbank.co.in 9829053439 Navneet Sharma navneet.sharma@dhanbank.co.in 9351910879 Ground Floor, G-3-4-5-6-7, Royal World, Opp. City Centre,Sansar Chand Road,Jaipur Rajasthan - 302 001. 0141-6501789/790/791 8 Jalandhar Branch 220 Mandeep Sharma mandeep.sharma1@dhanbank.co.in 8054017313 Rajiv Kumar rajiv.kumar@dhanbank.co.in 9780011219 Ground Floor, 465 - A, New Jawahar Nagar, Jalandhar, Punjab - 144 001 0181 - 6450562, 6450569, 6450573 9 Kochi/Ernakulam Branch 38 Ajit Gopal ajitk.gopal@dhanbank.co.in 9539004162 Mini Gopakumar mini.gopakumar@dhanbank.co.in 9539004239 DHANLAXMI BANK, 32/2383, KMM BUILDING, OPPOSITE KSEB ,S N JUNCTION , 38- PALARIVATTOM ,ERNAKULAM DIST ,KERALA - 682 025 , 0484-6453447 ,6453441 RPC 157 Arun Nair Arun.Nair@dhanbank.co.in 9539009699 Rajan Vl rajan.vl@dhanbank.co.in 9539003788 DHANLAXMI BANK, 32/2383, KMM BUILDING, OPPOSITE KSEB ,S N JUNCTION , 38- PALARIVATTOM ,ERNAKULAM DIST ,KERALA - 682 025 , 0484-6453447 ,6453441 10 Kolkata Branch 154 Susanta Bhowmik susanta.bhowmik@dhanbank.co.in 9674741703 Ramakrishnan K S ramakrishnanks@dhanbank.co.in 9674165252 Rejitha Chandran rejithachandran@dhanbank.co.in 9674165259 Ideal Plaza, Ground Floor, 11/1, Sarat Bose Road, 154- Kolkata, West Bengal - 700 020 033 - 22815100 RPC 905 B.K. Subrahmanian subramanianbk@dhanbank.co.in 9674165256 Debashish Mishra debashish.mishra@dhanbank.co.in 9674741704 11 Lucknow Branch 216 Vipul Kumar Vipul.Kumar@dhanbank.co.in 99184 6999 Kashif Lohani kashif.lohani@dhanbank.co.in 9721453412 Mohit Kaul mohit.kaul@dhanbank.co.in 9654957992 DHANLAXMI BANK , HALWASIYA HOUSE, , NO: 11, M G ROAD, HAZRATGANJ ,LUCKNOW ,UTTAR PRADESH - 226 001 , 0522 - 6569040, 6569042,6569041 12 Mumbai - Fort Branch 144 Ramesh Menon ramesh.menon@dhanbank.co.in 9167832288 Mathew Dasan mathew.dasan@dhanbank.co.in 9619490118 The Dhanlaxmi Bank GROUND FLOOR,JANMABHOOMI BHAVAN,PLOT 11-12 ,JANMABHOOMI MARG ,144- FORT, MUMBAI ,MAHARASHTRA - 400 001 022- 22871658, 22022943 Controlling Branch 144 Niranjan Ketkar niranjan.ketkar@dhanbank.co.in 9167390881 Nilesh Mahadik nilesh.mahadik@dhanbank.co.in 9619224452 Swapnil Kurte swapnil.kurte@dhanbank.co.in 9930558716 022 - 2202535 RPC 164 Mayur mayur.pandya@dhanbank.co.in 9619858103 Harshad Rane harshad.rane@dhanbank.co.in 9167390838 307, Jolly Bhavan 2 | Opp. Nirmal Niketan College| New Marine Lines | Mumbai 20. Mumbai-Dombivali Branch 209 ANITA IYER anita.iyer@dhanbank.co.in 9619490167 SHRADHA PANDURANG SAWANT shradha.sawant@dhanbank.co.in 9619490155 Gr. Floor, Mirage Arcade, Opp. Ganesh Temple, Nehru Road, Dombivli (East), Thane, maharashtra - 421 201 0251 - 6459503 /04 /05 Mumbai - Borivali Branch 201 Winniefred Dsouza winniefred.dsouza@dhanbank.co.in 9619490082 Sunil Andrew Rodrigues SunilAndrew.Rodrigues@dhanbank.co.in Gr Floor, 2/3/4, Kapoor Apt, Chandavarkar Road, Boriwali (W), Mumbai 13 Mysore Branch 223 Satish S P satishwayam.prakash@dhanbank.co.in 9742264146 Priya Rajakumar priya.rajkumar@dhankbank.co.in 9742264145 Santhosh Kumar V santosh.kumar@dhanbank.co.in 9008355883 Sri Lakshmi Arcade, D No:14, 4Th Main,8Th Cross, Kamakshi Hospital Road, Saraswathypuram, Mysore-570009 821 - 6450490/91/93 14 NewDelhi Branch 170 Rajat Sharma rajat.sharma@dhanbank.co.in 9582289769 Prashant Kumar prashant.kumar@dhanbank.co.in 9582158904 DHANLAXMI BANK , 16/15, W.E.A., J.S.PLAZA , ARYA SAMAJ ROAD , KAROL BAGH, NEW DELHI - 110 005 , 64508887, 64507036, 64506070, 64509992, 64505419, 64508708 RPC 992 Arvinder Palsingh arvindpal.singh@dhanbank.co.in 9582158546 Puneet Batra puneet.batra@dhanbank.co.in 9582214160 15 Panaji Branch 246 David De Souza david.desouza@dhanbank.co.in 9823141263 Narayan Bhaskar Dholye narayan.dholye@dhanbank.co.in 8007771679 Nilangi Umesh Naik nilangiumesh.naik@dhanbank.co.in Ground Floor, Behind Hotel Fidalgo, M.G. Road, panaji Goa - 403001 0832-6451286 16 Pune Branch 231 Bhavesh Nayak bhavesh.nayak@dhanbank.co.in 9049997056 Himangini Joshi himangini.joshi@dhanbank.co.in 9049000687 Sumit Kumar sumit.kumar1@dhanbank.co.in 7387990498 The Dhanlaxmi Bank Ground & 1st Floor, Radiant Arcade, M.G. Road, (East Street), Pune 411001 020 - 6400105, 6400106 17 Surat Branch 142 Vijay Singh vijay.singh@dhanbank.co.in 9879503936 AMIT HOTE amithote@dhanbank.co.in 9619490078 Manishpanchal manish.panchal@dhanbank.co.in 9998018559 142, 3Rd Floor,The Surat, Vankar Sahakari Sangh Building, Opp. Reshamwala Market, Ring Road, 142- Surat, Surat Dist, Gujarat - 395 003 18 Trichur Branch 4 Sreekanth V V sreekanth.vv@dhanbank.co.in 9539004385 Vidhya H vidhyah@dhanbank.co.in 9539003834 Dhanlaxmi Bank, Ground Floor, , Dhanalakshmi Complex, 4- Pushpagiri, DLB Bhavan, Punkunnam, Thrissur ,Kerala - 680 002 0487 - 6453956, 6453957, 6454011 RPC 159 Rekha T R rekhatr@dhanbank.co.in 9539003849 Sreeraman N S sreeramanns@dhanbank.co.in 9539007965 Sasikumar T P sasikumartp@dhanbank.co.in 9847084097 19 Varanasi Branch 222 Ashish Kumar Singh ashishkumar.singh@dhanbank.co.in 9721453405 Rana Yashpal Singh rana.yashpalsingh@dhanbank.co.in 9721453425 Abhishek Ojha abhishek.ohja@dhanbank.co.in 9721453404 Ground Floor, Shakti Sikha Complex, Sigra, Varanasi, Uttar Pradesh - 221 010 0542 -6452494, 6452495, 6452496\\n\\nSR No Branch Name Address City PIN CODE Telephone Numbers( Land line & RIM) Contact Persons 1 Agra First Floor,Pariney Garden,Bhagfarjana, Civil Lines, Agra - 282002. Agra 282002 0562 - 4010382 Anil Kumar Sharma / Manjesh Porwal 2 Asansol P.C Chatterjee Market , rambandhu talaw , asansol , pin - 713303 Asansol 713303 0341-2315179 , 9332277661 Mr. Arupendu Sengupta 3 Bangalore HDFC BANK LTD., Cash Management Services \"SALCO CENTRE\" # 8/24, Richmond Road BANGALORE 560025 8066633131 09343790037 Somashekar, Sam Abraham & Krishnaiah Murthy 4 Bhagalpur Triveni appartment, Dr. R. P ROAD ,BHAGALPUR BHAGALPUR 812002 9334391764 PRASHANT KUMAR 5 Bhilwara HDFC Bank Ltd., WBO, Shop no. 1-2-3-4, \"A\" Block, First Floor, SK Plaza, Pur Road Bhilwara (Raj.) 311001 Bhilwara 311001 01482-512686, 9351302102 Ambarish Singh Yadav, Mayank Sharma, Amit Kumar Verma 6 Bhubaneshwar C111, Business park, 1st Floor, Sahid Nagar Bhubaneswar 751007 0674 2543486 / 0 9338182087 Ashim Banerjee 7 Bikaner HDFC BANK LTD. ROSHAN PLAZA, RANI BAZAR, BIKANER BIKANER 334001 0151-5130042, 09314232073 Vijender Wadha, Divya Kothari 8 Burdwan 45 GT ROAD/BURDWAN-713101 BURDWAN 713101 0342-2566355/9333744965 SUDIP CHATTERJEE 9 Calicut HDFC BANK LTD, III FLOOR, SIMAX TOWERS, KANNUR ROAD, NADAKKAVE, CALICUT CALICUT 673011 9349206616, 9388241847/ 0495- 4433154,155 Anoop / Jose P varghese 10 Chandigarh sco-189-190 Sector 17 c Chandigharh 160017 0172-4603770-5088306/09316175094 Harish Bhardwaj/Sanjeev Parmar/Bhatnagar Rahul 11 Chennai No. 115, Dr. Radhakrishnan Salai, 2nd Floor, Opp. to CSI Kalyani Hospital, Mylapore, Chennai - 600004. Chennai 600002 9381750927 Miriyala Balaji , Bhuvana 12 Cuttack HDFC Bank Ltd, Holding No 32, 32/A Bajrakabati Road, Cuttack ,Orissa - 753 00 CUTTACK 753001 0671-2332744 / 9338016590 SABYASACHI DASH / RUPESH PATNAIK 13 Davangere #651 B H M Enclave, HM Road, Mandipet,Davangere Davangere 577001 08192-232781/82 Mahesh Palled/Siddharth D Kamat 14 Dhanbad SRI RAM PLAZA , 1ST FLOOR, BANK MORE DHANBAD JHARKHAND 826 001 (0326) 2308831 Joshi/ rupam 15 Durgapur Balai Commercial Complex,3rd Floor. Benachity,Nachan Road. DURGAPUR 713213 0343-2588501/9330038188 Snehamay De/ Jaideep Bose/ Soumen Dutta/ Subhendu Banerjee 16 Erode NO.680,Lotus Enclave,Brough Road,Erode Erode 638001 0424-2261287 S.Pradeep,M.Anandhakrishanan 17 Gorakhpur Wholesale Banking Operations,Shreenath Complex, 10, Park Road, Civil Lines, GORAKHPUR 273 001 0551-2205685 / 0551-3208666 / 09335086506 Kaushalendra Maurya,Parmesh Singh 18 Gwalior J K PLAZA, GAST KA TAZIA, LASHKAR GWALIOR 474001 07514015007/9300788558 Amit Patsaria/Upendra Singh/Chandrashekher Dixit 19 Hissar 3 & 4 MC AREA RED SQUARE MARKET RAILWAY ROAD HISAR HISAR 125000 01662-241023 MR. CHETAN ANAND GOYAL & DEEPAK SHARMA 20 Kakinada #20-1-46,Main Road,OPP SRMT KAKINADA, 533 001 844-2387666-555,093911-3249 0 21 Kanpur 15/46, 1st floor, Civil Lines, kanpur-208001 Kanpur 208001 0512-3028933, 09369078559, 09336015983 pradeep Kumarshukla, Ashutosh Goel, Sajjad Hasan Rizvi ( CH) 22 Kolkata Abhilasha - II, 6 Royd Street (2nd Floor) Kolkata 700016 22273761, 9331992557, 9331430891 Sourav Banerjee/ Debajyoti Basu/ Nirmalya Banerjee 23 Madurai Sri Nithyakalyani Towers, No 34 Krishnarayan Tank Street, North Veli Street Madurai 625001 0452 4246609 , 4246609\\\\9367156995 T Thangarajan , V Revathy , J Jegadesh Chandra Bose , K.M.M Balaji 24 Mumbai Ground Floor, Maneckji Wadia Building,Nanik Motwani Marg,Near Kala Ghoda,opp Mumbai University,Fort Mumbai- 400 001 Mumbai 400001 022-40801570 / 1528 / 1570 / 1560 Sushant shenoy Vinod Thakur Shahid Khan Ramesh Gujar 25 Muzaffarnagar First Floor, 53/4 Janshat Road,New Mandi Muzaffarnagar 251001 01131+2661333 / RIM 9319065143 shashnak j ain 26 Navsari GR FLR , NANDANI COMPLEX , STATION ROAD NAVSARI 396445 02637-280901,9327568065 TARAK GANDHI,NIRAV GANDHI 27 Nellore 17/126, G.V.R. Enclave, G.T. Road, NELLORE - 524001 Nellore 524001 0861-6450852/0861-2327171 Mr.Mulali Krishna/ Mr.Ayyangar/ Mr.Prasanna Kumar 28 Delhi Fig-Ops 1st Floor, Kailash Bldg New Delhi 110001 011-43174071/011-43174072/011-43174073 Onkarnath Mishra / Nitin Wahal 29 Pondicherry T.S.No.6, 100Ft Road, Ellaipillaichavady, Pondicherry 605005 0413 - 2206575/9362845444 R Kumar/R Dhakshinamoorthy/ G V Venkatasamy 30 Raipur HDFC BANK LTD, Chawla Towers, Near Bottle House, Shankar Nagar , Raipur, Chhattisgarh 492007 RAIPUR 492007 0771-4003110/3112 SUNIL NAGPAL 31 Rajahmundry 46-17-20, 1stFloor, Danavaipet, Rajahmundry Rajahmundry 533103 0883-2428691 / 0883-2428826, RIM:9391132485 Trivikram / Srikanth 32 Rajkot Shivalik - V , 3rd Floor, Gondal Road, Rajkot Rajkot 360002 0281-6536982/09377408494 Jigar Mankad/Paresh Patel/Kinara Patel/Dhaval Vakharia 33 Ranchi 56 ROHINI COMPLEX CIRCULAR ROAD LALPUR RANCHI 834001 RANCHI 834001 6512560522 NAVEEN CHANDRA/JYOTI PRAKASH 34 Salem HDFC Bank Ltd,5/241-F Rathan Arcade,Five Roads, Meyyanur, Salem-636004 SALEM 636004 0427-2331604 Rajakumar 35 Shillong ANDERS MANSION, POLICE BAZAR,SHILLONG SHILLONG 793001 3642506043 ASHISH BALODI 36 Solapur HDFC Bank Ltd 8516/11 Murarji Peth,Sun Plaza Bldg,Lucky Chowk,Solapur - 413007 Solapur 413007 0217-2320877 - 9325230208 Atul Ghavare, Ravishankar Hatyalikar, Satyanarayan Sadafule 37 Surendranagar Middle Point, A Wing, Nr : Milan Cenama, Main Road, Surendranagar. Surendranagar. 363002 Rim : 9328208807/02752-650105 Vikram Mehta/ Jinesh Hakani 38 Tirupati HDFC BANK LTD, 19-8-180,Krishna Arcade, Beside IBP Petrol pump, Near Annamaiah Circle TIRUPATI 517501 9391132489/ 9347527320/ 8772220374 E.Rajsekhar/ T.Ramesh/ V.Srinivas 39 Trichur Third Floor, Suharsha Towers,Shornur Road, TRICHUR 680001 0487-6452085/ 6452098 / 09387069206 Tito Joy/Shabeer Shaik 40 Trichy NO.11 PLA KANAGU TOWERS, 11 th Cross , Main Road, Thillainagar Trichy 620018 0431-2742204 & 09364192987 J Anthony Vijay, VLN Kasthuri Rengan 41 Udupi Panduranga Tower/ Diana Circle, Court Road, Udupi --5760101 Udupi 576101 0820-4294936/9343789969 Puneet Arora, Manoj Puthran / Hariprasad B 42 Vijayawada 40-1-48/2, 2nd Floor,Valluri Complex, M G Road VIJAYAWADA 520010 0866-6647400/9395153648 REGULA GIRI BABU/ KOTHAMASU HEMA CHANDRA PRASAD\\n\\nSr No Branch Name Branch Code Address Pincode Phone BM BM- EMAIL BM MOBILE 1 Ahmedabad 0024 JMC House, Opp. Parimal Gardens,Opp Parimal Garden, Ambawadi, Ahmedabad - 380 006. 380006 (079) 66523717-719 Farookh bharucha farookh.bharucha@icicibank.com 9825443232 2 Bangalore 0002 ICICI Bank Towers, 1, Commissariat Road, Ground Floor, Bangalore 560025 080 - 41296007 Kalyana chakravarthy k.chakravarthy@icicibank.com 9845728959 3 Belgaum 0176 Shree Krishna Towers, #14, Khanapur Road, RPD Cross, Tilakwadi, Belgaum. 590006 0831 -2404 203, 2404 204,2404 205 Jyoti murigeppa Bachari jyoti.bachari@icicibank.com 9845300729 4 Bharuch 0178 Blue Chip Complex, Sevashram Road, Panchbatti,Bharuch 392001 02642 - 252451/ 52 / 53 Ravi sharma ravi.sharm@icicibank.com 9825400250 5 Chennai 0009 110, Prakash Presidium, Uthamar Gandhi Salai, (Nungambakkam High Road), Chennai. 600034 044 - 28228003,4,28220713,28222461,28222399,28256359 Venkatesh kotta venkatesh.kotta@icicibank.com 9940161609 6 Ernakulam (Cochin) 0010 Emgee Square, M.G.Road, Ernakulam, Kochi. 682035 0484 - 4081222 Prakash s prakash.p@icicibank.com 9895779820 7 Coimbatore 0016 Cheran Plaza, 1090, Trichy Road, Coimbatore. 641018 0422-4292102 - 4292115 Rupesh rajagopal rupesh.rajagopal@icicibank.com 9944422101 8 Dehradun 0164 ICICI Bank Ltd., NCR, Plaza,24, New Cantt Road, Hathibarkala, Dehradun, Uttaranchal - 248001 248001 0135 - 2743664/0135-2743663 Abhay shukla abhay.shukla@icicibank.com 9719101626 9 Faridabad 0083 ICICI Bank Ltd., Booth No. 104-105, District Centre, Sector 16, Faridabad- 121007, Haryana 121007 0129-4091401-437 Amit bhatnagar amit.bhatnagar@icicibank.com 9873332015 10 Guntur 6307 ICICI Bank Ltd., 5-82-2, PMG Complex, Lakshmipuram Main Road, Guntur - 522002 (Andra Pradesh) 522002 0863 - 2233653 / 2252500 / 2234391 Jeevan zachariah jeeevan.zachariah@icicibank.com 9895040302 11 Gurgaon 0021 ICICI Bank Ltd, SCO 18 & 19, HUDA Shopping Centre, Sector-14, Market Complex, Gurgaon - 122001 122001 0124 - 4267151-7 Gagandeep Singh bedi gagandeep.bedi@icicibank.com 9873344279 12 Guwahati 6343 Ground Floor, Shanti Complex, G S Road, Bhangagarh, Guwahati. 781005 0361 - 2452748,2452743,2450943, 2457782 Samrat aich samrat.aich@icicibank.com 9954190813 13 Hubli 0157 Eureka Junction, Travellers Bungalow Road, Hubli. 580029 0836 - 4265212,4265216,4265223,4265240,4265229 Vaishali mahishi vaishali.mahishi@icicibank.com 9945512411 14 Jaipur 0012 C-99, Shreeji Towers, Subhash Marg, Near Ahimsa Circle, C Scheme, Jaipur. 302001 0141 - 5107444, 0141 - 2361992 Sharad mehta sharad.mehta@icicibank.com 9829050105 15 Jamshedpur 0089 Natraj Mansion, Main Road, Bistupur, Jamshedpur. 831001 0657 - 2422509 / 10 / 2425907 / 12 Ashwani kumar ashwani.k@icicibank.com 9934011710 16 Kanpur 6288 J.S Towers 208001 0512 - 2331041,42,43,44,45 Amiq khan amiq.khan@icicibank.com 9838077035 17 Kolhapur 0166 Ground Floor, Vasant Plaza, Rajaram Road, Rajarampuri, Kolhapur. 416001 0231 - 2534292/3/4 Kishore kumar kishore.kumar@icicibank.com 9923001769 18 Kolkata 0006 22, R N Mukherjee Road, Kolkata. 700001 033 - 22428537 / 22100995 Renab Kumar jha renab.jha@icicibank.com 9830010931 19 Lucknow 6281 ICICI Bank Ltd, Shalimar Tower, 31/54 M.G. Marg, Hazratganj, Lucknow -226001\\\\n 226001 0522 - 2214246 /2214247 / 2214254 Anurag srivastava anurag.sri@icicibank.com 9839531144 20 Madurai 6003 33, North Chitrai Street, Madurai. 625001 0452 - 2628027/2628072 Ramesh m m.ramesh@icicibank.com 99949 44788 21 Mumbai (Capital Market Div.) 0004 30,Mumbai Samachar Marg 400001 022-22627600 Roshan Tellis roshan.tellis@icicibank.com 22 Mysore 0152 2950, Aishwarya Arcade, 9th Cross, Kalidasa Road, V.V. Mohalla, Mysore. 570002 0821-2414006/2412222/2416888 Anil kumar K anilk@icicibank.com 9880850996 23 Nagpur 0059 Vishnu Vaibhav, 222, Palm Road, Civil Lines, Nagpur. 440001 0712 - 2540302 / 5614040 /2561983 /2540294 Ajit parwate ajit.parwate@icicibank.com 9923750309 24 New Delhi 0007 9A, Phelps Building, Inner Circle,Connaught Place, New Delhi. 110001 011 -41517954-58 Deepali chitnis deepali.chitnis@icicibank.com 9899699261 25 Noida 0031 K-1, Senior Mall, Sector 18, NOIDA-201301, Uttar Pradesh. 201301 (0120) - 4059801-75 Ravinder negi ravinder.negi@icicibank.com 9899457693 26 Pondicherry 0056 47, Mission Street, Pondicherry. 605001 0413 - 2332237 / 38 / 42 Gopalakrishnan devasenathipathy gopala.devasena@icicibank.com 9894630655 27 Pune 0005 A-Wing, Shangrila Gardens, Bund Garden Road, Pune. 411001 020 - 66270640 / 66270641 Salim sakarlal Khimani salim.khimani@icicibank.com 9821807561 28 Rajkot 0153 Jai Hind Press annexe, Opp. Shardabaug, Babubhai Shah Marg, Rajkot. 360001 0281 - 2443973 / 74 / 75 / 76 Rajeev nair rajeev.n@icicibank.com 9825304905 29 Salem 6119 Swarnambigai Plaza, S. F. No. 6/5, Block no. 7, Ward-C, Omalur Main Road, Near Bus Stand, Salem. 636009 0427 - 2336635 / 36 / 39 Vasudevan b N R vasudevan.bnr@icicibank.com 9944944120 30 Surat 0052 Anjan Shalaka, Lal Bungalow, Athwa Lines, Surat. 395007 0261 - 6452556, 2258234/35 Mehernosh kasad mehernosh.kasad@icicibank.com 9825484440 31 Trichy 6132 New no - 58, West Bouleward Road, Trichy. 620002 0431 - 2702252 / 2704052 Prasanna kannappa prasanna.k@icicibank.com 9994686661 32 Visakhapatnam 0060 47-14-18, Isnar Satyasri Complex, Main Road, Dwarkanagar, Vishakapatnam. 530016 0891 - 2500641 - 43,45,46 Chandra Shekar mukkavalli chandra.mukkavalli@icicibank.com 9885080180\\n\\nMuthoot Finance Limited- NCD Bond - IDBI Bank Limited - Collection Centre Details\\n\\nSr. No. Location Vertical Sol ID Contact Person/s Contact Nos. E-mail id\\'s Address Landline Mobile Fax No. Mr.Asheet Anand,BH,PBG 9897545441 asheet_anand@idbibank.com\\n\\n1 Ambala 310 Mr. Sushil Kumar Samal,ASOM,PBG (0171)-2631819 9896983085 ss.kumar@idbi.co.in IDBI Bank Ltd.,169/2,Rai Market,Ambala Cantt.Ambala-133001,Haryana Mr.Brajesh Singh,BH,PBG (0171)-2631819 9896808508 b_singh@idbi.co.in\\n\\n2 Amritsar 072 Ms.Shveta Aggarwal,SOM,PBG (0183)-2224574 9814244405 0183-5096575 shveta_aggarwal@idbi.co.in IDBI Bank Ltd.,Adjacent to Hotel Raj Continental,Court Road,Amritsar-143001,Punjab Ms.Meenakshi Arora,BH,PBG (0183)-2224575 m_arora@idbi.co.in\\n\\n3 Aurangabad 076 Mr.Nayan Desai,BH,PBG\\\\n (022) 22885288 9665900022 +91 (240) 5622-996 nayan_d@idbibank.com IDBI Bank Ltd.,Plot No. 07, Raghbir Chambers,Vidya Nagar,Jalna Road,Aurangabad Pin : 431003,Maharashtra Mr.Rakesh Krishnarao Bhoge,SOM,PBG \\\\n 9881472580 rakesh.bhoge@idbi.co.in\\n\\n4 Bangalore MCG 008 Mr.Jayesh K Gopalan,RM,CMS 9845294666 kg.jayesh@idbi.co.in IDBI Bank Ltd.,IDBI House, 58 Mission Road,Bangalore Pin : 560027,Karnataka Mr.Kashif Ali Khan,CMS-OPS \\\\n 9844646986 Kashif.khan@idbi.co.in\\n\\n5 Bareilly 232 Mr.Sharad B Yadav SOM,PBG 0581-2510399 997294942 sharad.yadav@idbi.co.in IDBI Bank Ltd.,146 Civil Lines,Circuit House,Chouraha.,Bareilly Pin-243001,UP Mr. Prashant Sethi,BH,PBG 0581-2510499 9997748889 prashant_sethi@idbi.co.in Mr.Dinesh Sahu,BH,PBG 9438730612 dinesh.sahu@idbi.co.in\\n\\n6 Bhilai 048 Mr.Ranjan Kumar Rath,BH 0788-2292158 9893395482 +91 (788) 292-274 ranjankumar.rath@idbi.co.in IDBI Bank Ltd.,New Era, 19, Priyadarshni Parisar,Nehru Nagar Square,Bhilai Pin : 490020,Chhattisgarh Mr. Chandresh Kumar,SOM 9752234138 chandresh.kumar@idbi.co.in\\n\\n7 Chennai PBG Mr. V. Venkatakrishnan, BH 044-2829 4443 99401 93947 manoj_alex@idbibank.com IDBI Bank Ltd, PM Towers, 37, Greams Road ,Chennai Pin : 600006 , Tamil Nadu Ms. Anitha A , RM CMS 099625 33870 a.anitha@idbi.co.in IDBI Bank Ltd, SCO 72-73, Sector 17 - B, Chandigarh Pin : 160017, Chandigarh Chennai PBG 005 Ms.Anuradha (044)-24301731 9940083004 +91 (44) 28295370 anuradha_ds@idbi.co.in IDBI Bank Ltd, PM Towers, 37, Greams Road ,Chennai Pin : 600006 , Tamil Nadu Ms. Anitha A , RM CMS 099625 33870 a.anitha@idbi.co.in Mr.Narayanan Ganesh Raj,RH,CMS \\\\n (044)-22355229 n.ganeshraj@idbi.co.in Anandhalakshmi R,SOM,PBG 9500115716 ra.lakshmi@idbi.co.in\\n\\n8 Chandigarh 003 Mr.Ajay Guleria,SOM,PBG (172)- 2547741 9872224535 (0172) 4678897 a_guleria@idbi.co.in IDBI Bank Ltd.,SCO 119-120,Sector 43 B ,Chandigarh Pin : 160022 Mr.Kapil Vashishat,CH & BH,PBG (0172) -4678894 9915555648 kapil.vashishat@idbi.co.in Mr. Harish Kumar,RM,CMS (0172)-5059735 9216220222 harish.kumar@idbi.co.in\\n\\n9 Gurgaon 038 Mr.Dhanesh Kumar Chaudhary \\\\n, BH, PBG 9971114271 +91 (124) 2357445 dhanesh.chaudhary@idbi.co.in IDBI Bank Ltd.,Pal Towers, Mehrauli-Gurgaon Road,Village Sikanderpur Chosi ,Sikanderpur,Gurgaon Pin : 122022,Haryana Mr.Rohit Samson Massey,SOM,PBG \\\\n 9871176655 rohit_massey@idbi.co.in\\n\\n10 Jaipur MCG Mr. Jagdish P Parish, BH,MCG (0141) 2741881 9928381815 jp.parish@idbi.co.in IDBI BANK LTD,Gr. Flr, Jeevan Nidhi Bldg., LIC Complex,Bhawani Singh Road,Jaipur Pin : 302005 Ms. Deepika Kispotta, SOM (0141) 5105300 9799621499 deepika.kispotta@idbi.co.in Jaipur 013 Ms.Payal Mathur,SOM,PBG 0141- 2701802 98290-50470 (0141)-2363174 payal_mathur@idbi.co.in IDBI Bank Ltd.,D-24 Durlabh Niwas,Prithviraj Road,C Scheme,Jaipur Pin : 302001 Mr.Vinayak Datt,BH,PBG 0141-5106302 9828087774 v_datt@idbi.co.in Ms.Chitra Mundra,SOM,PBG 0141-5113456 chitra_chib@idbi.co.in\\n\\n11 Kanpur 090 Mr.Sonia Markandeya ,SOM,PBG\\\\n 0512-2305437 9161000030 (0512)-2304286 sonia.markandeya@idbi.co.in IDBI BANK LTD, Jeevan Vikas,MG Road,Near Statue Junction,Kanpur Pin : 208001,Uttar Pradesh Mr.Vikas Gaba \\\\n,BH,PBG 9719016479 vikas.gaba@idbi.co.in 12 Kolkata MCG 135 Ms. Shaiju Varghese , RM, CMS (033) 66338866 \\\\n 9830216111 shaiju.varghese@idbi.co.in IDBI BANK LTD, 44, Shakespeare Sarani, Kolkata-700017 Mr. Kamlesh Kumar Thakur,RM,CMS (033) 66337772 9836933446 kamlesh.thakur@idbi.co.in Mr. Supriyo Dawn, Team Member, RPU \\\\n (033) 65500350 9836667739 supriyo.dawn@idbi.co.in\\n\\n13 Hyderabad 002 Mr.Shankar V. \\\\n,BH,PBG 040-66746020 9490117153 +91 (40) 3220373 v.shankar@idbi.co.in IDBI Bank Ltd.,Mahavir House,Basheerbagh Square,Hyderabad Pin : 500029,Andhra Pradesh Mr.Uma Manohar 040-66103644 uma_manohar@idbi.co.in\\n\\n14 Mumbai MCG 004 Mr. Kumarjit Kar, AM,CMS (022)-66588187 9702098016 +91 (22) 66588111,66588130 kumarjit.kar@idbi.co.in IDBI BANK LTD,Mittal Court, \\'A\\' Wing,2ND Floor,CMS Dept.,Nariman Point,Mumbai-400021,Maharashtra Mr. Alphonse Rajesh Rodrigues, AGM,CMS (022) 66588264 9820203722 a_rodrigues@idbi.co.in\\n\\n15 New Delhi 011 Mr.Manoj Kumar , BH,PBG\\\\n 9999219904 +91 (11) 26499680 sanjeev.gupta@idbi.co.in IDBI Bank Ltd.,Surya Kiran Building,Ground Floor,19 K G Marg,New Delhi Pin : 110001 Ms.Navpreet Kaur ,SOM,PBG 011-41510637 navpreet.kaur@idbi.co.in Mr.Deepak Sood, RH CMS (011) 41306645 9873714562 deepak.sood@idbi.co.in Mr. Manu Jaitly, RM CMS (011) 41306641 9871915988 manu.jaitly@idbi.co.in\\n\\n16 Patna 065 Mr.Rajeev Kamal Ramanna \\\\n 9473388578 +91 (612) 2203777 rajeev.ramanna@idbi.co.in IDBI Bank Ltd.,Kashi Palace Complex,Dak Bungalow Road,Opposite Heera Palace ,Patna Pin : 800001,Bihar Mr.Rakesh Kumar,BH,PBG\\\\n (0612)-6510293 9507036553 k.rakesh@idbi.co.in\\n\\n17 Patiala 079 Gurpreet Singh Sekhon,SOM,PBG 9988067679 +91 (175) 5005-379 gurpreet.sekhon@idbi.co.in IDBI Bank Ltd.,10,Chotti Baradari,The Mall,Patiala Pin : 147001,Punjab Mr.Ramesh Chand,BH,PBG\\\\n (0175)-5005380 9463710637 ramesh.chand@idbi.co.in\\n\\n18 Pune PBG 007 Sanjay Ganpati Vanarase, SOM,PBG \\\\n (020))-66004107 9422009679 91(20) 66004121 satyajit_deshpande@idbibank.com IDBI Bank Ltd., Dynaneshwar Paduka Chowk, Fergusson College Road ,Pune Pin : 411004, Maharashtra Mrs. Preeti Jitendra Jeswani, RM,CMS\\\\n (020) 66004165 9158985490 pp.dhaliwal@idbi.co.in Mr.Tapas Ranjan Dash (020)-64019708 9890877746 tapas_dash@idbi.co.in Mr.Rajesh Madan Shivarkar \\\\n 9422590549 rajesh.shivarkar@idbi.co.in\\n\\n19 Surat 051 Mr.Jatin Umeshchandra Vyas,SOM,PBG +91 (261) 2656374 +91 (261) 2656374 vyas.jatin@idbi.co.in IDBI Bank Ltd.,ESS EN House,Ghod Dod Road,Surat Pin : 395001,Gujarat Mr.Vikas Pandit,BH,PBG\\\\n (0261)-6540385 9998944941 vikas_pandit@idbi.co.in\\n\\n20 Vadodara 021 Mr.Maheshkumar Mansukhlal Chavda,SOM 0265-2350362-363 +91 (265) 2355880 maheshkumar.chavda@idbi.co.in IDBI Bank Ltd.,3rd Floor - CMS Desk, Garg Complex - 46A, Gautam Nagar, Opp. Pizza Hut, Race Course, Vadodara Pin : 390007,Gujarat Ms.Swati S. Madhav,BH,PBG\\\\n (0265)-2350362-363 9998944933 swati_s@idbi.co.in\\n\\nSR No State Branch Name Address STD Numbers Fax 1 Guj Ahmedabad World Business House, M. G. Road \\'Nr. Parimal Garden, Ellis Bridge Ahmedabad - 380 015 \\' ahar@indusind.com 079 26426104 - 8 26560401 2 RAJSTHAN Ajmer Rang Vihar, 9/86,Kutchery Road \\'M. G. Road, Ajmer - 305 001 ajra@indusind.com 0145 2631999, 2428239, 2428240 2428251 3 UP Allahabad Ganpati Towers, 56 Sardar Patel Marg\\'Civil Lines, Allahabad - 211 001 \\'alup@indusind.com 0532 2260354, 2260353 2260355 4 Guj Anand Ground Floor, Maruti Sunay Building, Aryanager, Amul Dairy Road, Anand – 388001.\\\\n\\'angu@indusind.com 02692 267351/ 52/ 53/54\\\\n266630, 26631 5 Punjab Bhatinda 2679 / C - 1- A, Guru Kanshi Marg\\' Bhatinda 151 001\\' bapu@indusind.com 0164 2213466- 68 / 222 4284 5004187 6 Guj Bhavnagar Shop Nos 1 to 7 and 13, Madhav Hills \\'Waghawadi Road, Bhavnagar – 364 002 bhgu@indusind.com 0278 2512055 / 2011 2512088 7 TAMILNADU Chennai No.3 Village Road Nungambakkam,\\'Chennai - 600 034 \\'manb@indusind.com 044 044 4596 2500 / 01 / 02 / 03\\\\n 4596 2510\\\\n(Operations) \\\\n4596 2520 (SPOPS) 8 UTTARANCHAL Dehradun Ground Floor, 59/4, International Trade Center, Rajpur Road,\\'Opposite Uttaranchal Secretariat, Dehradun 248001 \\'derr@indusind.com 0135 2740411/ 2740522 2740433 9 Guj Gandhinagar GF, Unit no. 14, Suman City, Plot no. 17, Sec – 11, Gandhinagar – 382 0110. 079 23240596-7 / 84 / 85 / 86 10 UP Ghaziabad C-76, RDC Main Road, Opp Petrol Pump, Ghaziabad – 201002 0120 4041560 11 AP Hyderabad 1-8-448, Sardar Patel Rd.Begumpet,\\'Secunderabad – 500 003. \\'hybe@indusind.com 040 2790 7660 / 64 / 65 / 4663 27907673 12 MP Indore Industry House15 Agra Mumbai Road,\\'Old Palasia, Indore - 452 001. \\'inab@indusind.com 0731 2542696 / 7 / 8 2539092 13 RAJSTHAN Jaipur Sangam Complex,Gr.Flr.Church Road, \\'Jaipur 302 001\\' jach@indusind.com 0141 2387301-05 2387084 14 Jammu and Kashmir Jammu Gupta Plaza, Bahu Plaza, Jammu – 180 004.\\\\njajk@indusind.com 0191 2470040 15 Guj Jamnagar Shivam Complex, Teen Batti, Opp. Badri Complex,\\'Jamnagar -361 001 \\'jagu@indusind.com 0288 2664322 / 5760 2664321 16 RAJSTHAN Jodhpur Showroom No. 3&4, Olympic Tower Bldg.,\\'Station Road, Jodhpur 342 003 \\'josr@indusind.com 0291 510 2288 / 2289 / 6990 5105662 17 WEST BENGAL Kolkata Savitri Towers, 3A, Upper Wood Street, Kolkata – 700 017 \\'caps@indusind.com 033 30212400 / 01 (30 lines)\\\\nBH - direct 22896204.\\\\nHead Ops direct - 22896205 22896206 18 RAJSTHAN Kota 412 Shopping Centre,\\'Kota 324 007 \\'kora@indusind.com 0744 2366677 - 80 2366681 19 UP Lucknow K\\'s Trident, 10 Rana Pratap Marg, Lucknow – 226 001, Uttar Pradesh\\\\nlula@indusind.com 0522 220 4681 / 82 / 83 220 4680 20 Punjab Ludhiana S.C.O. 21, Feroze Gandhi Market, Opp. Ludhiana Stock Exch.,\\'Ludhiana 141 001 \\'lufe@indusind.com 0161 5043801 / 06 2771810 21 PUNJAB Mohali S.C.F. 23-24Phase III-B-2,\\'SAS Nagar, Mohali, Chandigarh 160 059 \\'moha@indusind.com 0172 502 0821 / 832 505 3651 22 MAHARASHTRA Mumbai Premises No. 1, Sonawala Building 57, Mumbai Samachar Marg, Fort, Mumbai 400 001 022 66366580 - 83 66366590 / 87 MAHARASHTRA Thane(mumbai) Jinja Society, Opp. Dhamani Estate, L. B. S. Marg,\\'Thane 400 602 \\'both@indusind.com 022 25390387 / 88 / 89 25390376 23 MAHARASHTRA Nagpur Shri Swami Plaza, 97, East High Court Road, Ramdas Peth, Nagpur – 440 010 \\'nass@indusind.com\\' 0712 2547456, 2534188 2547547 24 NEW DELHI NewDelhi/Delhi Dr. Gopal Das Bhawan 28,\\'Barakhamba Road, New Delhi - 110 001. 011 23738040 / 8408 / 8407 23738041 25 HARYANA Rohtak SCO 19 & 20 , Subhash Park , Civil Lines , Rohtak - 124 001 \\' roha@indusind.com 01262 645715 / 645669 / 327890 255944 26 Guj Rajkot Pick Point,I Floor,Dr Yagnik Road,\\'Near Vivekananda Statue,Rajkot - 360 001 \\'rara@indusind.com 0281 2461893 / 94 2461892 27 HIMACHAL PRADESH Shimla Bell Villa, The Mall,\\'Shimla - 171 001. \\'shhp@indusind.com 0177 2654187, 2652217 2651251 28 Karnataka Shimoga Bandigadi Complex, I Floor,\\'Nehru Road,Shimoga- 577 201 \\' shka@indusind.com 08182 227722 220944 29 Guj Surat G-2, Empire State Bldg., Near Udhana Darwaja,\\'Ring Road, Surat 395 002 \\'surr@indusind.com 0261 2366823 / 24 / 27 / 30 2346469 30 TAMILNADU Tirupur No. 19, T. S. Puram (Logu Building), \\\\nValipalayam, Tirupur – 641 601 tico@indusind.com 0421 2242875/ 2242885 2202471 31 RAJSTHAN Udaipur 2-C, Chowgaan Yojana, Near Lok Kala Mandal,\\'Panchavati Choraha, Udaipur - 313 001 \\'udra@indusind.com 0294 2417294/295 2415240 32 Guj Vadodara Ground Floor, Swami Premdas Jalaram Hospital, Behind R. T. O.\\'Harni Warasia Ring Road, Warasia, Vadodara– 390 006 0265 2512595, 2512597 2512596 33 Guj Valsad Shop No. 7 –10, Megh Rachana Tower, Sheela Park, Tithal Road, Valsad 396 001 \\'vamr@indusind.com 02632 254665, 254666 254972')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "\n",
    "dir_loader= DirectoryLoader(\n",
    "    \"../data/excel\",\n",
    "    glob=\"**/*.xls\",\n",
    "    loader_cls=UnstructuredExcelLoader\n",
    "    )\n",
    "\n",
    "excel_documents = dir_loader.load()\n",
    "excel_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987c35b",
   "metadata": {},
   "source": [
    "### embeddings and vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22286e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ca7143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Enbedding Dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1dc22d8ae40>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles Document Embedding generation using SentenceTransformers\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading the embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Enbedding Dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded.\")\n",
    "        \n",
    "        print(f\"Generating embedding for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generate embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    # def get_embedding_dimension(self) -> int:\n",
    "    #     \"\"\"Get the embedding dimension of the model\"\"\"\n",
    "    #     if not self.model:\n",
    "    #         raise ValueError(\"Model not loaded.\")\n",
    "    #     return self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "\n",
    "## initialize embedding manager\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac9dcb",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7c5d1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing document in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1dc2378ee40>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embedding in ChromaDB vector store\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistant ChromaDB client\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embedding for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing document in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_document(self, document: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add document and their emebedding to vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain Documents\n",
    "            embeddings: Corresponding embeddings for the document\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # Prepare Metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                id=ids,\n",
    "                embedding=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection {self.colleciton.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to the vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e33e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
