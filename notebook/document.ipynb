{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ff6541",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e7678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Document Structure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7568115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'file.txt', 'author': 'Laksh', 'date_created': '2025-09-08'}, page_content='This is the content for the RAG system.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"This is the content for the RAG system.\",\n",
    "    metadata={\n",
    "        \"source\": \"file.txt\",\n",
    "        \"author\": \"Laksh\",\n",
    "        \"date_created\": \"2025-09-08\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee85bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple txt file\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01137cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created.\n"
     ]
    }
   ],
   "source": [
    "sample_text ={\n",
    "    \"../data/text_files/Why_RAG.txt\": \"\"\"Why is Retrieval-Augmented Generation important?\n",
    "LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.\n",
    "\n",
    "Known challenges of LLMs include:\n",
    "\n",
    "Presenting false information when it does not have the answer.\n",
    "Presenting out-of-date or generic information when the user expects a specific, current response.\n",
    "Creating a response from non-authoritative sources.\n",
    "Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.\n",
    "You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\n",
    "\n",
    "RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\n",
    "\"\"\",\n",
    "    \"../data/text_files/RAG_benefits.txt\": \"\"\"What are the benefits of Retrieval-Augmented Generation?\n",
    "RAG technology brings several benefits to an organization's generative AI efforts.\n",
    "\n",
    "Cost-effective implementation\n",
    "Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable.\n",
    "\n",
    "Current information\n",
    "Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users.\n",
    "\n",
    "Enhanced user trust\n",
    "RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution.\n",
    "\n",
    "More developer control\n",
    "With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM's information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications.\n",
    "\"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "for filepath, content in sample_text.items():\n",
    "    with open(filepath,'w',encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text files created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de2a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TextLoader\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader= TextLoader(\"../data/text_files/Why_RAG.txt\",encoding= \"utf-8\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
